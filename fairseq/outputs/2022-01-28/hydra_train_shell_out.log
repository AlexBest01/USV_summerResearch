2022-01-28 12:59:15 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13934
2022-01-28 12:59:15 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13934
2022-01-28 12:59:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-01-28 12:59:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-01-28 12:59:15 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-01-28 12:59:15 | INFO | fairseq.distributed.utils | initialized host cuda6.ecs.vuw.ac.nz as rank 0
2022-01-28 12:59:15 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-01-28 12:59:15 | INFO | fairseq.distributed.utils | initialized host cuda6.ecs.vuw.ac.nz as rank 1
2022-01-28 12:59:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'json', 'log_file': '/home/bestalex/Documents/fairseq/outputs/2022-01-28/hydra_train.log', 'tensorboard_logdir': 'outputs/2022-01-28/tb_logs', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13934', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1500000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1500000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.005], 'stop_min_lr': 1e-09, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec', 'prediction_steps': 12, 'sample_distance': None, 'cross_sample_negatives': 0, 'num_negatives': 10, 'conv_feature_layers': '[(512,10,5),(512,8,4),(512,4,2),(512,4,2),(512,4,2),(512,1,1),(512,1,1)]', 'conv_aggregator_layers': '[(512,2,1),(512,3,1),(512,4,1),(512,5,1),(512,6,1),(512,7,1),(512,8,1),(512,9,1),(512,10,1),(512,11,1),(512,12,1),(512,13,1)]', 'dropout': 0.0, 'dropout_features': 0.0, 'dropout_agg': 0.0, 'aggregator': 'cnn', 'gru_dim': 512, 'no_conv_bias': False, 'agg_zero_pad': False, 'skip_connections_feat': False, 'skip_connections_agg': False, 'residual_scale': 0.5, 'log_compression': False, 'balanced_classes': False, 'project_features': 'none', 'non_affine_group_norm': False, 'offset': 'auto', 'activation': 'relu', 'vq_type': 'none', 'vq_vars': 320, 'vq_groups': 2, 'vq_dim': 0, 'vq_depth': 1, 'combine_groups': False, 'vq_temp': [2.0, 0.5, 0.999995], 'vq_gamma': 0.25, 'infonce': False}, 'task': {'_name': 'audio_pretraining', 'data': '/local/scratch/bestalex/cut_10s_mixed_usv/train-clean-100', 'labels': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': 150000, 'min_sample_size': None, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': 'none'}, 'criterion': {'_name': 'wav2vec', 'infonce': False, 'loss_weights': None, 'log_keys': ['temp', 'accuracy']}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.005]}, 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 500, 'warmup_init_lr': 1e-07, 'lr': [0.005], 'min_lr': 1e-06, 't_mult': 1.0, 'lr_period_updates': -1.0, 'lr_shrink': 0.1, 'max_update': 400000}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-28 12:59:20 | INFO | fairseq.models.wav2vec.wav2vec | Wav2VecModel(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (1): Sequential(
        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (2): Sequential(
        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (3): Sequential(
        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (4): Sequential(
        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (5): Sequential(
        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (6): Sequential(
        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
    )
  )
  (feature_aggregator): ConvAggegator(
    (conv_layers): Sequential(
      (0): Sequential(
        (0): ReplicationPad1d((1, 0))
        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (1): Sequential(
        (0): ReplicationPad1d((2, 0))
        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (2): Sequential(
        (0): ReplicationPad1d((3, 0))
        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (3): Sequential(
        (0): ReplicationPad1d((4, 0))
        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (4): Sequential(
        (0): ReplicationPad1d((5, 0))
        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (5): Sequential(
        (0): ReplicationPad1d((6, 0))
        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (6): Sequential(
        (0): ReplicationPad1d((7, 0))
        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (7): Sequential(
        (0): ReplicationPad1d((8, 0))
        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (8): Sequential(
        (0): ReplicationPad1d((9, 0))
        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (9): Sequential(
        (0): ReplicationPad1d((10, 0))
        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (10): Sequential(
        (0): ReplicationPad1d((11, 0))
        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (11): Sequential(
        (0): ReplicationPad1d((12, 0))
        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
    )
    (residual_proj): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
      (5): None
      (6): None
      (7): None
      (8): None
      (9): None
      (10): None
      (11): None
    )
  )
  (wav2vec_predictions): Wav2VecPredictionsModel(
    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (dropout_feats): Dropout(p=0.0, inplace=False)
  (dropout_agg): Dropout(p=0.0, inplace=False)
)
2022-01-28 12:59:20 | INFO | fairseq_cli.train | Wav2VecModel(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (1): Sequential(
        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (2): Sequential(
        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (3): Sequential(
        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (4): Sequential(
        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (5): Sequential(
        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
      (6): Sequential(
        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (3): ReLU()
      )
    )
  )
  (feature_aggregator): ConvAggegator(
    (conv_layers): Sequential(
      (0): Sequential(
        (0): ReplicationPad1d((1, 0))
        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (1): Sequential(
        (0): ReplicationPad1d((2, 0))
        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (2): Sequential(
        (0): ReplicationPad1d((3, 0))
        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (3): Sequential(
        (0): ReplicationPad1d((4, 0))
        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (4): Sequential(
        (0): ReplicationPad1d((5, 0))
        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (5): Sequential(
        (0): ReplicationPad1d((6, 0))
        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (6): Sequential(
        (0): ReplicationPad1d((7, 0))
        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (7): Sequential(
        (0): ReplicationPad1d((8, 0))
        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (8): Sequential(
        (0): ReplicationPad1d((9, 0))
        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (9): Sequential(
        (0): ReplicationPad1d((10, 0))
        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (10): Sequential(
        (0): ReplicationPad1d((11, 0))
        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
      (11): Sequential(
        (0): ReplicationPad1d((12, 0))
        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))
        (2): Dropout(p=0.0, inplace=False)
        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)
        (4): ReLU()
      )
    )
    (residual_proj): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
      (5): None
      (6): None
      (7): None
      (8): None
      (9): None
      (10): None
      (11): None
    )
  )
  (wav2vec_predictions): Wav2VecPredictionsModel(
    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (dropout_feats): Dropout(p=0.0, inplace=False)
  (dropout_agg): Dropout(p=0.0, inplace=False)
)
2022-01-28 12:59:20 | INFO | fairseq_cli.train | task: AudioPretrainingTask
2022-01-28 12:59:20 | INFO | fairseq_cli.train | model: Wav2VecModel
2022-01-28 12:59:20 | INFO | fairseq_cli.train | criterion: Wav2vecCriterion
2022-01-28 12:59:20 | INFO | fairseq_cli.train | num. shared model params: 32,537,088 (num. trained: 32,537,088)
2022-01-28 12:59:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-28 12:59:20 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 272, skipped 0 samples
2022-01-28 12:59:20 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2022-01-28 12:59:20 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.
2022-01-28 12:59:20 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias
2022-01-28 12:59:20 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias
2022-01-28 12:59:20 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias
2022-01-28 12:59:20 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias
2022-01-28 12:59:20 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias
2022-01-28 12:59:20 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias
2022-01-28 12:59:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2022-01-28 12:59:20 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA RTX A5000                        
2022-01-28 12:59:20 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA RTX A5000                        
2022-01-28 12:59:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2022-01-28 12:59:20 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)
2022-01-28 12:59:20 | INFO | fairseq_cli.train | max tokens per device = 1500000 and max sentences per device = None
2022-01-28 12:59:20 | INFO | fairseq.trainer | Preparing to load checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 12:59:20 | INFO | fairseq.trainer | No existing checkpoint found /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 12:59:20 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-28 12:59:20 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 26450, skipped 0 samples
2022-01-28 12:59:21 | INFO | fairseq.trainer | begin training epoch 1
2022-01-28 12:59:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-28 12:59:32 | INFO | root | Reducer buckets have been rebuilt in this iteration.
2022-01-28 12:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-01-28 12:59:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-01-28 12:59:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-01-28 12:59:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-01-28 12:59:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-01-28 12:59:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-01-28 12:59:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-01-28 12:59:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2022-01-28 12:59:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2022-01-28 12:59:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2022-01-28 12:59:59 | INFO | train_inner | {"epoch": 1, "update": 0.071, "loss": "5.985", "ntokens": "174862", "nsentences": "16.72", "wps": "731417", "ups": "4.18", "wpb": "174862", "bsz": "16.7", "num_updates": "100", "lr": "0.00100008", "gnorm": "41.622", "loss_scale": "0.0625", "train_wall": "27", "gb_free": "20.4", "wall": "38"}
2022-01-28 13:00:23 | INFO | train_inner | {"epoch": 1, "update": 0.135, "loss": "4.858", "ntokens": "174060", "nsentences": "17.92", "wps": "725022", "ups": "4.17", "wpb": "174060", "bsz": "17.9", "num_updates": "200", "lr": "0.00200006", "gnorm": "1.983", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "62"}
2022-01-28 13:00:47 | INFO | train_inner | {"epoch": 1, "update": 0.199, "loss": "4.773", "ntokens": "176862", "nsentences": "16.56", "wps": "729585", "ups": "4.13", "wpb": "176862", "bsz": "16.6", "num_updates": "300", "lr": "0.00300004", "gnorm": "2.167", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "87"}
2022-01-28 13:01:11 | INFO | train_inner | {"epoch": 1, "update": 0.262, "loss": "4.968", "ntokens": "174594", "nsentences": "16.72", "wps": "722286", "ups": "4.14", "wpb": "174594", "bsz": "16.7", "num_updates": "400", "lr": "0.00400002", "gnorm": "1.8", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "111"}
2022-01-28 13:01:35 | INFO | train_inner | {"epoch": 1, "update": 0.326, "loss": "4.74", "ntokens": "174552", "nsentences": "16.56", "wps": "719700", "ups": "4.12", "wpb": "174552", "bsz": "16.6", "num_updates": "500", "lr": "0.005", "gnorm": "0.706", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "135"}
2022-01-28 13:02:00 | INFO | train_inner | {"epoch": 1, "update": 0.39, "loss": "4.435", "ntokens": "175542", "nsentences": "16.88", "wps": "723681", "ups": "4.12", "wpb": "175542", "bsz": "16.9", "num_updates": "600", "lr": "0.005", "gnorm": "0.907", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "159"}
2022-01-28 13:02:24 | INFO | train_inner | {"epoch": 1, "update": 0.454, "loss": "4.26", "ntokens": "176162", "nsentences": "16.96", "wps": "723098", "ups": "4.1", "wpb": "176162", "bsz": "17", "num_updates": "700", "lr": "0.005", "gnorm": "0.775", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "184"}
2022-01-28 13:02:48 | INFO | train_inner | {"epoch": 1, "update": 0.518, "loss": "4.171", "ntokens": "174155", "nsentences": "17.12", "wps": "714759", "ups": "4.1", "wpb": "174155", "bsz": "17.1", "num_updates": "800", "lr": "0.00499999", "gnorm": "0.721", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.6", "wall": "208"}
2022-01-28 13:03:13 | INFO | train_inner | {"epoch": 1, "update": 0.582, "loss": "4.109", "ntokens": "175546", "nsentences": "16.4", "wps": "721756", "ups": "4.11", "wpb": "175546", "bsz": "16.4", "num_updates": "900", "lr": "0.00499999", "gnorm": "0.797", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "232"}
2022-01-28 13:03:37 | INFO | train_inner | {"epoch": 1, "update": 0.646, "loss": "4.064", "ntokens": "174503", "nsentences": "16.9", "wps": "719055", "ups": "4.12", "wpb": "174503", "bsz": "16.9", "num_updates": "1000", "lr": "0.00499998", "gnorm": "0.787", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "257"}
2022-01-28 13:04:01 | INFO | train_inner | {"epoch": 1, "update": 0.709, "loss": "4.015", "ntokens": "174296", "nsentences": "17.52", "wps": "716765", "ups": "4.11", "wpb": "174296", "bsz": "17.5", "num_updates": "1100", "lr": "0.00499997", "gnorm": "0.639", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "281"}
2022-01-28 13:04:26 | INFO | train_inner | {"epoch": 1, "update": 0.773, "loss": "3.976", "ntokens": "174199", "nsentences": "16.48", "wps": "716464", "ups": "4.11", "wpb": "174199", "bsz": "16.5", "num_updates": "1200", "lr": "0.00499996", "gnorm": "0.724", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "305"}
2022-01-28 13:04:50 | INFO | train_inner | {"epoch": 1, "update": 0.837, "loss": "3.925", "ntokens": "175364", "nsentences": "16.8", "wps": "720038", "ups": "4.11", "wpb": "175364", "bsz": "16.8", "num_updates": "1300", "lr": "0.00499995", "gnorm": "0.652", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "330"}
2022-01-28 13:05:14 | INFO | train_inner | {"epoch": 1, "update": 0.901, "loss": "3.886", "ntokens": "176977", "nsentences": "16.8", "wps": "725195", "ups": "4.1", "wpb": "176977", "bsz": "16.8", "num_updates": "1400", "lr": "0.00499994", "gnorm": "0.645", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "354"}
2022-01-28 13:05:39 | INFO | train_inner | {"epoch": 1, "update": 0.965, "loss": "3.861", "ntokens": "175622", "nsentences": "16.56", "wps": "723547", "ups": "4.12", "wpb": "175622", "bsz": "16.6", "num_updates": "1500", "lr": "0.00499992", "gnorm": "0.671", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "378"}
2022-01-28 13:05:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:05:57 | INFO | valid | {"epoch": 1, "valid_loss": "3.853", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.56821e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "1555"}
2022-01-28 13:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1555 updates
2022-01-28 13:05:57 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:06:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:06:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 1 @ 1555 updates, score 3.853) (writing took 10.933185921050608 seconds)
2022-01-28 13:06:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-28 13:06:08 | INFO | train | {"epoch": 1, "train_loss": "4.381", "train_ntokens": "175156", "train_nsentences": "16.8913", "train_wps": "693037", "train_ups": "3.96", "train_wpb": "175156", "train_bsz": "16.9", "train_num_updates": "1555", "train_lr": "0.00499991", "train_gnorm": "3.597", "train_loss_scale": "0.0625", "train_train_wall": "378", "train_gb_free": "20.4", "train_wall": "407"}
2022-01-28 13:06:08 | INFO | fairseq.trainer | begin training epoch 2
2022-01-28 13:06:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:06:29 | INFO | train_inner | {"epoch": 2, "update": 1.029, "loss": "3.831", "ntokens": "175038", "nsentences": "17.68", "wps": "345509", "ups": "1.97", "wpb": "175038", "bsz": "17.7", "num_updates": "1600", "lr": "0.00499991", "gnorm": "0.617", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "429"}
2022-01-28 13:06:53 | INFO | train_inner | {"epoch": 2, "update": 1.093, "loss": "3.795", "ntokens": "176345", "nsentences": "16.42", "wps": "729224", "ups": "4.14", "wpb": "176345", "bsz": "16.4", "num_updates": "1700", "lr": "0.00499989", "gnorm": "0.594", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "453"}
2022-01-28 13:07:18 | INFO | train_inner | {"epoch": 2, "update": 1.156, "loss": "3.766", "ntokens": "176070", "nsentences": "17.12", "wps": "726069", "ups": "4.12", "wpb": "176070", "bsz": "17.1", "num_updates": "1800", "lr": "0.00499987", "gnorm": "0.619", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "478"}
2022-01-28 13:07:42 | INFO | train_inner | {"epoch": 2, "update": 1.22, "loss": "3.742", "ntokens": "173501", "nsentences": "16.56", "wps": "715835", "ups": "4.13", "wpb": "173501", "bsz": "16.6", "num_updates": "1900", "lr": "0.00499985", "gnorm": "0.611", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "502"}
2022-01-28 13:08:06 | INFO | train_inner | {"epoch": 2, "update": 1.284, "loss": "3.714", "ntokens": "175516", "nsentences": "16.48", "wps": "722251", "ups": "4.12", "wpb": "175516", "bsz": "16.5", "num_updates": "2000", "lr": "0.00499983", "gnorm": "0.585", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "526"}
2022-01-28 13:08:31 | INFO | train_inner | {"epoch": 2, "update": 1.348, "loss": "3.696", "ntokens": "173388", "nsentences": "16.72", "wps": "716484", "ups": "4.13", "wpb": "173388", "bsz": "16.7", "num_updates": "2100", "lr": "0.0049998", "gnorm": "0.597", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "550"}
2022-01-28 13:08:55 | INFO | train_inner | {"epoch": 2, "update": 1.412, "loss": "3.666", "ntokens": "175885", "nsentences": "16.48", "wps": "722702", "ups": "4.11", "wpb": "175885", "bsz": "16.5", "num_updates": "2200", "lr": "0.00499978", "gnorm": "0.58", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "575"}
2022-01-28 13:09:19 | INFO | train_inner | {"epoch": 2, "update": 1.476, "loss": "3.643", "ntokens": "174976", "nsentences": "17.04", "wps": "719508", "ups": "4.11", "wpb": "174976", "bsz": "17", "num_updates": "2300", "lr": "0.00499975", "gnorm": "0.559", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "599"}
2022-01-28 13:09:43 | INFO | train_inner | {"epoch": 2, "update": 1.54, "loss": "3.618", "ntokens": "175494", "nsentences": "16.8", "wps": "722549", "ups": "4.12", "wpb": "175494", "bsz": "16.8", "num_updates": "2400", "lr": "0.00499972", "gnorm": "0.558", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "623"}
2022-01-28 13:10:08 | INFO | train_inner | {"epoch": 2, "update": 1.603, "loss": "3.602", "ntokens": "175280", "nsentences": "17.28", "wps": "720561", "ups": "4.11", "wpb": "175280", "bsz": "17.3", "num_updates": "2500", "lr": "0.00499969", "gnorm": "0.537", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "648"}
2022-01-28 13:10:32 | INFO | train_inner | {"epoch": 2, "update": 1.667, "loss": "3.593", "ntokens": "174106", "nsentences": "16.96", "wps": "716934", "ups": "4.12", "wpb": "174106", "bsz": "17", "num_updates": "2600", "lr": "0.00499966", "gnorm": "0.571", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "672"}
2022-01-28 13:10:57 | INFO | train_inner | {"epoch": 2, "update": 1.731, "loss": "3.574", "ntokens": "174157", "nsentences": "16.56", "wps": "713926", "ups": "4.1", "wpb": "174157", "bsz": "16.6", "num_updates": "2700", "lr": "0.00499963", "gnorm": "0.555", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "696"}
2022-01-28 13:11:21 | INFO | train_inner | {"epoch": 2, "update": 1.795, "loss": "3.571", "ntokens": "175724", "nsentences": "16.72", "wps": "721290", "ups": "4.1", "wpb": "175724", "bsz": "16.7", "num_updates": "2800", "lr": "0.00499959", "gnorm": "0.545", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.6", "wall": "721"}
2022-01-28 13:11:45 | INFO | train_inner | {"epoch": 2, "update": 1.859, "loss": "3.543", "ntokens": "176151", "nsentences": "17.2", "wps": "720665", "ups": "4.09", "wpb": "176151", "bsz": "17.2", "num_updates": "2900", "lr": "0.00499955", "gnorm": "0.541", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.1", "wall": "745"}
2022-01-28 13:12:10 | INFO | train_inner | {"epoch": 2, "update": 1.923, "loss": "3.525", "ntokens": "175037", "nsentences": "16.88", "wps": "721215", "ups": "4.12", "wpb": "175037", "bsz": "16.9", "num_updates": "3000", "lr": "0.00499952", "gnorm": "0.541", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "769"}
2022-01-28 13:12:34 | INFO | train_inner | {"epoch": 2, "update": 1.987, "loss": "3.53", "ntokens": "177071", "nsentences": "17.76", "wps": "727162", "ups": "4.11", "wpb": "177071", "bsz": "17.8", "num_updates": "3100", "lr": "0.00499948", "gnorm": "0.581", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "794"}
2022-01-28 13:12:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:12:44 | INFO | valid | {"epoch": 2, "valid_loss": "3.543", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.67339e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "3121", "valid_best_loss": "3.543"}
2022-01-28 13:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3121 updates
2022-01-28 13:12:44 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:12:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:12:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 2 @ 3121 updates, score 3.543) (writing took 11.639189205132425 seconds)
2022-01-28 13:12:55 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-28 13:12:55 | INFO | train | {"epoch": 2, "train_loss": "3.642", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "672893", "train_ups": "3.84", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "3121", "train_lr": "0.00499947", "train_gnorm": "0.573", "train_loss_scale": "0.0625", "train_train_wall": "378", "train_gb_free": "20.4", "train_wall": "815"}
2022-01-28 13:12:55 | INFO | fairseq.trainer | begin training epoch 3
2022-01-28 13:12:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:13:25 | INFO | train_inner | {"epoch": 3, "update": 2.05, "loss": "3.516", "ntokens": "173705", "nsentences": "17.2", "wps": "337310", "ups": "1.94", "wpb": "173705", "bsz": "17.2", "num_updates": "3200", "lr": "0.00499944", "gnorm": "0.556", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "845"}
2022-01-28 13:13:50 | INFO | train_inner | {"epoch": 3, "update": 2.114, "loss": "3.495", "ntokens": "174592", "nsentences": "17.36", "wps": "725642", "ups": "4.16", "wpb": "174592", "bsz": "17.4", "num_updates": "3300", "lr": "0.00499939", "gnorm": "0.521", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "869"}
2022-01-28 13:14:14 | INFO | train_inner | {"epoch": 3, "update": 2.178, "loss": "3.499", "ntokens": "175011", "nsentences": "16.64", "wps": "722946", "ups": "4.13", "wpb": "175011", "bsz": "16.6", "num_updates": "3400", "lr": "0.00499935", "gnorm": "0.542", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "894"}
2022-01-28 13:14:38 | INFO | train_inner | {"epoch": 3, "update": 2.242, "loss": "3.485", "ntokens": "176167", "nsentences": "16.56", "wps": "724282", "ups": "4.11", "wpb": "176167", "bsz": "16.6", "num_updates": "3500", "lr": "0.0049993", "gnorm": "0.537", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "918"}
2022-01-28 13:15:02 | INFO | train_inner | {"epoch": 3, "update": 2.306, "loss": "3.49", "ntokens": "174878", "nsentences": "17.04", "wps": "720355", "ups": "4.12", "wpb": "174878", "bsz": "17", "num_updates": "3600", "lr": "0.00499926", "gnorm": "0.534", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "942"}
2022-01-28 13:15:27 | INFO | train_inner | {"epoch": 3, "update": 2.37, "loss": "3.456", "ntokens": "175181", "nsentences": "16.8", "wps": "721880", "ups": "4.12", "wpb": "175181", "bsz": "16.8", "num_updates": "3700", "lr": "0.00499921", "gnorm": "0.53", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "966"}
2022-01-28 13:15:51 | INFO | train_inner | {"epoch": 3, "update": 2.434, "loss": "3.459", "ntokens": "174125", "nsentences": "16.82", "wps": "717103", "ups": "4.12", "wpb": "174125", "bsz": "16.8", "num_updates": "3800", "lr": "0.00499916", "gnorm": "0.556", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "991"}
2022-01-28 13:16:15 | INFO | train_inner | {"epoch": 3, "update": 2.497, "loss": "3.454", "ntokens": "174613", "nsentences": "16.8", "wps": "721432", "ups": "4.13", "wpb": "174613", "bsz": "16.8", "num_updates": "3900", "lr": "0.00499911", "gnorm": "0.531", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1015"}
2022-01-28 13:16:39 | INFO | train_inner | {"epoch": 3, "update": 2.561, "loss": "3.439", "ntokens": "175797", "nsentences": "16.72", "wps": "723640", "ups": "4.12", "wpb": "175797", "bsz": "16.7", "num_updates": "4000", "lr": "0.00499905", "gnorm": "0.55", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1039"}
2022-01-28 13:17:04 | INFO | train_inner | {"epoch": 3, "update": 2.625, "loss": "3.436", "ntokens": "175486", "nsentences": "16.56", "wps": "721687", "ups": "4.11", "wpb": "175486", "bsz": "16.6", "num_updates": "4100", "lr": "0.004999", "gnorm": "0.561", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1064"}
2022-01-28 13:17:28 | INFO | train_inner | {"epoch": 3, "update": 2.689, "loss": "3.429", "ntokens": "175675", "nsentences": "17.12", "wps": "722753", "ups": "4.11", "wpb": "175675", "bsz": "17.1", "num_updates": "4200", "lr": "0.00499894", "gnorm": "0.545", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1088"}
2022-01-28 13:17:52 | INFO | train_inner | {"epoch": 3, "update": 2.753, "loss": "3.43", "ntokens": "176560", "nsentences": "16.72", "wps": "725215", "ups": "4.11", "wpb": "176560", "bsz": "16.7", "num_updates": "4300", "lr": "0.00499888", "gnorm": "0.574", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1112"}
2022-01-28 13:18:17 | INFO | train_inner | {"epoch": 3, "update": 2.817, "loss": "3.427", "ntokens": "175077", "nsentences": "17.2", "wps": "721679", "ups": "4.12", "wpb": "175077", "bsz": "17.2", "num_updates": "4400", "lr": "0.00499882", "gnorm": "0.571", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1137"}
2022-01-28 13:18:41 | INFO | train_inner | {"epoch": 3, "update": 2.881, "loss": "3.416", "ntokens": "174704", "nsentences": "16.56", "wps": "720884", "ups": "4.13", "wpb": "174704", "bsz": "16.6", "num_updates": "4500", "lr": "0.00499876", "gnorm": "0.563", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1161"}
2022-01-28 13:19:05 | INFO | train_inner | {"epoch": 3, "update": 2.944, "loss": "3.409", "ntokens": "175531", "nsentences": "17.36", "wps": "725698", "ups": "4.13", "wpb": "175531", "bsz": "17.4", "num_updates": "4600", "lr": "0.0049987", "gnorm": "0.569", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1185"}
2022-01-28 13:19:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:19:31 | INFO | valid | {"epoch": 3, "valid_loss": "3.385", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.62006e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "4687", "valid_best_loss": "3.385"}
2022-01-28 13:19:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4687 updates
2022-01-28 13:19:31 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:19:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 3 @ 4687 updates, score 3.385) (writing took 11.762583547271788 seconds)
2022-01-28 13:19:43 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-28 13:19:43 | INFO | train | {"epoch": 3, "train_loss": "3.453", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "673444", "train_ups": "3.84", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "4687", "train_lr": "0.00499865", "train_gnorm": "0.55", "train_loss_scale": "0.0625", "train_train_wall": "378", "train_gb_free": "20.4", "train_wall": "1223"}
2022-01-28 13:19:43 | INFO | fairseq.trainer | begin training epoch 4
2022-01-28 13:19:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:19:57 | INFO | train_inner | {"epoch": 4, "update": 3.008, "loss": "3.409", "ntokens": "175217", "nsentences": "16.72", "wps": "335799", "ups": "1.92", "wpb": "175217", "bsz": "16.7", "num_updates": "4700", "lr": "0.00499864", "gnorm": "0.548", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1237"}
2022-01-28 13:20:21 | INFO | train_inner | {"epoch": 4, "update": 3.072, "loss": "3.387", "ntokens": "174228", "nsentences": "16.96", "wps": "727843", "ups": "4.18", "wpb": "174228", "bsz": "17", "num_updates": "4800", "lr": "0.00499857", "gnorm": "0.572", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1261"}
2022-01-28 13:20:46 | INFO | train_inner | {"epoch": 4, "update": 3.136, "loss": "3.386", "ntokens": "175732", "nsentences": "17.52", "wps": "726502", "ups": "4.13", "wpb": "175732", "bsz": "17.5", "num_updates": "4900", "lr": "0.0049985", "gnorm": "0.576", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1285"}
2022-01-28 13:21:10 | INFO | train_inner | {"epoch": 4, "update": 3.2, "loss": "3.39", "ntokens": "175179", "nsentences": "16.48", "wps": "723812", "ups": "4.13", "wpb": "175179", "bsz": "16.5", "num_updates": "5000", "lr": "0.00499844", "gnorm": "0.61", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1310"}
2022-01-28 13:21:34 | INFO | train_inner | {"epoch": 4, "update": 3.264, "loss": "3.385", "ntokens": "175280", "nsentences": "16.96", "wps": "723094", "ups": "4.13", "wpb": "175280", "bsz": "17", "num_updates": "5100", "lr": "0.00499836", "gnorm": "0.583", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1334"}
2022-01-28 13:21:58 | INFO | train_inner | {"epoch": 4, "update": 3.328, "loss": "3.364", "ntokens": "175132", "nsentences": "17.6", "wps": "723864", "ups": "4.13", "wpb": "175132", "bsz": "17.6", "num_updates": "5200", "lr": "0.00499829", "gnorm": "0.562", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1358"}
2022-01-28 13:22:22 | INFO | train_inner | {"epoch": 4, "update": 3.391, "loss": "3.355", "ntokens": "176097", "nsentences": "17.28", "wps": "726190", "ups": "4.12", "wpb": "176097", "bsz": "17.3", "num_updates": "5300", "lr": "0.00499822", "gnorm": "0.575", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1382"}
2022-01-28 13:22:47 | INFO | train_inner | {"epoch": 4, "update": 3.455, "loss": "3.381", "ntokens": "175488", "nsentences": "16.64", "wps": "722190", "ups": "4.12", "wpb": "175488", "bsz": "16.6", "num_updates": "5400", "lr": "0.00499814", "gnorm": "0.594", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1407"}
2022-01-28 13:23:11 | INFO | train_inner | {"epoch": 4, "update": 3.519, "loss": "3.359", "ntokens": "176110", "nsentences": "17.28", "wps": "725754", "ups": "4.12", "wpb": "176110", "bsz": "17.3", "num_updates": "5500", "lr": "0.00499807", "gnorm": "0.6", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1431"}
2022-01-28 13:23:35 | INFO | train_inner | {"epoch": 4, "update": 3.583, "loss": "3.356", "ntokens": "175562", "nsentences": "16.64", "wps": "722602", "ups": "4.12", "wpb": "175562", "bsz": "16.6", "num_updates": "5600", "lr": "0.00499799", "gnorm": "0.614", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "19.9", "wall": "1455"}
2022-01-28 13:24:00 | INFO | train_inner | {"epoch": 4, "update": 3.647, "loss": "3.347", "ntokens": "174086", "nsentences": "16.8", "wps": "718176", "ups": "4.13", "wpb": "174086", "bsz": "16.8", "num_updates": "5700", "lr": "0.00499791", "gnorm": "0.592", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1479"}
2022-01-28 13:24:24 | INFO | train_inner | {"epoch": 4, "update": 3.711, "loss": "3.346", "ntokens": "175130", "nsentences": "16.4", "wps": "721704", "ups": "4.12", "wpb": "175130", "bsz": "16.4", "num_updates": "5800", "lr": "0.00499783", "gnorm": "0.632", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "1504"}
2022-01-28 13:24:48 | INFO | train_inner | {"epoch": 4, "update": 3.775, "loss": "3.352", "ntokens": "175527", "nsentences": "17.28", "wps": "724528", "ups": "4.13", "wpb": "175527", "bsz": "17.3", "num_updates": "5900", "lr": "0.00499775", "gnorm": "0.571", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1528"}
2022-01-28 13:25:12 | INFO | train_inner | {"epoch": 4, "update": 3.838, "loss": "3.325", "ntokens": "174912", "nsentences": "17.2", "wps": "720183", "ups": "4.12", "wpb": "174912", "bsz": "17.2", "num_updates": "6000", "lr": "0.00499766", "gnorm": "0.6", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "19.9", "wall": "1552"}
2022-01-28 13:25:37 | INFO | train_inner | {"epoch": 4, "update": 3.902, "loss": "3.343", "ntokens": "174880", "nsentences": "16.26", "wps": "720278", "ups": "4.12", "wpb": "174880", "bsz": "16.3", "num_updates": "6100", "lr": "0.00499758", "gnorm": "0.646", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1576"}
2022-01-28 13:26:01 | INFO | train_inner | {"epoch": 4, "update": 3.966, "loss": "3.326", "ntokens": "174699", "nsentences": "16.4", "wps": "719022", "ups": "4.12", "wpb": "174699", "bsz": "16.4", "num_updates": "6200", "lr": "0.00499749", "gnorm": "0.604", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1601"}
2022-01-28 13:26:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:26:18 | INFO | valid | {"epoch": 4, "valid_loss": "3.315", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.53066e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "6253", "valid_best_loss": "3.315"}
2022-01-28 13:26:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6253 updates
2022-01-28 13:26:18 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:26:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:26:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 4 @ 6253 updates, score 3.315) (writing took 11.465320487506688 seconds)
2022-01-28 13:26:30 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-28 13:26:30 | INFO | train | {"epoch": 4, "train_loss": "3.359", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "673778", "train_ups": "3.85", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "6253", "train_lr": "0.00499744", "train_gnorm": "0.596", "train_loss_scale": "0.0625", "train_train_wall": "377", "train_gb_free": "20.4", "train_wall": "1630"}
2022-01-28 13:26:30 | INFO | fairseq.trainer | begin training epoch 5
2022-01-28 13:26:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:26:52 | INFO | train_inner | {"epoch": 5, "update": 4.03, "loss": "3.32", "ntokens": "175156", "nsentences": "17.12", "wps": "340564", "ups": "1.94", "wpb": "175156", "bsz": "17.1", "num_updates": "6300", "lr": "0.0049974", "gnorm": "0.612", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1652"}
2022-01-28 13:27:16 | INFO | train_inner | {"epoch": 5, "update": 4.094, "loss": "3.313", "ntokens": "173820", "nsentences": "16.64", "wps": "723113", "ups": "4.16", "wpb": "173820", "bsz": "16.6", "num_updates": "6400", "lr": "0.00499731", "gnorm": "0.64", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.8", "wall": "1676"}
2022-01-28 13:27:41 | INFO | train_inner | {"epoch": 5, "update": 4.158, "loss": "3.321", "ntokens": "174064", "nsentences": "16.72", "wps": "722443", "ups": "4.15", "wpb": "174064", "bsz": "16.7", "num_updates": "6500", "lr": "0.00499722", "gnorm": "0.636", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1700"}
2022-01-28 13:28:05 | INFO | train_inner | {"epoch": 5, "update": 4.222, "loss": "3.313", "ntokens": "175029", "nsentences": "17.6", "wps": "725047", "ups": "4.14", "wpb": "175029", "bsz": "17.6", "num_updates": "6600", "lr": "0.00499712", "gnorm": "0.607", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1725"}
2022-01-28 13:28:29 | INFO | train_inner | {"epoch": 5, "update": 4.285, "loss": "3.311", "ntokens": "174945", "nsentences": "16.66", "wps": "723351", "ups": "4.13", "wpb": "174945", "bsz": "16.7", "num_updates": "6700", "lr": "0.00499703", "gnorm": "0.551", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1749"}
2022-01-28 13:28:53 | INFO | train_inner | {"epoch": 5, "update": 4.349, "loss": "3.32", "ntokens": "175916", "nsentences": "16.88", "wps": "727544", "ups": "4.14", "wpb": "175916", "bsz": "16.9", "num_updates": "6800", "lr": "0.00499693", "gnorm": "0.619", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1773"}
2022-01-28 13:29:17 | INFO | train_inner | {"epoch": 5, "update": 4.413, "loss": "3.299", "ntokens": "174588", "nsentences": "16.48", "wps": "723598", "ups": "4.14", "wpb": "174588", "bsz": "16.5", "num_updates": "6900", "lr": "0.00499684", "gnorm": "0.587", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1797"}
2022-01-28 13:29:41 | INFO | train_inner | {"epoch": 5, "update": 4.477, "loss": "3.299", "ntokens": "175336", "nsentences": "16.24", "wps": "723821", "ups": "4.13", "wpb": "175336", "bsz": "16.2", "num_updates": "7000", "lr": "0.00499674", "gnorm": "0.631", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1821"}
2022-01-28 13:30:06 | INFO | train_inner | {"epoch": 5, "update": 4.541, "loss": "3.301", "ntokens": "176374", "nsentences": "17.44", "wps": "728081", "ups": "4.13", "wpb": "176374", "bsz": "17.4", "num_updates": "7100", "lr": "0.00499663", "gnorm": "0.604", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1845"}
2022-01-28 13:30:30 | INFO | train_inner | {"epoch": 5, "update": 4.605, "loss": "3.3", "ntokens": "175144", "nsentences": "17.52", "wps": "726884", "ups": "4.15", "wpb": "175144", "bsz": "17.5", "num_updates": "7200", "lr": "0.00499653", "gnorm": "0.653", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1870"}
2022-01-28 13:30:54 | INFO | train_inner | {"epoch": 5, "update": 4.669, "loss": "3.295", "ntokens": "176405", "nsentences": "16.96", "wps": "728565", "ups": "4.13", "wpb": "176405", "bsz": "17", "num_updates": "7300", "lr": "0.00499643", "gnorm": "0.619", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1894"}
2022-01-28 13:31:18 | INFO | train_inner | {"epoch": 5, "update": 4.732, "loss": "3.293", "ntokens": "175392", "nsentences": "16.64", "wps": "724320", "ups": "4.13", "wpb": "175392", "bsz": "16.6", "num_updates": "7400", "lr": "0.00499632", "gnorm": "0.709", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1918"}
2022-01-28 13:31:42 | INFO | train_inner | {"epoch": 5, "update": 4.796, "loss": "3.287", "ntokens": "173892", "nsentences": "16.48", "wps": "719046", "ups": "4.14", "wpb": "173892", "bsz": "16.5", "num_updates": "7500", "lr": "0.00499621", "gnorm": "0.64", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1942"}
2022-01-28 13:32:07 | INFO | train_inner | {"epoch": 5, "update": 4.86, "loss": "3.297", "ntokens": "175959", "nsentences": "17.52", "wps": "727022", "ups": "4.13", "wpb": "175959", "bsz": "17.5", "num_updates": "7600", "lr": "0.00499611", "gnorm": "0.675", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1966"}
2022-01-28 13:32:31 | INFO | train_inner | {"epoch": 5, "update": 4.924, "loss": "3.306", "ntokens": "175887", "nsentences": "16.8", "wps": "729427", "ups": "4.15", "wpb": "175887", "bsz": "16.8", "num_updates": "7700", "lr": "0.00499599", "gnorm": "0.675", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "1991"}
2022-01-28 13:32:55 | INFO | train_inner | {"epoch": 5, "update": 4.988, "loss": "3.282", "ntokens": "174494", "nsentences": "16.48", "wps": "720757", "ups": "4.13", "wpb": "174494", "bsz": "16.5", "num_updates": "7800", "lr": "0.00499588", "gnorm": "0.684", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2015"}
2022-01-28 13:33:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:33:04 | INFO | valid | {"epoch": 5, "valid_loss": "3.278", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.62331e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "7819", "valid_best_loss": "3.278"}
2022-01-28 13:33:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7819 updates
2022-01-28 13:33:04 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:33:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:33:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 5 @ 7819 updates, score 3.278) (writing took 12.99911128077656 seconds)
2022-01-28 13:33:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-28 13:33:17 | INFO | train | {"epoch": 5, "train_loss": "3.302", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "673582", "train_ups": "3.85", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "7819", "train_lr": "0.00499586", "train_gnorm": "0.634", "train_loss_scale": "0.0625", "train_train_wall": "376", "train_gb_free": "20.5", "train_wall": "2037"}
2022-01-28 13:33:17 | INFO | fairseq.trainer | begin training epoch 6
2022-01-28 13:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:33:47 | INFO | train_inner | {"epoch": 6, "update": 5.052, "loss": "3.271", "ntokens": "175227", "nsentences": "16.56", "wps": "334848", "ups": "1.91", "wpb": "175227", "bsz": "16.6", "num_updates": "7900", "lr": "0.00499577", "gnorm": "0.612", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2067"}
2022-01-28 13:34:11 | INFO | train_inner | {"epoch": 6, "update": 5.116, "loss": "3.283", "ntokens": "174562", "nsentences": "17.76", "wps": "726554", "ups": "4.16", "wpb": "174562", "bsz": "17.8", "num_updates": "8000", "lr": "0.00499565", "gnorm": "0.673", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2091"}
2022-01-28 13:34:36 | INFO | train_inner | {"epoch": 6, "update": 5.179, "loss": "3.265", "ntokens": "175612", "nsentences": "16.4", "wps": "727420", "ups": "4.14", "wpb": "175612", "bsz": "16.4", "num_updates": "8100", "lr": "0.00499554", "gnorm": "0.666", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2115"}
2022-01-28 13:35:00 | INFO | train_inner | {"epoch": 6, "update": 5.243, "loss": "3.275", "ntokens": "175109", "nsentences": "16.24", "wps": "724235", "ups": "4.14", "wpb": "175109", "bsz": "16.2", "num_updates": "8200", "lr": "0.00499542", "gnorm": "0.669", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "2140"}
2022-01-28 13:35:24 | INFO | train_inner | {"epoch": 6, "update": 5.307, "loss": "3.288", "ntokens": "175987", "nsentences": "16.88", "wps": "728967", "ups": "4.14", "wpb": "175987", "bsz": "16.9", "num_updates": "8300", "lr": "0.0049953", "gnorm": "0.7", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "2164"}
2022-01-28 13:35:48 | INFO | train_inner | {"epoch": 6, "update": 5.371, "loss": "3.28", "ntokens": "174449", "nsentences": "16.48", "wps": "723340", "ups": "4.15", "wpb": "174449", "bsz": "16.5", "num_updates": "8400", "lr": "0.00499518", "gnorm": "0.718", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "2188"}
2022-01-28 13:36:12 | INFO | train_inner | {"epoch": 6, "update": 5.435, "loss": "3.266", "ntokens": "176450", "nsentences": "16.8", "wps": "728968", "ups": "4.13", "wpb": "176450", "bsz": "16.8", "num_updates": "8500", "lr": "0.00499506", "gnorm": "0.686", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "2212"}
2022-01-28 13:36:36 | INFO | train_inner | {"epoch": 6, "update": 5.499, "loss": "3.262", "ntokens": "173591", "nsentences": "16.96", "wps": "718732", "ups": "4.14", "wpb": "173591", "bsz": "17", "num_updates": "8600", "lr": "0.00499493", "gnorm": "0.644", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "2236"}
2022-01-28 13:36:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2022-01-28 13:37:01 | INFO | train_inner | {"epoch": 6, "update": 5.563, "loss": "3.264", "ntokens": "175500", "nsentences": "17.36", "wps": "717908", "ups": "4.09", "wpb": "175500", "bsz": "17.4", "num_updates": "8700", "lr": "0.00499481", "gnorm": "0.671", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2261"}
2022-01-28 13:37:25 | INFO | train_inner | {"epoch": 6, "update": 5.627, "loss": "3.278", "ntokens": "174103", "nsentences": "16.74", "wps": "720459", "ups": "4.14", "wpb": "174103", "bsz": "16.7", "num_updates": "8800", "lr": "0.00499468", "gnorm": "0.761", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2285"}
2022-01-28 13:37:49 | INFO | train_inner | {"epoch": 6, "update": 5.691, "loss": "3.255", "ntokens": "175240", "nsentences": "16.48", "wps": "723661", "ups": "4.13", "wpb": "175240", "bsz": "16.5", "num_updates": "8900", "lr": "0.00499455", "gnorm": "0.666", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2309"}
2022-01-28 13:38:13 | INFO | train_inner | {"epoch": 6, "update": 5.755, "loss": "3.268", "ntokens": "174586", "nsentences": "17.28", "wps": "723186", "ups": "4.14", "wpb": "174586", "bsz": "17.3", "num_updates": "9000", "lr": "0.00499442", "gnorm": "0.67", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2333"}
2022-01-28 13:38:37 | INFO | train_inner | {"epoch": 6, "update": 5.819, "loss": "3.247", "ntokens": "175450", "nsentences": "17.12", "wps": "727584", "ups": "4.15", "wpb": "175450", "bsz": "17.1", "num_updates": "9100", "lr": "0.00499429", "gnorm": "0.668", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2357"}
2022-01-28 13:39:02 | INFO | train_inner | {"epoch": 6, "update": 5.883, "loss": "3.248", "ntokens": "174923", "nsentences": "17.12", "wps": "722985", "ups": "4.13", "wpb": "174923", "bsz": "17.1", "num_updates": "9200", "lr": "0.00499415", "gnorm": "0.728", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2381"}
2022-01-28 13:39:26 | INFO | train_inner | {"epoch": 6, "update": 5.946, "loss": "3.257", "ntokens": "176510", "nsentences": "17.12", "wps": "729986", "ups": "4.14", "wpb": "176510", "bsz": "17.1", "num_updates": "9300", "lr": "0.00499402", "gnorm": "0.669", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2406"}
2022-01-28 13:39:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:39:51 | INFO | valid | {"epoch": 6, "valid_loss": "3.289", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.51824e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "9384", "valid_best_loss": "3.278"}
2022-01-28 13:39:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9384 updates
2022-01-28 13:39:51 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 13:39:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 13:39:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 6 @ 9384 updates, score 3.289) (writing took 4.26597388740629 seconds)
2022-01-28 13:39:55 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-28 13:39:55 | INFO | train | {"epoch": 6, "train_loss": "3.267", "train_ntokens": "175180", "train_nsentences": "16.8907", "train_wps": "688730", "train_ups": "3.93", "train_wpb": "175180", "train_bsz": "16.9", "train_num_updates": "9384", "train_lr": "0.0049939", "train_gnorm": "0.685", "train_loss_scale": "0.0625", "train_train_wall": "376", "train_gb_free": "20.4", "train_wall": "2435"}
2022-01-28 13:39:55 | INFO | fairseq.trainer | begin training epoch 7
2022-01-28 13:39:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:40:10 | INFO | train_inner | {"epoch": 7, "update": 6.01, "loss": "3.263", "ntokens": "175686", "nsentences": "16.8", "wps": "396684", "ups": "2.26", "wpb": "175686", "bsz": "16.8", "num_updates": "9400", "lr": "0.00499388", "gnorm": "0.735", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2450"}
2022-01-28 13:40:34 | INFO | train_inner | {"epoch": 7, "update": 6.074, "loss": "3.255", "ntokens": "175069", "nsentences": "17.52", "wps": "730352", "ups": "4.17", "wpb": "175069", "bsz": "17.5", "num_updates": "9500", "lr": "0.00499374", "gnorm": "0.671", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2474"}
2022-01-28 13:40:58 | INFO | train_inner | {"epoch": 7, "update": 6.138, "loss": "3.287", "ntokens": "174750", "nsentences": "16.88", "wps": "725624", "ups": "4.15", "wpb": "174750", "bsz": "16.9", "num_updates": "9600", "lr": "0.0049936", "gnorm": "0.769", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2498"}
2022-01-28 13:41:22 | INFO | train_inner | {"epoch": 7, "update": 6.202, "loss": "3.259", "ntokens": "175629", "nsentences": "16.88", "wps": "728630", "ups": "4.15", "wpb": "175629", "bsz": "16.9", "num_updates": "9700", "lr": "0.00499346", "gnorm": "0.714", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2522"}
2022-01-28 13:41:46 | INFO | train_inner | {"epoch": 7, "update": 6.266, "loss": "3.262", "ntokens": "174403", "nsentences": "16.72", "wps": "723571", "ups": "4.15", "wpb": "174403", "bsz": "16.7", "num_updates": "9800", "lr": "0.00499332", "gnorm": "0.738", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "2546"}
2022-01-28 13:42:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2022-01-28 13:42:11 | INFO | train_inner | {"epoch": 7, "update": 6.33, "loss": "3.25", "ntokens": "174697", "nsentences": "16.48", "wps": "716773", "ups": "4.1", "wpb": "174697", "bsz": "16.5", "num_updates": "9900", "lr": "0.00499317", "gnorm": "0.684", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2571"}
2022-01-28 13:42:35 | INFO | train_inner | {"epoch": 7, "update": 6.394, "loss": "3.223", "ntokens": "175395", "nsentences": "16.72", "wps": "727072", "ups": "4.15", "wpb": "175395", "bsz": "16.7", "num_updates": "10000", "lr": "0.00499303", "gnorm": "0.719", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.3", "wall": "2595"}
2022-01-28 13:42:59 | INFO | train_inner | {"epoch": 7, "update": 6.458, "loss": "3.243", "ntokens": "176709", "nsentences": "16.8", "wps": "732819", "ups": "4.15", "wpb": "176709", "bsz": "16.8", "num_updates": "10100", "lr": "0.00499288", "gnorm": "0.721", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2619"}
2022-01-28 13:43:23 | INFO | train_inner | {"epoch": 7, "update": 6.522, "loss": "3.228", "ntokens": "175158", "nsentences": "16.8", "wps": "724812", "ups": "4.14", "wpb": "175158", "bsz": "16.8", "num_updates": "10200", "lr": "0.00499273", "gnorm": "0.752", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2643"}
2022-01-28 13:43:47 | INFO | train_inner | {"epoch": 7, "update": 6.586, "loss": "3.237", "ntokens": "176011", "nsentences": "16.32", "wps": "728887", "ups": "4.14", "wpb": "176011", "bsz": "16.3", "num_updates": "10300", "lr": "0.00499258", "gnorm": "0.764", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2667"}
2022-01-28 13:44:12 | INFO | train_inner | {"epoch": 7, "update": 6.649, "loss": "3.24", "ntokens": "174688", "nsentences": "16.72", "wps": "722269", "ups": "4.13", "wpb": "174688", "bsz": "16.7", "num_updates": "10400", "lr": "0.00499243", "gnorm": "0.733", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2691"}
2022-01-28 13:44:36 | INFO | train_inner | {"epoch": 7, "update": 6.713, "loss": "3.255", "ntokens": "176242", "nsentences": "17.2", "wps": "730429", "ups": "4.14", "wpb": "176242", "bsz": "17.2", "num_updates": "10500", "lr": "0.00499228", "gnorm": "0.726", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2716"}
2022-01-28 13:45:00 | INFO | train_inner | {"epoch": 7, "update": 6.777, "loss": "3.25", "ntokens": "174147", "nsentences": "17.62", "wps": "724972", "ups": "4.16", "wpb": "174146", "bsz": "17.6", "num_updates": "10600", "lr": "0.00499212", "gnorm": "0.726", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2740"}
2022-01-28 13:45:24 | INFO | train_inner | {"epoch": 7, "update": 6.841, "loss": "3.235", "ntokens": "174945", "nsentences": "16.96", "wps": "725147", "ups": "4.15", "wpb": "174945", "bsz": "17", "num_updates": "10700", "lr": "0.00499196", "gnorm": "0.727", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.5", "wall": "2764"}
2022-01-28 13:45:48 | INFO | train_inner | {"epoch": 7, "update": 6.905, "loss": "3.225", "ntokens": "174876", "nsentences": "17.2", "wps": "724953", "ups": "4.15", "wpb": "174876", "bsz": "17.2", "num_updates": "10800", "lr": "0.00499181", "gnorm": "0.712", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21.1", "wall": "2788"}
2022-01-28 13:46:12 | INFO | train_inner | {"epoch": 7, "update": 6.969, "loss": "3.247", "ntokens": "174565", "nsentences": "16.56", "wps": "724600", "ups": "4.15", "wpb": "174565", "bsz": "16.6", "num_updates": "10900", "lr": "0.00499165", "gnorm": "0.746", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2812"}
2022-01-28 13:46:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:46:29 | INFO | valid | {"epoch": 7, "valid_loss": "3.233", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.46785e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "10949", "valid_best_loss": "3.233"}
2022-01-28 13:46:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10949 updates
2022-01-28 13:46:29 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:46:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:46:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 7 @ 10949 updates, score 3.233) (writing took 11.897429616190493 seconds)
2022-01-28 13:46:41 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-28 13:46:41 | INFO | train | {"epoch": 7, "train_loss": "3.246", "train_ntokens": "175180", "train_nsentences": "16.8907", "train_wps": "676437", "train_ups": "3.86", "train_wpb": "175180", "train_bsz": "16.9", "train_num_updates": "10949", "train_lr": "0.00499157", "train_gnorm": "0.726", "train_loss_scale": "0.0312", "train_train_wall": "376", "train_gb_free": "20.4", "train_wall": "2840"}
2022-01-28 13:46:41 | INFO | fairseq.trainer | begin training epoch 8
2022-01-28 13:46:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:47:03 | INFO | train_inner | {"epoch": 8, "update": 7.033, "loss": "3.231", "ntokens": "174848", "nsentences": "16.88", "wps": "341001", "ups": "1.95", "wpb": "174848", "bsz": "16.9", "num_updates": "11000", "lr": "0.00499148", "gnorm": "0.739", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2863"}
2022-01-28 13:47:27 | INFO | train_inner | {"epoch": 8, "update": 7.096, "loss": "3.242", "ntokens": "175735", "nsentences": "16.88", "wps": "734507", "ups": "4.18", "wpb": "175735", "bsz": "16.9", "num_updates": "11100", "lr": "0.00499132", "gnorm": "0.709", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2887"}
2022-01-28 13:47:51 | INFO | train_inner | {"epoch": 8, "update": 7.16, "loss": "3.246", "ntokens": "175096", "nsentences": "16.4", "wps": "727658", "ups": "4.16", "wpb": "175096", "bsz": "16.4", "num_updates": "11200", "lr": "0.00499116", "gnorm": "0.744", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2911"}
2022-01-28 13:48:16 | INFO | train_inner | {"epoch": 8, "update": 7.224, "loss": "3.255", "ntokens": "175237", "nsentences": "16.4", "wps": "729000", "ups": "4.16", "wpb": "175237", "bsz": "16.4", "num_updates": "11300", "lr": "0.00499099", "gnorm": "0.797", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2935"}
2022-01-28 13:48:40 | INFO | train_inner | {"epoch": 8, "update": 7.288, "loss": "3.308", "ntokens": "175422", "nsentences": "16.8", "wps": "729250", "ups": "4.16", "wpb": "175422", "bsz": "16.8", "num_updates": "11400", "lr": "0.00499082", "gnorm": "0.748", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2959"}
2022-01-28 13:49:04 | INFO | train_inner | {"epoch": 8, "update": 7.352, "loss": "3.271", "ntokens": "175231", "nsentences": "17.14", "wps": "725921", "ups": "4.14", "wpb": "175231", "bsz": "17.1", "num_updates": "11500", "lr": "0.00499065", "gnorm": "0.657", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "2984"}
2022-01-28 13:49:28 | INFO | train_inner | {"epoch": 8, "update": 7.416, "loss": "3.24", "ntokens": "175351", "nsentences": "16.56", "wps": "727033", "ups": "4.15", "wpb": "175351", "bsz": "16.6", "num_updates": "11600", "lr": "0.00499048", "gnorm": "0.76", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3008"}
2022-01-28 13:49:52 | INFO | train_inner | {"epoch": 8, "update": 7.48, "loss": "3.245", "ntokens": "176173", "nsentences": "16.72", "wps": "730767", "ups": "4.15", "wpb": "176173", "bsz": "16.7", "num_updates": "11700", "lr": "0.00499031", "gnorm": "0.816", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3032"}
2022-01-28 13:50:16 | INFO | train_inner | {"epoch": 8, "update": 7.543, "loss": "3.256", "ntokens": "173628", "nsentences": "16.72", "wps": "720434", "ups": "4.15", "wpb": "173628", "bsz": "16.7", "num_updates": "11800", "lr": "0.00499014", "gnorm": "0.808", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3056"}
2022-01-28 13:50:40 | INFO | train_inner | {"epoch": 8, "update": 7.607, "loss": "3.241", "ntokens": "174892", "nsentences": "16.8", "wps": "727813", "ups": "4.16", "wpb": "174892", "bsz": "16.8", "num_updates": "11900", "lr": "0.00498996", "gnorm": "0.783", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3080"}
2022-01-28 13:51:04 | INFO | train_inner | {"epoch": 8, "update": 7.671, "loss": "3.248", "ntokens": "175078", "nsentences": "17.36", "wps": "726214", "ups": "4.15", "wpb": "175078", "bsz": "17.4", "num_updates": "12000", "lr": "0.00498979", "gnorm": "0.772", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3104"}
2022-01-28 13:51:28 | INFO | train_inner | {"epoch": 8, "update": 7.735, "loss": "3.257", "ntokens": "172644", "nsentences": "17.28", "wps": "719715", "ups": "4.17", "wpb": "172644", "bsz": "17.3", "num_updates": "12100", "lr": "0.00498961", "gnorm": "0.808", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21.2", "wall": "3128"}
2022-01-28 13:51:52 | INFO | train_inner | {"epoch": 8, "update": 7.799, "loss": "3.245", "ntokens": "176579", "nsentences": "17.04", "wps": "731786", "ups": "4.14", "wpb": "176579", "bsz": "17", "num_updates": "12200", "lr": "0.00498943", "gnorm": "0.805", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.9", "wall": "3152"}
2022-01-28 13:52:16 | INFO | train_inner | {"epoch": 8, "update": 7.863, "loss": "3.259", "ntokens": "176057", "nsentences": "16.56", "wps": "730247", "ups": "4.15", "wpb": "176057", "bsz": "16.6", "num_updates": "12300", "lr": "0.00498925", "gnorm": "0.797", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3176"}
2022-01-28 13:52:41 | INFO | train_inner | {"epoch": 8, "update": 7.927, "loss": "3.269", "ntokens": "176362", "nsentences": "17.2", "wps": "732236", "ups": "4.15", "wpb": "176362", "bsz": "17.2", "num_updates": "12400", "lr": "0.00498906", "gnorm": "0.82", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.5", "wall": "3200"}
2022-01-28 13:53:05 | INFO | train_inner | {"epoch": 8, "update": 7.99, "loss": "3.232", "ntokens": "174650", "nsentences": "17.52", "wps": "724160", "ups": "4.15", "wpb": "174650", "bsz": "17.5", "num_updates": "12500", "lr": "0.00498888", "gnorm": "0.771", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3225"}
2022-01-28 13:53:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:53:13 | INFO | valid | {"epoch": 8, "valid_loss": "3.224", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.5419e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "12515", "valid_best_loss": "3.224"}
2022-01-28 13:53:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12515 updates
2022-01-28 13:53:13 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:53:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 13:53:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 8 @ 12515 updates, score 3.224) (writing took 11.468226242810488 seconds)
2022-01-28 13:53:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-28 13:53:25 | INFO | train | {"epoch": 8, "train_loss": "3.253", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679174", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "12515", "train_lr": "0.00498885", "train_gnorm": "0.772", "train_loss_scale": "0.0312", "train_train_wall": "375", "train_gb_free": "20.4", "train_wall": "3244"}
2022-01-28 13:53:25 | INFO | fairseq.trainer | begin training epoch 9
2022-01-28 13:53:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:53:56 | INFO | train_inner | {"epoch": 9, "update": 8.054, "loss": "3.245", "ntokens": "176696", "nsentences": "16.48", "wps": "345933", "ups": "1.96", "wpb": "176696", "bsz": "16.5", "num_updates": "12600", "lr": "0.00498869", "gnorm": "0.812", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3276"}
2022-01-28 13:54:20 | INFO | train_inner | {"epoch": 9, "update": 8.118, "loss": "3.231", "ntokens": "174056", "nsentences": "16.56", "wps": "727267", "ups": "4.18", "wpb": "174056", "bsz": "16.6", "num_updates": "12700", "lr": "0.00498851", "gnorm": "0.783", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3300"}
2022-01-28 13:54:44 | INFO | train_inner | {"epoch": 9, "update": 8.182, "loss": "3.259", "ntokens": "174911", "nsentences": "16.96", "wps": "728831", "ups": "4.17", "wpb": "174911", "bsz": "17", "num_updates": "12800", "lr": "0.00498832", "gnorm": "0.827", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21", "wall": "3324"}
2022-01-28 13:55:08 | INFO | train_inner | {"epoch": 9, "update": 8.246, "loss": "3.249", "ntokens": "176344", "nsentences": "17.12", "wps": "734657", "ups": "4.17", "wpb": "176344", "bsz": "17.1", "num_updates": "12900", "lr": "0.00498813", "gnorm": "0.77", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "3348"}
2022-01-28 13:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2022-01-28 13:55:32 | INFO | train_inner | {"epoch": 9, "update": 8.31, "loss": "3.225", "ntokens": "174672", "nsentences": "16.88", "wps": "720539", "ups": "4.13", "wpb": "174672", "bsz": "16.9", "num_updates": "13000", "lr": "0.00498793", "gnorm": "0.816", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.9", "wall": "3372"}
2022-01-28 13:55:56 | INFO | train_inner | {"epoch": 9, "update": 8.374, "loss": "3.273", "ntokens": "174384", "nsentences": "16.9", "wps": "725464", "ups": "4.16", "wpb": "174384", "bsz": "16.9", "num_updates": "13100", "lr": "0.00498774", "gnorm": "0.807", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3396"}
2022-01-28 13:56:20 | INFO | train_inner | {"epoch": 9, "update": 8.438, "loss": "3.308", "ntokens": "175007", "nsentences": "16.56", "wps": "726675", "ups": "4.15", "wpb": "175007", "bsz": "16.6", "num_updates": "13200", "lr": "0.00498755", "gnorm": "0.847", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3420"}
2022-01-28 13:56:44 | INFO | train_inner | {"epoch": 9, "update": 8.502, "loss": "3.248", "ntokens": "175571", "nsentences": "17.44", "wps": "731857", "ups": "4.17", "wpb": "175571", "bsz": "17.4", "num_updates": "13300", "lr": "0.00498735", "gnorm": "0.717", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3444"}
2022-01-28 13:57:08 | INFO | train_inner | {"epoch": 9, "update": 8.566, "loss": "3.23", "ntokens": "173318", "nsentences": "16.64", "wps": "719399", "ups": "4.15", "wpb": "173318", "bsz": "16.6", "num_updates": "13400", "lr": "0.00498715", "gnorm": "0.818", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.1", "wall": "3468"}
2022-01-28 13:57:32 | INFO | train_inner | {"epoch": 9, "update": 8.63, "loss": "3.219", "ntokens": "176149", "nsentences": "17.28", "wps": "730467", "ups": "4.15", "wpb": "176149", "bsz": "17.3", "num_updates": "13500", "lr": "0.00498695", "gnorm": "0.807", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3492"}
2022-01-28 13:57:56 | INFO | train_inner | {"epoch": 9, "update": 8.693, "loss": "3.231", "ntokens": "175060", "nsentences": "16.8", "wps": "726407", "ups": "4.15", "wpb": "175060", "bsz": "16.8", "num_updates": "13600", "lr": "0.00498675", "gnorm": "0.839", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.2", "wall": "3516"}
2022-01-28 13:58:21 | INFO | train_inner | {"epoch": 9, "update": 8.757, "loss": "3.249", "ntokens": "175031", "nsentences": "17.2", "wps": "726817", "ups": "4.15", "wpb": "175031", "bsz": "17.2", "num_updates": "13700", "lr": "0.00498655", "gnorm": "0.868", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3540"}
2022-01-28 13:58:45 | INFO | train_inner | {"epoch": 9, "update": 8.821, "loss": "3.271", "ntokens": "174802", "nsentences": "16.64", "wps": "726751", "ups": "4.16", "wpb": "174802", "bsz": "16.6", "num_updates": "13800", "lr": "0.00498634", "gnorm": "0.886", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3564"}
2022-01-28 13:59:09 | INFO | train_inner | {"epoch": 9, "update": 8.885, "loss": "3.244", "ntokens": "175596", "nsentences": "16.96", "wps": "728281", "ups": "4.15", "wpb": "175596", "bsz": "17", "num_updates": "13900", "lr": "0.00498614", "gnorm": "0.787", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3589"}
2022-01-28 13:59:33 | INFO | train_inner | {"epoch": 9, "update": 8.949, "loss": "3.281", "ntokens": "176196", "nsentences": "17.44", "wps": "730515", "ups": "4.15", "wpb": "176196", "bsz": "17.4", "num_updates": "14000", "lr": "0.00498593", "gnorm": "0.799", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3613"}
2022-01-28 13:59:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:59:57 | INFO | valid | {"epoch": 9, "valid_loss": "3.238", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.45281e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "14080", "valid_best_loss": "3.224"}
2022-01-28 13:59:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14080 updates
2022-01-28 13:59:57 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 14:00:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 14:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 9 @ 14080 updates, score 3.238) (writing took 4.274902182631195 seconds)
2022-01-28 14:00:01 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-28 14:00:01 | INFO | train | {"epoch": 9, "train_loss": "3.251", "train_ntokens": "175180", "train_nsentences": "16.8907", "train_wps": "691368", "train_ups": "3.95", "train_wpb": "175180", "train_bsz": "16.9", "train_num_updates": "14080", "train_lr": "0.00498576", "train_gnorm": "0.814", "train_loss_scale": "0.0156", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "3641"}
2022-01-28 14:00:01 | INFO | fairseq.trainer | begin training epoch 10
2022-01-28 14:00:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:00:17 | INFO | train_inner | {"epoch": 10, "update": 9.013, "loss": "3.233", "ntokens": "175253", "nsentences": "16.16", "wps": "397099", "ups": "2.27", "wpb": "175253", "bsz": "16.2", "num_updates": "14100", "lr": "0.00498572", "gnorm": "0.831", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3657"}
2022-01-28 14:00:41 | INFO | train_inner | {"epoch": 10, "update": 9.077, "loss": "3.311", "ntokens": "174500", "nsentences": "16.72", "wps": "730670", "ups": "4.19", "wpb": "174500", "bsz": "16.7", "num_updates": "14200", "lr": "0.00498551", "gnorm": "0.909", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3681"}
2022-01-28 14:01:05 | INFO | train_inner | {"epoch": 10, "update": 9.14, "loss": "3.279", "ntokens": "175027", "nsentences": "17.12", "wps": "731303", "ups": "4.18", "wpb": "175027", "bsz": "17.1", "num_updates": "14300", "lr": "0.0049853", "gnorm": "0.857", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3705"}
2022-01-28 14:01:29 | INFO | train_inner | {"epoch": 10, "update": 9.204, "loss": "3.284", "ntokens": "175747", "nsentences": "16.8", "wps": "730499", "ups": "4.16", "wpb": "175747", "bsz": "16.8", "num_updates": "14400", "lr": "0.00498508", "gnorm": "0.815", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3729"}
2022-01-28 14:01:53 | INFO | train_inner | {"epoch": 10, "update": 9.268, "loss": "3.237", "ntokens": "175222", "nsentences": "17.12", "wps": "730636", "ups": "4.17", "wpb": "175222", "bsz": "17.1", "num_updates": "14500", "lr": "0.00498487", "gnorm": "0.727", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3753"}
2022-01-28 14:02:17 | INFO | train_inner | {"epoch": 10, "update": 9.332, "loss": "3.24", "ntokens": "176124", "nsentences": "16.8", "wps": "731122", "ups": "4.15", "wpb": "176124", "bsz": "16.8", "num_updates": "14600", "lr": "0.00498465", "gnorm": "0.854", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3777"}
2022-01-28 14:02:41 | INFO | train_inner | {"epoch": 10, "update": 9.396, "loss": "3.227", "ntokens": "175537", "nsentences": "16.4", "wps": "729431", "ups": "4.16", "wpb": "175537", "bsz": "16.4", "num_updates": "14700", "lr": "0.00498443", "gnorm": "0.806", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3801"}
2022-01-28 14:03:05 | INFO | train_inner | {"epoch": 10, "update": 9.46, "loss": "3.26", "ntokens": "174816", "nsentences": "16.72", "wps": "727876", "ups": "4.16", "wpb": "174816", "bsz": "16.7", "num_updates": "14800", "lr": "0.00498421", "gnorm": "0.832", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "21.1", "wall": "3825"}
2022-01-28 14:03:29 | INFO | train_inner | {"epoch": 10, "update": 9.524, "loss": "3.277", "ntokens": "174829", "nsentences": "16.72", "wps": "727603", "ups": "4.16", "wpb": "174829", "bsz": "16.7", "num_updates": "14900", "lr": "0.00498399", "gnorm": "0.876", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.1", "wall": "3849"}
2022-01-28 14:03:53 | INFO | train_inner | {"epoch": 10, "update": 9.587, "loss": "3.255", "ntokens": "175568", "nsentences": "16.72", "wps": "728397", "ups": "4.15", "wpb": "175568", "bsz": "16.7", "num_updates": "15000", "lr": "0.00498377", "gnorm": "0.909", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3873"}
2022-01-28 14:04:17 | INFO | train_inner | {"epoch": 10, "update": 9.651, "loss": "3.282", "ntokens": "174651", "nsentences": "16.64", "wps": "725761", "ups": "4.16", "wpb": "174651", "bsz": "16.6", "num_updates": "15100", "lr": "0.00498354", "gnorm": "0.988", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3897"}
2022-01-28 14:04:41 | INFO | train_inner | {"epoch": 10, "update": 9.715, "loss": "3.252", "ntokens": "174324", "nsentences": "16.72", "wps": "723154", "ups": "4.15", "wpb": "174324", "bsz": "16.7", "num_updates": "15200", "lr": "0.00498332", "gnorm": "0.922", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3921"}
2022-01-28 14:05:06 | INFO | train_inner | {"epoch": 10, "update": 9.779, "loss": "3.245", "ntokens": "175178", "nsentences": "17.04", "wps": "727450", "ups": "4.15", "wpb": "175178", "bsz": "17", "num_updates": "15300", "lr": "0.00498309", "gnorm": "0.815", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3945"}
2022-01-28 14:05:30 | INFO | train_inner | {"epoch": 10, "update": 9.843, "loss": "3.237", "ntokens": "175829", "nsentences": "17.44", "wps": "729690", "ups": "4.15", "wpb": "175829", "bsz": "17.4", "num_updates": "15400", "lr": "0.00498286", "gnorm": "0.779", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3969"}
2022-01-28 14:05:54 | INFO | train_inner | {"epoch": 10, "update": 9.907, "loss": "3.209", "ntokens": "175500", "nsentences": "17.12", "wps": "729073", "ups": "4.15", "wpb": "175500", "bsz": "17.1", "num_updates": "15500", "lr": "0.00498263", "gnorm": "0.857", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "3994"}
2022-01-28 14:06:18 | INFO | train_inner | {"epoch": 10, "update": 9.971, "loss": "3.221", "ntokens": "175171", "nsentences": "17.6", "wps": "727963", "ups": "4.16", "wpb": "175171", "bsz": "17.6", "num_updates": "15600", "lr": "0.0049824", "gnorm": "0.821", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4018"}
2022-01-28 14:06:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:06:34 | INFO | valid | {"epoch": 10, "valid_loss": "3.326", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.53065e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "15646", "valid_best_loss": "3.224"}
2022-01-28 14:06:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15646 updates
2022-01-28 14:06:34 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 14:06:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 14:06:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 10 @ 15646 updates, score 3.326) (writing took 4.296427779830992 seconds)
2022-01-28 14:06:38 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-28 14:06:38 | INFO | train | {"epoch": 10, "train_loss": "3.253", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "691035", "train_ups": "3.94", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "15646", "train_lr": "0.00498229", "train_gnorm": "0.856", "train_loss_scale": "0.0156", "train_train_wall": "375", "train_gb_free": "20.4", "train_wall": "4038"}
2022-01-28 14:06:38 | INFO | fairseq.trainer | begin training epoch 11
2022-01-28 14:06:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:07:02 | INFO | train_inner | {"epoch": 11, "update": 10.034, "loss": "3.263", "ntokens": "174282", "nsentences": "16.98", "wps": "395635", "ups": "2.27", "wpb": "174282", "bsz": "17", "num_updates": "15700", "lr": "0.00498217", "gnorm": "0.924", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4062"}
2022-01-28 14:07:26 | INFO | train_inner | {"epoch": 11, "update": 10.098, "loss": "3.245", "ntokens": "173691", "nsentences": "16.64", "wps": "725966", "ups": "4.18", "wpb": "173691", "bsz": "16.6", "num_updates": "15800", "lr": "0.00498193", "gnorm": "0.819", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.6", "wall": "4086"}
2022-01-28 14:07:50 | INFO | train_inner | {"epoch": 11, "update": 10.162, "loss": "3.236", "ntokens": "176382", "nsentences": "16.72", "wps": "737018", "ups": "4.18", "wpb": "176382", "bsz": "16.7", "num_updates": "15900", "lr": "0.00498169", "gnorm": "0.839", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4110"}
2022-01-28 14:08:14 | INFO | train_inner | {"epoch": 11, "update": 10.226, "loss": "3.213", "ntokens": "174945", "nsentences": "17.52", "wps": "732153", "ups": "4.19", "wpb": "174945", "bsz": "17.5", "num_updates": "16000", "lr": "0.00498146", "gnorm": "0.862", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4133"}
2022-01-28 14:08:38 | INFO | train_inner | {"epoch": 11, "update": 10.29, "loss": "3.226", "ntokens": "174164", "nsentences": "17.54", "wps": "723425", "ups": "4.15", "wpb": "174164", "bsz": "17.5", "num_updates": "16100", "lr": "0.00498122", "gnorm": "0.851", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4158"}
2022-01-28 14:09:02 | INFO | train_inner | {"epoch": 11, "update": 10.354, "loss": "3.264", "ntokens": "175704", "nsentences": "16.56", "wps": "732193", "ups": "4.17", "wpb": "175704", "bsz": "16.6", "num_updates": "16200", "lr": "0.00498097", "gnorm": "0.917", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4182"}
2022-01-28 14:09:26 | INFO | train_inner | {"epoch": 11, "update": 10.418, "loss": "3.234", "ntokens": "175443", "nsentences": "17.04", "wps": "729251", "ups": "4.16", "wpb": "175443", "bsz": "17", "num_updates": "16300", "lr": "0.00498073", "gnorm": "0.779", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4206"}
2022-01-28 14:09:50 | INFO | train_inner | {"epoch": 11, "update": 10.481, "loss": "3.234", "ntokens": "175896", "nsentences": "16.16", "wps": "729676", "ups": "4.15", "wpb": "175896", "bsz": "16.2", "num_updates": "16400", "lr": "0.00498049", "gnorm": "0.834", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4230"}
2022-01-28 14:10:14 | INFO | train_inner | {"epoch": 11, "update": 10.545, "loss": "3.268", "ntokens": "175120", "nsentences": "16.8", "wps": "729372", "ups": "4.16", "wpb": "175120", "bsz": "16.8", "num_updates": "16500", "lr": "0.00498024", "gnorm": "0.917", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4254"}
2022-01-28 14:10:38 | INFO | train_inner | {"epoch": 11, "update": 10.609, "loss": "3.239", "ntokens": "175588", "nsentences": "16.64", "wps": "728868", "ups": "4.15", "wpb": "175588", "bsz": "16.6", "num_updates": "16600", "lr": "0.00497999", "gnorm": "0.881", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4278"}
2022-01-28 14:11:02 | INFO | train_inner | {"epoch": 11, "update": 10.673, "loss": "3.272", "ntokens": "175501", "nsentences": "16.8", "wps": "729843", "ups": "4.16", "wpb": "175501", "bsz": "16.8", "num_updates": "16700", "lr": "0.00497975", "gnorm": "0.951", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4302"}
2022-01-28 14:11:26 | INFO | train_inner | {"epoch": 11, "update": 10.737, "loss": "3.308", "ntokens": "175127", "nsentences": "16.72", "wps": "728615", "ups": "4.16", "wpb": "175127", "bsz": "16.7", "num_updates": "16800", "lr": "0.00497949", "gnorm": "0.942", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4326"}
2022-01-28 14:11:50 | INFO | train_inner | {"epoch": 11, "update": 10.801, "loss": "3.25", "ntokens": "174185", "nsentences": "16.88", "wps": "728708", "ups": "4.18", "wpb": "174185", "bsz": "16.9", "num_updates": "16900", "lr": "0.00497924", "gnorm": "0.888", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4350"}
2022-01-28 14:12:14 | INFO | train_inner | {"epoch": 11, "update": 10.865, "loss": "3.207", "ntokens": "176047", "nsentences": "17.04", "wps": "732199", "ups": "4.16", "wpb": "176047", "bsz": "17", "num_updates": "17000", "lr": "0.00497899", "gnorm": "0.818", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4374"}
2022-01-28 14:12:38 | INFO | train_inner | {"epoch": 11, "update": 10.928, "loss": "3.229", "ntokens": "175474", "nsentences": "16.96", "wps": "729166", "ups": "4.16", "wpb": "175474", "bsz": "17", "num_updates": "17100", "lr": "0.00497873", "gnorm": "0.917", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20", "wall": "4398"}
2022-01-28 14:13:02 | INFO | train_inner | {"epoch": 11, "update": 10.992, "loss": "3.25", "ntokens": "175084", "nsentences": "17.2", "wps": "728607", "ups": "4.16", "wpb": "175084", "bsz": "17.2", "num_updates": "17200", "lr": "0.00497848", "gnorm": "0.825", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20", "wall": "4422"}
2022-01-28 14:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:13:10 | INFO | valid | {"epoch": 11, "valid_loss": "3.192", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.57427e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "17212", "valid_best_loss": "3.192"}
2022-01-28 14:13:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 17212 updates
2022-01-28 14:13:10 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:13:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 11 @ 17212 updates, score 3.192) (writing took 11.421034036204219 seconds)
2022-01-28 14:13:21 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-28 14:13:21 | INFO | train | {"epoch": 11, "train_loss": "3.246", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680279", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "17212", "train_lr": "0.00497845", "train_gnorm": "0.866", "train_loss_scale": "0.0156", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "4441"}
2022-01-28 14:13:21 | INFO | fairseq.trainer | begin training epoch 12
2022-01-28 14:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:13:53 | INFO | train_inner | {"epoch": 12, "update": 11.056, "loss": "3.197", "ntokens": "175815", "nsentences": "16.96", "wps": "344439", "ups": "1.96", "wpb": "175815", "bsz": "17", "num_updates": "17300", "lr": "0.00497822", "gnorm": "0.813", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4473"}
2022-01-28 14:14:17 | INFO | train_inner | {"epoch": 12, "update": 11.12, "loss": "3.172", "ntokens": "176071", "nsentences": "16.72", "wps": "736726", "ups": "4.18", "wpb": "176071", "bsz": "16.7", "num_updates": "17400", "lr": "0.00497796", "gnorm": "0.837", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.5", "wall": "4497"}
2022-01-28 14:14:41 | INFO | train_inner | {"epoch": 12, "update": 11.184, "loss": "3.199", "ntokens": "175389", "nsentences": "16.4", "wps": "730447", "ups": "4.16", "wpb": "175389", "bsz": "16.4", "num_updates": "17500", "lr": "0.0049777", "gnorm": "0.817", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4521"}
2022-01-28 14:15:05 | INFO | train_inner | {"epoch": 12, "update": 11.248, "loss": "3.213", "ntokens": "176286", "nsentences": "16.64", "wps": "734433", "ups": "4.17", "wpb": "176286", "bsz": "16.6", "num_updates": "17600", "lr": "0.00497744", "gnorm": "0.887", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4545"}
2022-01-28 14:15:29 | INFO | train_inner | {"epoch": 12, "update": 11.312, "loss": "3.255", "ntokens": "175887", "nsentences": "16.48", "wps": "731374", "ups": "4.16", "wpb": "175887", "bsz": "16.5", "num_updates": "17700", "lr": "0.00497717", "gnorm": "0.962", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4569"}
2022-01-28 14:15:53 | INFO | train_inner | {"epoch": 12, "update": 11.375, "loss": "3.23", "ntokens": "174399", "nsentences": "16.88", "wps": "723913", "ups": "4.15", "wpb": "174399", "bsz": "16.9", "num_updates": "17800", "lr": "0.00497691", "gnorm": "0.892", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4593"}
2022-01-28 14:16:17 | INFO | train_inner | {"epoch": 12, "update": 11.439, "loss": "3.283", "ntokens": "173600", "nsentences": "16.8", "wps": "725110", "ups": "4.18", "wpb": "173600", "bsz": "16.8", "num_updates": "17900", "lr": "0.00497664", "gnorm": "1.098", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4617"}
2022-01-28 14:16:41 | INFO | train_inner | {"epoch": 12, "update": 11.503, "loss": "3.211", "ntokens": "174901", "nsentences": "16.32", "wps": "726552", "ups": "4.15", "wpb": "174901", "bsz": "16.3", "num_updates": "18000", "lr": "0.00497637", "gnorm": "0.846", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4641"}
2022-01-28 14:17:05 | INFO | train_inner | {"epoch": 12, "update": 11.567, "loss": "3.194", "ntokens": "175269", "nsentences": "16.72", "wps": "730256", "ups": "4.17", "wpb": "175269", "bsz": "16.7", "num_updates": "18100", "lr": "0.0049761", "gnorm": "0.842", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "21", "wall": "4665"}
2022-01-28 14:17:29 | INFO | train_inner | {"epoch": 12, "update": 11.631, "loss": "3.151", "ntokens": "174701", "nsentences": "17.04", "wps": "726494", "ups": "4.16", "wpb": "174701", "bsz": "17", "num_updates": "18200", "lr": "0.00497583", "gnorm": "0.749", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "21.2", "wall": "4689"}
2022-01-28 14:17:53 | INFO | train_inner | {"epoch": 12, "update": 11.695, "loss": "3.136", "ntokens": "175045", "nsentences": "17.12", "wps": "729301", "ups": "4.17", "wpb": "175045", "bsz": "17.1", "num_updates": "18300", "lr": "0.00497555", "gnorm": "0.849", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "21.1", "wall": "4713"}
2022-01-28 14:18:18 | INFO | train_inner | {"epoch": 12, "update": 11.759, "loss": "3.138", "ntokens": "175025", "nsentences": "17.3", "wps": "727788", "ups": "4.16", "wpb": "175025", "bsz": "17.3", "num_updates": "18400", "lr": "0.00497528", "gnorm": "0.863", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4737"}
2022-01-28 14:18:42 | INFO | train_inner | {"epoch": 12, "update": 11.822, "loss": "3.114", "ntokens": "174947", "nsentences": "17.6", "wps": "727818", "ups": "4.16", "wpb": "174947", "bsz": "17.6", "num_updates": "18500", "lr": "0.004975", "gnorm": "0.803", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4761"}
2022-01-28 14:19:06 | INFO | train_inner | {"epoch": 12, "update": 11.886, "loss": "3.085", "ntokens": "175752", "nsentences": "17.2", "wps": "732789", "ups": "4.17", "wpb": "175752", "bsz": "17.2", "num_updates": "18600", "lr": "0.00497472", "gnorm": "0.659", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4785"}
2022-01-28 14:19:30 | INFO | train_inner | {"epoch": 12, "update": 11.95, "loss": "3.057", "ntokens": "174938", "nsentences": "16.56", "wps": "728935", "ups": "4.17", "wpb": "174938", "bsz": "16.6", "num_updates": "18700", "lr": "0.00497444", "gnorm": "0.793", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4809"}
2022-01-28 14:19:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:19:53 | INFO | valid | {"epoch": 12, "valid_loss": "3.07", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.70023e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "18778", "valid_best_loss": "3.07"}
2022-01-28 14:19:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 18778 updates
2022-01-28 14:19:53 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:19:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:20:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 12 @ 18778 updates, score 3.07) (writing took 12.843158094212413 seconds)
2022-01-28 14:20:06 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-28 14:20:06 | INFO | train | {"epoch": 12, "train_loss": "3.17", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "678208", "train_ups": "3.87", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "18778", "train_lr": "0.00497422", "train_gnorm": "0.841", "train_loss_scale": "0.0156", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "4846"}
2022-01-28 14:20:06 | INFO | fairseq.trainer | begin training epoch 13
2022-01-28 14:20:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:20:22 | INFO | train_inner | {"epoch": 13, "update": 12.014, "loss": "3.068", "ntokens": "175462", "nsentences": "17.52", "wps": "333918", "ups": "1.9", "wpb": "175462", "bsz": "17.5", "num_updates": "18800", "lr": "0.00497416", "gnorm": "0.695", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4862"}
2022-01-28 14:20:46 | INFO | train_inner | {"epoch": 13, "update": 12.078, "loss": "3.033", "ntokens": "175951", "nsentences": "17.12", "wps": "737480", "ups": "4.19", "wpb": "175951", "bsz": "17.1", "num_updates": "18900", "lr": "0.00497388", "gnorm": "0.634", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4886"}
2022-01-28 14:21:10 | INFO | train_inner | {"epoch": 13, "update": 12.142, "loss": "2.993", "ntokens": "175472", "nsentences": "16.72", "wps": "733475", "ups": "4.18", "wpb": "175472", "bsz": "16.7", "num_updates": "19000", "lr": "0.0049736", "gnorm": "0.615", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4910"}
2022-01-28 14:21:34 | INFO | train_inner | {"epoch": 13, "update": 12.206, "loss": "2.988", "ntokens": "175352", "nsentences": "16.56", "wps": "730889", "ups": "4.17", "wpb": "175352", "bsz": "16.6", "num_updates": "19100", "lr": "0.00497331", "gnorm": "0.577", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4934"}
2022-01-28 14:21:58 | INFO | train_inner | {"epoch": 13, "update": 12.269, "loss": "2.973", "ntokens": "174820", "nsentences": "16.24", "wps": "725890", "ups": "4.15", "wpb": "174820", "bsz": "16.2", "num_updates": "19200", "lr": "0.00497302", "gnorm": "0.585", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4958"}
2022-01-28 14:22:22 | INFO | train_inner | {"epoch": 13, "update": 12.333, "loss": "2.969", "ntokens": "175806", "nsentences": "16.88", "wps": "728966", "ups": "4.15", "wpb": "175806", "bsz": "16.9", "num_updates": "19300", "lr": "0.00497273", "gnorm": "0.581", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "4982"}
2022-01-28 14:22:46 | INFO | train_inner | {"epoch": 13, "update": 12.397, "loss": "2.956", "ntokens": "174257", "nsentences": "17.28", "wps": "724462", "ups": "4.16", "wpb": "174257", "bsz": "17.3", "num_updates": "19400", "lr": "0.00497244", "gnorm": "0.525", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.6", "wall": "5006"}
2022-01-28 14:23:10 | INFO | train_inner | {"epoch": 13, "update": 12.461, "loss": "2.951", "ntokens": "174033", "nsentences": "17.14", "wps": "724938", "ups": "4.17", "wpb": "174033", "bsz": "17.1", "num_updates": "19500", "lr": "0.00497215", "gnorm": "0.581", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5030"}
2022-01-28 14:23:34 | INFO | train_inner | {"epoch": 13, "update": 12.525, "loss": "2.945", "ntokens": "175683", "nsentences": "17.84", "wps": "729185", "ups": "4.15", "wpb": "175683", "bsz": "17.8", "num_updates": "19600", "lr": "0.00497186", "gnorm": "0.557", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5054"}
2022-01-28 14:23:58 | INFO | train_inner | {"epoch": 13, "update": 12.589, "loss": "2.932", "ntokens": "175355", "nsentences": "16.64", "wps": "731719", "ups": "4.17", "wpb": "175355", "bsz": "16.6", "num_updates": "19700", "lr": "0.00497156", "gnorm": "0.589", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5078"}
2022-01-28 14:24:22 | INFO | train_inner | {"epoch": 13, "update": 12.653, "loss": "2.936", "ntokens": "173808", "nsentences": "16.72", "wps": "726512", "ups": "4.18", "wpb": "173808", "bsz": "16.7", "num_updates": "19800", "lr": "0.00497127", "gnorm": "0.573", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.8", "wall": "5102"}
2022-01-28 14:24:46 | INFO | train_inner | {"epoch": 13, "update": 12.716, "loss": "2.934", "ntokens": "173525", "nsentences": "16.96", "wps": "723202", "ups": "4.17", "wpb": "173525", "bsz": "17", "num_updates": "19900", "lr": "0.00497097", "gnorm": "0.585", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.7", "wall": "5126"}
2022-01-28 14:25:10 | INFO | train_inner | {"epoch": 13, "update": 12.78, "loss": "2.918", "ntokens": "175995", "nsentences": "16.96", "wps": "732930", "ups": "4.16", "wpb": "175995", "bsz": "17", "num_updates": "20000", "lr": "0.00497067", "gnorm": "0.563", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5150"}
2022-01-28 14:25:34 | INFO | train_inner | {"epoch": 13, "update": 12.844, "loss": "2.899", "ntokens": "175022", "nsentences": "16.8", "wps": "729295", "ups": "4.17", "wpb": "175022", "bsz": "16.8", "num_updates": "20100", "lr": "0.00497037", "gnorm": "0.531", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5174"}
2022-01-28 14:25:58 | INFO | train_inner | {"epoch": 13, "update": 12.908, "loss": "2.896", "ntokens": "175510", "nsentences": "16.64", "wps": "729750", "ups": "4.16", "wpb": "175510", "bsz": "16.6", "num_updates": "20200", "lr": "0.00497007", "gnorm": "0.56", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5198"}
2022-01-28 14:26:22 | INFO | train_inner | {"epoch": 13, "update": 12.972, "loss": "2.886", "ntokens": "176025", "nsentences": "16.8", "wps": "732846", "ups": "4.16", "wpb": "176025", "bsz": "16.8", "num_updates": "20300", "lr": "0.00496976", "gnorm": "0.508", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5222"}
2022-01-28 14:26:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:26:38 | INFO | valid | {"epoch": 13, "valid_loss": "2.901", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.62128e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "20344", "valid_best_loss": "2.901"}
2022-01-28 14:26:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20344 updates
2022-01-28 14:26:38 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:26:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:26:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 13 @ 20344 updates, score 2.901) (writing took 14.558573162183166 seconds)
2022-01-28 14:26:52 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-28 14:26:52 | INFO | train | {"epoch": 13, "train_loss": "2.947", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "675134", "train_ups": "3.85", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "20344", "train_lr": "0.00496963", "train_gnorm": "0.573", "train_loss_scale": "0.0156", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "5252"}
2022-01-28 14:26:52 | INFO | fairseq.trainer | begin training epoch 14
2022-01-28 14:26:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:27:17 | INFO | train_inner | {"epoch": 14, "update": 13.036, "loss": "2.881", "ntokens": "175370", "nsentences": "16.4", "wps": "321606", "ups": "1.83", "wpb": "175370", "bsz": "16.4", "num_updates": "20400", "lr": "0.00496946", "gnorm": "0.552", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5277"}
2022-01-28 14:27:41 | INFO | train_inner | {"epoch": 14, "update": 13.1, "loss": "2.866", "ntokens": "176341", "nsentences": "16.96", "wps": "739262", "ups": "4.19", "wpb": "176341", "bsz": "17", "num_updates": "20500", "lr": "0.00496915", "gnorm": "0.56", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.5", "wall": "5301"}
2022-01-28 14:28:05 | INFO | train_inner | {"epoch": 14, "update": 13.163, "loss": "2.867", "ntokens": "174358", "nsentences": "16.4", "wps": "727640", "ups": "4.17", "wpb": "174358", "bsz": "16.4", "num_updates": "20600", "lr": "0.00496884", "gnorm": "0.576", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.6", "wall": "5325"}
2022-01-28 14:28:29 | INFO | train_inner | {"epoch": 14, "update": 13.227, "loss": "2.858", "ntokens": "175645", "nsentences": "16.48", "wps": "730805", "ups": "4.16", "wpb": "175645", "bsz": "16.5", "num_updates": "20700", "lr": "0.00496853", "gnorm": "0.558", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5349"}
2022-01-28 14:28:53 | INFO | train_inner | {"epoch": 14, "update": 13.291, "loss": "2.846", "ntokens": "175451", "nsentences": "16.88", "wps": "730304", "ups": "4.16", "wpb": "175451", "bsz": "16.9", "num_updates": "20800", "lr": "0.00496822", "gnorm": "0.519", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5373"}
2022-01-28 14:29:17 | INFO | train_inner | {"epoch": 14, "update": 13.355, "loss": "2.848", "ntokens": "175735", "nsentences": "16.88", "wps": "729792", "ups": "4.15", "wpb": "175735", "bsz": "16.9", "num_updates": "20900", "lr": "0.00496791", "gnorm": "0.571", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5397"}
2022-01-28 14:29:41 | INFO | train_inner | {"epoch": 14, "update": 13.419, "loss": "2.849", "ntokens": "174025", "nsentences": "17.12", "wps": "724137", "ups": "4.16", "wpb": "174025", "bsz": "17.1", "num_updates": "21000", "lr": "0.00496759", "gnorm": "0.548", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5421"}
2022-01-28 14:30:05 | INFO | train_inner | {"epoch": 14, "update": 13.483, "loss": "2.839", "ntokens": "175426", "nsentences": "16.88", "wps": "729485", "ups": "4.16", "wpb": "175426", "bsz": "16.9", "num_updates": "21100", "lr": "0.00496728", "gnorm": "0.582", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "5445"}
2022-01-28 14:30:29 | INFO | train_inner | {"epoch": 14, "update": 13.547, "loss": "2.839", "ntokens": "176314", "nsentences": "17.04", "wps": "732411", "ups": "4.15", "wpb": "176314", "bsz": "17", "num_updates": "21200", "lr": "0.00496696", "gnorm": "0.578", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5469"}
2022-01-28 14:30:53 | INFO | train_inner | {"epoch": 14, "update": 13.61, "loss": "2.847", "ntokens": "174891", "nsentences": "17.04", "wps": "727088", "ups": "4.16", "wpb": "174891", "bsz": "17", "num_updates": "21300", "lr": "0.00496664", "gnorm": "0.578", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5493"}
2022-01-28 14:31:17 | INFO | train_inner | {"epoch": 14, "update": 13.674, "loss": "2.857", "ntokens": "174129", "nsentences": "16.8", "wps": "726498", "ups": "4.17", "wpb": "174129", "bsz": "16.8", "num_updates": "21400", "lr": "0.00496632", "gnorm": "0.596", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21.4", "wall": "5517"}
2022-01-28 14:31:41 | INFO | train_inner | {"epoch": 14, "update": 13.738, "loss": "2.834", "ntokens": "175997", "nsentences": "17.04", "wps": "730799", "ups": "4.15", "wpb": "175997", "bsz": "17", "num_updates": "21500", "lr": "0.004966", "gnorm": "0.555", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5541"}
2022-01-28 14:32:05 | INFO | train_inner | {"epoch": 14, "update": 13.802, "loss": "2.835", "ntokens": "173743", "nsentences": "17.7", "wps": "721987", "ups": "4.16", "wpb": "173743", "bsz": "17.7", "num_updates": "21600", "lr": "0.00496567", "gnorm": "0.581", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5565"}
2022-01-28 14:32:29 | INFO | train_inner | {"epoch": 14, "update": 13.866, "loss": "2.825", "ntokens": "174095", "nsentences": "16.96", "wps": "726629", "ups": "4.17", "wpb": "174095", "bsz": "17", "num_updates": "21700", "lr": "0.00496535", "gnorm": "0.587", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5589"}
2022-01-28 14:32:53 | INFO | train_inner | {"epoch": 14, "update": 13.93, "loss": "2.818", "ntokens": "176114", "nsentences": "16.72", "wps": "734373", "ups": "4.17", "wpb": "176114", "bsz": "16.7", "num_updates": "21800", "lr": "0.00496502", "gnorm": "0.558", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5613"}
2022-01-28 14:33:17 | INFO | train_inner | {"epoch": 14, "update": 13.994, "loss": "2.819", "ntokens": "175644", "nsentences": "16.96", "wps": "727142", "ups": "4.14", "wpb": "175644", "bsz": "17", "num_updates": "21900", "lr": "0.00496469", "gnorm": "0.575", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5637"}
2022-01-28 14:33:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:33:25 | INFO | valid | {"epoch": 14, "valid_loss": "2.805", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.59175e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "21910", "valid_best_loss": "2.805"}
2022-01-28 14:33:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21910 updates
2022-01-28 14:33:25 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:33:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:33:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 14 @ 21910 updates, score 2.805) (writing took 12.409527456387877 seconds)
2022-01-28 14:33:37 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-28 14:33:37 | INFO | train | {"epoch": 14, "train_loss": "2.844", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "677827", "train_ups": "3.87", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "21910", "train_lr": "0.00496466", "train_gnorm": "0.567", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "5657"}
2022-01-28 14:33:37 | INFO | fairseq.trainer | begin training epoch 15
2022-01-28 14:33:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:34:09 | INFO | train_inner | {"epoch": 15, "update": 14.057, "loss": "2.807", "ntokens": "175265", "nsentences": "16.96", "wps": "336955", "ups": "1.92", "wpb": "175265", "bsz": "17", "num_updates": "22000", "lr": "0.00496436", "gnorm": "0.601", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5689"}
2022-01-28 14:34:33 | INFO | train_inner | {"epoch": 15, "update": 14.121, "loss": "2.804", "ntokens": "175883", "nsentences": "16.88", "wps": "736178", "ups": "4.19", "wpb": "175883", "bsz": "16.9", "num_updates": "22100", "lr": "0.00496403", "gnorm": "0.626", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5713"}
2022-01-28 14:34:57 | INFO | train_inner | {"epoch": 15, "update": 14.185, "loss": "2.808", "ntokens": "174587", "nsentences": "16.8", "wps": "729086", "ups": "4.18", "wpb": "174587", "bsz": "16.8", "num_updates": "22200", "lr": "0.0049637", "gnorm": "0.593", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5737"}
2022-01-28 14:35:21 | INFO | train_inner | {"epoch": 15, "update": 14.249, "loss": "2.8", "ntokens": "175236", "nsentences": "17.04", "wps": "732107", "ups": "4.18", "wpb": "175236", "bsz": "17", "num_updates": "22300", "lr": "0.00496336", "gnorm": "0.6", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5761"}
2022-01-28 14:35:45 | INFO | train_inner | {"epoch": 15, "update": 14.313, "loss": "2.798", "ntokens": "175901", "nsentences": "16.48", "wps": "730772", "ups": "4.15", "wpb": "175901", "bsz": "16.5", "num_updates": "22400", "lr": "0.00496303", "gnorm": "0.576", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5785"}
2022-01-28 14:36:09 | INFO | train_inner | {"epoch": 15, "update": 14.377, "loss": "2.824", "ntokens": "174375", "nsentences": "17.38", "wps": "725034", "ups": "4.16", "wpb": "174376", "bsz": "17.4", "num_updates": "22500", "lr": "0.00496269", "gnorm": "0.612", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5809"}
2022-01-28 14:36:33 | INFO | train_inner | {"epoch": 15, "update": 14.441, "loss": "2.792", "ntokens": "174760", "nsentences": "17.28", "wps": "725516", "ups": "4.15", "wpb": "174760", "bsz": "17.3", "num_updates": "22600", "lr": "0.00496235", "gnorm": "0.566", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5833"}
2022-01-28 14:36:57 | INFO | train_inner | {"epoch": 15, "update": 14.504, "loss": "2.802", "ntokens": "175188", "nsentences": "16.48", "wps": "729968", "ups": "4.17", "wpb": "175188", "bsz": "16.5", "num_updates": "22700", "lr": "0.00496201", "gnorm": "0.584", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5857"}
2022-01-28 14:37:22 | INFO | train_inner | {"epoch": 15, "update": 14.568, "loss": "2.789", "ntokens": "174992", "nsentences": "17.2", "wps": "726203", "ups": "4.15", "wpb": "174992", "bsz": "17.2", "num_updates": "22800", "lr": "0.00496167", "gnorm": "0.606", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5881"}
2022-01-28 14:37:46 | INFO | train_inner | {"epoch": 15, "update": 14.632, "loss": "2.787", "ntokens": "175122", "nsentences": "16.72", "wps": "727217", "ups": "4.15", "wpb": "175122", "bsz": "16.7", "num_updates": "22900", "lr": "0.00496132", "gnorm": "0.573", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5905"}
2022-01-28 14:38:10 | INFO | train_inner | {"epoch": 15, "update": 14.696, "loss": "2.772", "ntokens": "174910", "nsentences": "16.56", "wps": "727548", "ups": "4.16", "wpb": "174910", "bsz": "16.6", "num_updates": "23000", "lr": "0.00496098", "gnorm": "0.548", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5930"}
2022-01-28 14:38:34 | INFO | train_inner | {"epoch": 15, "update": 14.76, "loss": "2.762", "ntokens": "173956", "nsentences": "17.28", "wps": "726310", "ups": "4.18", "wpb": "173956", "bsz": "17.3", "num_updates": "23100", "lr": "0.00496063", "gnorm": "0.542", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5953"}
2022-01-28 14:38:58 | INFO | train_inner | {"epoch": 15, "update": 14.824, "loss": "2.773", "ntokens": "175441", "nsentences": "16.64", "wps": "729784", "ups": "4.16", "wpb": "175441", "bsz": "16.6", "num_updates": "23200", "lr": "0.00496028", "gnorm": "0.564", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "5978"}
2022-01-28 14:39:22 | INFO | train_inner | {"epoch": 15, "update": 14.888, "loss": "2.756", "ntokens": "175439", "nsentences": "16.56", "wps": "729228", "ups": "4.16", "wpb": "175439", "bsz": "16.6", "num_updates": "23300", "lr": "0.00495993", "gnorm": "0.556", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.5", "wall": "6002"}
2022-01-28 14:39:46 | INFO | train_inner | {"epoch": 15, "update": 14.951, "loss": "2.758", "ntokens": "175880", "nsentences": "17.12", "wps": "731984", "ups": "4.16", "wpb": "175880", "bsz": "17.1", "num_updates": "23400", "lr": "0.00495958", "gnorm": "0.572", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6026"}
2022-01-28 14:40:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:40:09 | INFO | valid | {"epoch": 15, "valid_loss": "2.78", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.75153e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "23476", "valid_best_loss": "2.78"}
2022-01-28 14:40:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23476 updates
2022-01-28 14:40:09 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:40:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:40:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 15 @ 23476 updates, score 2.78) (writing took 12.278304075822234 seconds)
2022-01-28 14:40:21 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-28 14:40:21 | INFO | train | {"epoch": 15, "train_loss": "2.787", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "678787", "train_ups": "3.87", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "23476", "train_lr": "0.00495931", "train_gnorm": "0.581", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "6061"}
2022-01-28 14:40:21 | INFO | fairseq.trainer | begin training epoch 16
2022-01-28 14:40:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:40:38 | INFO | train_inner | {"epoch": 16, "update": 15.015, "loss": "2.766", "ntokens": "176349", "nsentences": "17.36", "wps": "338343", "ups": "1.92", "wpb": "176349", "bsz": "17.4", "num_updates": "23500", "lr": "0.00495923", "gnorm": "0.57", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6078"}
2022-01-28 14:41:02 | INFO | train_inner | {"epoch": 16, "update": 15.079, "loss": "2.753", "ntokens": "175699", "nsentences": "16.64", "wps": "736234", "ups": "4.19", "wpb": "175699", "bsz": "16.6", "num_updates": "23600", "lr": "0.00495887", "gnorm": "0.584", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6102"}
2022-01-28 14:41:26 | INFO | train_inner | {"epoch": 16, "update": 15.143, "loss": "2.749", "ntokens": "175135", "nsentences": "16.56", "wps": "731085", "ups": "4.17", "wpb": "175135", "bsz": "16.6", "num_updates": "23700", "lr": "0.00495852", "gnorm": "0.614", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6126"}
2022-01-28 14:41:50 | INFO | train_inner | {"epoch": 16, "update": 15.207, "loss": "2.761", "ntokens": "174430", "nsentences": "16.66", "wps": "728208", "ups": "4.17", "wpb": "174430", "bsz": "16.7", "num_updates": "23800", "lr": "0.00495816", "gnorm": "0.568", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6150"}
2022-01-28 14:42:14 | INFO | train_inner | {"epoch": 16, "update": 15.271, "loss": "2.749", "ntokens": "175626", "nsentences": "16.64", "wps": "731023", "ups": "4.16", "wpb": "175626", "bsz": "16.6", "num_updates": "23900", "lr": "0.0049578", "gnorm": "0.598", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6174"}
2022-01-28 14:42:38 | INFO | train_inner | {"epoch": 16, "update": 15.335, "loss": "2.752", "ntokens": "173736", "nsentences": "17.44", "wps": "723628", "ups": "4.17", "wpb": "173736", "bsz": "17.4", "num_updates": "24000", "lr": "0.00495744", "gnorm": "0.606", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6198"}
2022-01-28 14:43:02 | INFO | train_inner | {"epoch": 16, "update": 15.398, "loss": "2.747", "ntokens": "176110", "nsentences": "16.56", "wps": "731704", "ups": "4.15", "wpb": "176110", "bsz": "16.6", "num_updates": "24100", "lr": "0.00495708", "gnorm": "0.582", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6222"}
2022-01-28 14:43:26 | INFO | train_inner | {"epoch": 16, "update": 15.462, "loss": "2.752", "ntokens": "175081", "nsentences": "16.96", "wps": "730628", "ups": "4.17", "wpb": "175081", "bsz": "17", "num_updates": "24200", "lr": "0.00495672", "gnorm": "0.59", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.8", "wall": "6246"}
2022-01-28 14:43:50 | INFO | train_inner | {"epoch": 16, "update": 15.526, "loss": "2.75", "ntokens": "175643", "nsentences": "16.96", "wps": "730881", "ups": "4.16", "wpb": "175643", "bsz": "17", "num_updates": "24300", "lr": "0.00495635", "gnorm": "0.612", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6270"}
2022-01-28 14:44:14 | INFO | train_inner | {"epoch": 16, "update": 15.59, "loss": "2.736", "ntokens": "175744", "nsentences": "17.04", "wps": "730288", "ups": "4.16", "wpb": "175744", "bsz": "17", "num_updates": "24400", "lr": "0.00495598", "gnorm": "0.622", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6294"}
2022-01-28 14:44:38 | INFO | train_inner | {"epoch": 16, "update": 15.654, "loss": "2.737", "ntokens": "175313", "nsentences": "16.8", "wps": "730868", "ups": "4.17", "wpb": "175313", "bsz": "16.8", "num_updates": "24500", "lr": "0.00495562", "gnorm": "0.586", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.8", "wall": "6318"}
2022-01-28 14:45:02 | INFO | train_inner | {"epoch": 16, "update": 15.718, "loss": "2.733", "ntokens": "173463", "nsentences": "16.72", "wps": "725304", "ups": "4.18", "wpb": "173463", "bsz": "16.7", "num_updates": "24600", "lr": "0.00495525", "gnorm": "0.601", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6342"}
2022-01-28 14:45:26 | INFO | train_inner | {"epoch": 16, "update": 15.782, "loss": "2.725", "ntokens": "176123", "nsentences": "17.04", "wps": "734242", "ups": "4.17", "wpb": "176123", "bsz": "17", "num_updates": "24700", "lr": "0.00495488", "gnorm": "0.597", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6366"}
2022-01-28 14:45:50 | INFO | train_inner | {"epoch": 16, "update": 15.845, "loss": "2.724", "ntokens": "176182", "nsentences": "17.12", "wps": "733930", "ups": "4.17", "wpb": "176182", "bsz": "17.1", "num_updates": "24800", "lr": "0.0049545", "gnorm": "0.564", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.5", "wall": "6390"}
2022-01-28 14:46:14 | INFO | train_inner | {"epoch": 16, "update": 15.909, "loss": "2.722", "ntokens": "174820", "nsentences": "16.96", "wps": "725909", "ups": "4.15", "wpb": "174820", "bsz": "17", "num_updates": "24900", "lr": "0.00495413", "gnorm": "0.595", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.9", "wall": "6414"}
2022-01-28 14:46:38 | INFO | train_inner | {"epoch": 16, "update": 15.973, "loss": "2.718", "ntokens": "174808", "nsentences": "16.72", "wps": "726942", "ups": "4.16", "wpb": "174808", "bsz": "16.7", "num_updates": "25000", "lr": "0.00495375", "gnorm": "0.565", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6438"}
2022-01-28 14:46:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:46:53 | INFO | valid | {"epoch": 16, "valid_loss": "2.736", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.59712e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "25042", "valid_best_loss": "2.736"}
2022-01-28 14:46:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 25042 updates
2022-01-28 14:46:53 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:46:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:47:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 16 @ 25042 updates, score 2.736) (writing took 11.613538260571659 seconds)
2022-01-28 14:47:04 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-28 14:47:04 | INFO | train | {"epoch": 16, "train_loss": "2.74", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680182", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "25042", "train_lr": "0.0049536", "train_gnorm": "0.591", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "6464"}
2022-01-28 14:47:04 | INFO | fairseq.trainer | begin training epoch 17
2022-01-28 14:47:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:47:29 | INFO | train_inner | {"epoch": 17, "update": 16.037, "loss": "2.716", "ntokens": "174106", "nsentences": "16.56", "wps": "340765", "ups": "1.96", "wpb": "174106", "bsz": "16.6", "num_updates": "25100", "lr": "0.00495338", "gnorm": "0.573", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6489"}
2022-01-28 14:47:53 | INFO | train_inner | {"epoch": 17, "update": 16.101, "loss": "2.722", "ntokens": "175363", "nsentences": "16.72", "wps": "735083", "ups": "4.19", "wpb": "175363", "bsz": "16.7", "num_updates": "25200", "lr": "0.004953", "gnorm": "0.623", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21.3", "wall": "6513"}
2022-01-28 14:48:17 | INFO | train_inner | {"epoch": 17, "update": 16.165, "loss": "2.724", "ntokens": "176091", "nsentences": "16.64", "wps": "735028", "ups": "4.17", "wpb": "176091", "bsz": "16.6", "num_updates": "25300", "lr": "0.00495262", "gnorm": "0.601", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6537"}
2022-01-28 14:48:41 | INFO | train_inner | {"epoch": 17, "update": 16.229, "loss": "2.704", "ntokens": "175684", "nsentences": "16.4", "wps": "732962", "ups": "4.17", "wpb": "175684", "bsz": "16.4", "num_updates": "25400", "lr": "0.00495224", "gnorm": "0.584", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6561"}
2022-01-28 14:49:05 | INFO | train_inner | {"epoch": 17, "update": 16.292, "loss": "2.723", "ntokens": "174971", "nsentences": "17.92", "wps": "726088", "ups": "4.15", "wpb": "174971", "bsz": "17.9", "num_updates": "25500", "lr": "0.00495185", "gnorm": "0.602", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6585"}
2022-01-28 14:49:29 | INFO | train_inner | {"epoch": 17, "update": 16.356, "loss": "2.72", "ntokens": "175333", "nsentences": "16.8", "wps": "729201", "ups": "4.16", "wpb": "175333", "bsz": "16.8", "num_updates": "25600", "lr": "0.00495147", "gnorm": "0.63", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.3", "wall": "6609"}
2022-01-28 14:49:53 | INFO | train_inner | {"epoch": 17, "update": 16.42, "loss": "2.703", "ntokens": "175162", "nsentences": "16.56", "wps": "728613", "ups": "4.16", "wpb": "175162", "bsz": "16.6", "num_updates": "25700", "lr": "0.00495108", "gnorm": "0.571", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6633"}
2022-01-28 14:50:17 | INFO | train_inner | {"epoch": 17, "update": 16.484, "loss": "2.713", "ntokens": "175265", "nsentences": "17.28", "wps": "729196", "ups": "4.16", "wpb": "175265", "bsz": "17.3", "num_updates": "25800", "lr": "0.00495069", "gnorm": "0.566", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6657"}
2022-01-28 14:50:41 | INFO | train_inner | {"epoch": 17, "update": 16.548, "loss": "2.721", "ntokens": "174687", "nsentences": "16.56", "wps": "725370", "ups": "4.15", "wpb": "174687", "bsz": "16.6", "num_updates": "25900", "lr": "0.00495031", "gnorm": "0.592", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6681"}
2022-01-28 14:51:05 | INFO | train_inner | {"epoch": 17, "update": 16.612, "loss": "2.724", "ntokens": "176100", "nsentences": "17.44", "wps": "733059", "ups": "4.16", "wpb": "176100", "bsz": "17.4", "num_updates": "26000", "lr": "0.00494991", "gnorm": "0.604", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6705"}
2022-01-28 14:51:29 | INFO | train_inner | {"epoch": 17, "update": 16.676, "loss": "2.705", "ntokens": "174925", "nsentences": "16.64", "wps": "727092", "ups": "4.16", "wpb": "174925", "bsz": "16.6", "num_updates": "26100", "lr": "0.00494952", "gnorm": "0.619", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6729"}
2022-01-28 14:51:53 | INFO | train_inner | {"epoch": 17, "update": 16.739, "loss": "2.719", "ntokens": "175190", "nsentences": "17.44", "wps": "729768", "ups": "4.17", "wpb": "175190", "bsz": "17.4", "num_updates": "26200", "lr": "0.00494913", "gnorm": "0.568", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6753"}
2022-01-28 14:52:17 | INFO | train_inner | {"epoch": 17, "update": 16.803, "loss": "2.7", "ntokens": "175348", "nsentences": "16.56", "wps": "731219", "ups": "4.17", "wpb": "175348", "bsz": "16.6", "num_updates": "26300", "lr": "0.00494873", "gnorm": "0.573", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.6", "wall": "6777"}
2022-01-28 14:52:42 | INFO | train_inner | {"epoch": 17, "update": 16.867, "loss": "2.691", "ntokens": "176580", "nsentences": "16.8", "wps": "733702", "ups": "4.16", "wpb": "176580", "bsz": "16.8", "num_updates": "26400", "lr": "0.00494834", "gnorm": "0.568", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6801"}
2022-01-28 14:53:06 | INFO | train_inner | {"epoch": 17, "update": 16.931, "loss": "2.701", "ntokens": "174961", "nsentences": "16.88", "wps": "728641", "ups": "4.16", "wpb": "174961", "bsz": "16.9", "num_updates": "26500", "lr": "0.00494794", "gnorm": "0.615", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21.3", "wall": "6825"}
2022-01-28 14:53:30 | INFO | train_inner | {"epoch": 17, "update": 16.995, "loss": "2.693", "ntokens": "172757", "nsentences": "17.06", "wps": "713788", "ups": "4.13", "wpb": "172757", "bsz": "17.1", "num_updates": "26600", "lr": "0.00494754", "gnorm": "0.57", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6850"}
2022-01-28 14:53:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:53:36 | INFO | valid | {"epoch": 17, "valid_loss": "2.701", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.58643e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "26608", "valid_best_loss": "2.701"}
2022-01-28 14:53:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 26608 updates
2022-01-28 14:53:36 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:53:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 14:53:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 17 @ 26608 updates, score 2.701) (writing took 11.924456408247352 seconds)
2022-01-28 14:53:48 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-28 14:53:48 | INFO | train | {"epoch": 17, "train_loss": "2.711", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679438", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "26608", "train_lr": "0.00494751", "train_gnorm": "0.592", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "6868"}
2022-01-28 14:53:48 | INFO | fairseq.trainer | begin training epoch 18
2022-01-28 14:53:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:54:21 | INFO | train_inner | {"epoch": 18, "update": 17.059, "loss": "2.685", "ntokens": "175061", "nsentences": "16.8", "wps": "341916", "ups": "1.95", "wpb": "175061", "bsz": "16.8", "num_updates": "26700", "lr": "0.00494714", "gnorm": "0.61", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6901"}
2022-01-28 14:54:45 | INFO | train_inner | {"epoch": 18, "update": 17.123, "loss": "2.685", "ntokens": "174735", "nsentences": "16.96", "wps": "728778", "ups": "4.17", "wpb": "174735", "bsz": "17", "num_updates": "26800", "lr": "0.00494673", "gnorm": "0.612", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6925"}
2022-01-28 14:55:09 | INFO | train_inner | {"epoch": 18, "update": 17.186, "loss": "2.687", "ntokens": "174517", "nsentences": "17.04", "wps": "727428", "ups": "4.17", "wpb": "174517", "bsz": "17", "num_updates": "26900", "lr": "0.00494633", "gnorm": "0.562", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6949"}
2022-01-28 14:55:33 | INFO | train_inner | {"epoch": 18, "update": 17.25, "loss": "2.69", "ntokens": "175012", "nsentences": "16.56", "wps": "729822", "ups": "4.17", "wpb": "175012", "bsz": "16.6", "num_updates": "27000", "lr": "0.00494592", "gnorm": "0.6", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6973"}
2022-01-28 14:55:57 | INFO | train_inner | {"epoch": 18, "update": 17.314, "loss": "2.68", "ntokens": "175485", "nsentences": "16.98", "wps": "731910", "ups": "4.17", "wpb": "175485", "bsz": "17", "num_updates": "27100", "lr": "0.00494552", "gnorm": "0.571", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "6997"}
2022-01-28 14:56:21 | INFO | train_inner | {"epoch": 18, "update": 17.378, "loss": "2.677", "ntokens": "174965", "nsentences": "16.72", "wps": "727210", "ups": "4.16", "wpb": "174965", "bsz": "16.7", "num_updates": "27200", "lr": "0.00494511", "gnorm": "0.593", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7021"}
2022-01-28 14:56:45 | INFO | train_inner | {"epoch": 18, "update": 17.442, "loss": "2.695", "ntokens": "176214", "nsentences": "17.44", "wps": "730587", "ups": "4.15", "wpb": "176214", "bsz": "17.4", "num_updates": "27300", "lr": "0.0049447", "gnorm": "0.588", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7045"}
2022-01-28 14:57:09 | INFO | train_inner | {"epoch": 18, "update": 17.506, "loss": "2.671", "ntokens": "175392", "nsentences": "16.4", "wps": "729796", "ups": "4.16", "wpb": "175392", "bsz": "16.4", "num_updates": "27400", "lr": "0.00494428", "gnorm": "0.584", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7069"}
2022-01-28 14:57:33 | INFO | train_inner | {"epoch": 18, "update": 17.57, "loss": "2.67", "ntokens": "175590", "nsentences": "16.56", "wps": "728476", "ups": "4.15", "wpb": "175590", "bsz": "16.6", "num_updates": "27500", "lr": "0.00494387", "gnorm": "0.585", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21", "wall": "7093"}
2022-01-28 14:57:57 | INFO | train_inner | {"epoch": 18, "update": 17.633, "loss": "2.659", "ntokens": "176649", "nsentences": "16.96", "wps": "733134", "ups": "4.15", "wpb": "176649", "bsz": "17", "num_updates": "27600", "lr": "0.00494346", "gnorm": "0.59", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7117"}
2022-01-28 14:58:21 | INFO | train_inner | {"epoch": 18, "update": 17.697, "loss": "2.694", "ntokens": "175170", "nsentences": "17.36", "wps": "729921", "ups": "4.17", "wpb": "175170", "bsz": "17.4", "num_updates": "27700", "lr": "0.00494304", "gnorm": "0.639", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7141"}
2022-01-28 14:58:45 | INFO | train_inner | {"epoch": 18, "update": 17.761, "loss": "2.693", "ntokens": "173028", "nsentences": "17.2", "wps": "721051", "ups": "4.17", "wpb": "173028", "bsz": "17.2", "num_updates": "27800", "lr": "0.00494262", "gnorm": "0.626", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7165"}
2022-01-28 14:59:09 | INFO | train_inner | {"epoch": 18, "update": 17.825, "loss": "2.68", "ntokens": "176143", "nsentences": "16.88", "wps": "733295", "ups": "4.16", "wpb": "176143", "bsz": "16.9", "num_updates": "27900", "lr": "0.0049422", "gnorm": "0.61", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7189"}
2022-01-28 14:59:33 | INFO | train_inner | {"epoch": 18, "update": 17.889, "loss": "2.674", "ntokens": "174638", "nsentences": "16.56", "wps": "726376", "ups": "4.16", "wpb": "174638", "bsz": "16.6", "num_updates": "28000", "lr": "0.00494178", "gnorm": "0.618", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7213"}
2022-01-28 14:59:58 | INFO | train_inner | {"epoch": 18, "update": 17.953, "loss": "2.672", "ntokens": "175862", "nsentences": "17.36", "wps": "731234", "ups": "4.16", "wpb": "175862", "bsz": "17.4", "num_updates": "28100", "lr": "0.00494136", "gnorm": "0.596", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7237"}
2022-01-28 15:00:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:00:20 | INFO | valid | {"epoch": 18, "valid_loss": "2.693", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.74508e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "28174", "valid_best_loss": "2.693"}
2022-01-28 15:00:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 28174 updates
2022-01-28 15:00:20 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:00:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:00:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 18 @ 28174 updates, score 2.693) (writing took 11.408457763493061 seconds)
2022-01-28 15:00:31 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-28 15:00:31 | INFO | train | {"epoch": 18, "train_loss": "2.681", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680601", "train_ups": "3.89", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "28174", "train_lr": "0.00494105", "train_gnorm": "0.6", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "7271"}
2022-01-28 15:00:31 | INFO | fairseq.trainer | begin training epoch 19
2022-01-28 15:00:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:00:49 | INFO | train_inner | {"epoch": 19, "update": 18.017, "loss": "2.675", "ntokens": "174376", "nsentences": "16.56", "wps": "340804", "ups": "1.95", "wpb": "174376", "bsz": "16.6", "num_updates": "28200", "lr": "0.00494093", "gnorm": "0.612", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7289"}
2022-01-28 15:01:13 | INFO | train_inner | {"epoch": 19, "update": 18.08, "loss": "2.657", "ntokens": "174661", "nsentences": "16.72", "wps": "734029", "ups": "4.2", "wpb": "174661", "bsz": "16.7", "num_updates": "28300", "lr": "0.00494051", "gnorm": "0.616", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7312"}
2022-01-28 15:01:36 | INFO | train_inner | {"epoch": 19, "update": 18.144, "loss": "2.665", "ntokens": "175741", "nsentences": "16.8", "wps": "734222", "ups": "4.18", "wpb": "175741", "bsz": "16.8", "num_updates": "28400", "lr": "0.00494008", "gnorm": "0.626", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7336"}
2022-01-28 15:02:00 | INFO | train_inner | {"epoch": 19, "update": 18.208, "loss": "2.661", "ntokens": "174174", "nsentences": "17.2", "wps": "724749", "ups": "4.16", "wpb": "174174", "bsz": "17.2", "num_updates": "28500", "lr": "0.00493965", "gnorm": "0.596", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "19.9", "wall": "7360"}
2022-01-28 15:02:24 | INFO | train_inner | {"epoch": 19, "update": 18.272, "loss": "2.656", "ntokens": "174980", "nsentences": "17.2", "wps": "731041", "ups": "4.18", "wpb": "174980", "bsz": "17.2", "num_updates": "28600", "lr": "0.00493922", "gnorm": "0.578", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7384"}
2022-01-28 15:02:48 | INFO | train_inner | {"epoch": 19, "update": 18.336, "loss": "2.662", "ntokens": "174869", "nsentences": "17.2", "wps": "727502", "ups": "4.16", "wpb": "174869", "bsz": "17.2", "num_updates": "28700", "lr": "0.00493879", "gnorm": "0.558", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7408"}
2022-01-28 15:03:13 | INFO | train_inner | {"epoch": 19, "update": 18.4, "loss": "2.66", "ntokens": "175766", "nsentences": "16.72", "wps": "729549", "ups": "4.15", "wpb": "175766", "bsz": "16.7", "num_updates": "28800", "lr": "0.00493836", "gnorm": "0.611", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7432"}
2022-01-28 15:03:37 | INFO | train_inner | {"epoch": 19, "update": 18.464, "loss": "2.657", "ntokens": "174048", "nsentences": "16.72", "wps": "727707", "ups": "4.18", "wpb": "174048", "bsz": "16.7", "num_updates": "28900", "lr": "0.00493792", "gnorm": "0.596", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7456"}
2022-01-28 15:04:01 | INFO | train_inner | {"epoch": 19, "update": 18.527, "loss": "2.651", "ntokens": "175931", "nsentences": "16.72", "wps": "730502", "ups": "4.15", "wpb": "175931", "bsz": "16.7", "num_updates": "29000", "lr": "0.00493749", "gnorm": "0.564", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7480"}
2022-01-28 15:04:25 | INFO | train_inner | {"epoch": 19, "update": 18.591, "loss": "2.653", "ntokens": "175093", "nsentences": "16.8", "wps": "729507", "ups": "4.17", "wpb": "175093", "bsz": "16.8", "num_updates": "29100", "lr": "0.00493705", "gnorm": "0.591", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7504"}
2022-01-28 15:04:49 | INFO | train_inner | {"epoch": 19, "update": 18.655, "loss": "2.66", "ntokens": "174645", "nsentences": "16.98", "wps": "728098", "ups": "4.17", "wpb": "174645", "bsz": "17", "num_updates": "29200", "lr": "0.00493661", "gnorm": "0.594", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21.2", "wall": "7528"}
2022-01-28 15:05:13 | INFO | train_inner | {"epoch": 19, "update": 18.719, "loss": "2.658", "ntokens": "177051", "nsentences": "17.28", "wps": "735206", "ups": "4.15", "wpb": "177051", "bsz": "17.3", "num_updates": "29300", "lr": "0.00493617", "gnorm": "0.608", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "7553"}
2022-01-28 15:05:37 | INFO | train_inner | {"epoch": 19, "update": 18.783, "loss": "2.649", "ntokens": "175439", "nsentences": "16.64", "wps": "730168", "ups": "4.16", "wpb": "175439", "bsz": "16.6", "num_updates": "29400", "lr": "0.00493573", "gnorm": "0.619", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7577"}
2022-01-28 15:06:01 | INFO | train_inner | {"epoch": 19, "update": 18.847, "loss": "2.654", "ntokens": "174996", "nsentences": "16.32", "wps": "727151", "ups": "4.16", "wpb": "174996", "bsz": "16.3", "num_updates": "29500", "lr": "0.00493529", "gnorm": "0.59", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.9", "wall": "7601"}
2022-01-28 15:06:25 | INFO | train_inner | {"epoch": 19, "update": 18.911, "loss": "2.66", "ntokens": "174142", "nsentences": "17.04", "wps": "725051", "ups": "4.16", "wpb": "174142", "bsz": "17", "num_updates": "29600", "lr": "0.00493484", "gnorm": "0.619", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7625"}
2022-01-28 15:06:49 | INFO | train_inner | {"epoch": 19, "update": 18.974, "loss": "2.679", "ntokens": "175725", "nsentences": "16.88", "wps": "730943", "ups": "4.16", "wpb": "175725", "bsz": "16.9", "num_updates": "29700", "lr": "0.00493439", "gnorm": "0.628", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7649"}
2022-01-28 15:06:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:07:03 | INFO | valid | {"epoch": 19, "valid_loss": "2.657", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.52313e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "29740", "valid_best_loss": "2.657"}
2022-01-28 15:07:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 29740 updates
2022-01-28 15:07:03 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:07:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 19 @ 29740 updates, score 2.657) (writing took 12.009818895719945 seconds)
2022-01-28 15:07:15 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-28 15:07:15 | INFO | train | {"epoch": 19, "train_loss": "2.658", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "678954", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "29740", "train_lr": "0.00493421", "train_gnorm": "0.6", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "7675"}
2022-01-28 15:07:15 | INFO | fairseq.trainer | begin training epoch 20
2022-01-28 15:07:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:07:41 | INFO | train_inner | {"epoch": 20, "update": 19.038, "loss": "2.641", "ntokens": "175698", "nsentences": "16.72", "wps": "338367", "ups": "1.93", "wpb": "175698", "bsz": "16.7", "num_updates": "29800", "lr": "0.00493395", "gnorm": "0.61", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7701"}
2022-01-28 15:08:05 | INFO | train_inner | {"epoch": 20, "update": 19.102, "loss": "2.649", "ntokens": "174506", "nsentences": "16.82", "wps": "732918", "ups": "4.2", "wpb": "174506", "bsz": "16.8", "num_updates": "29900", "lr": "0.0049335", "gnorm": "0.592", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7724"}
2022-01-28 15:08:29 | INFO | train_inner | {"epoch": 20, "update": 19.166, "loss": "2.639", "ntokens": "174602", "nsentences": "16.8", "wps": "728874", "ups": "4.17", "wpb": "174602", "bsz": "16.8", "num_updates": "30000", "lr": "0.00493304", "gnorm": "0.623", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "7748"}
2022-01-28 15:08:53 | INFO | train_inner | {"epoch": 20, "update": 19.23, "loss": "2.637", "ntokens": "176146", "nsentences": "16.64", "wps": "734823", "ups": "4.17", "wpb": "176146", "bsz": "16.6", "num_updates": "30100", "lr": "0.00493259", "gnorm": "0.61", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7772"}
2022-01-28 15:09:17 | INFO | train_inner | {"epoch": 20, "update": 19.294, "loss": "2.647", "ntokens": "175661", "nsentences": "16.72", "wps": "731278", "ups": "4.16", "wpb": "175661", "bsz": "16.7", "num_updates": "30200", "lr": "0.00493214", "gnorm": "0.642", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7796"}
2022-01-28 15:09:41 | INFO | train_inner | {"epoch": 20, "update": 19.358, "loss": "2.644", "ntokens": "175104", "nsentences": "17.52", "wps": "731976", "ups": "4.18", "wpb": "175104", "bsz": "17.5", "num_updates": "30300", "lr": "0.00493168", "gnorm": "0.607", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7820"}
2022-01-28 15:10:05 | INFO | train_inner | {"epoch": 20, "update": 19.421, "loss": "2.625", "ntokens": "174390", "nsentences": "16.56", "wps": "726567", "ups": "4.17", "wpb": "174390", "bsz": "16.6", "num_updates": "30400", "lr": "0.00493123", "gnorm": "0.573", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7844"}
2022-01-28 15:10:29 | INFO | train_inner | {"epoch": 20, "update": 19.485, "loss": "2.64", "ntokens": "176785", "nsentences": "17.12", "wps": "733053", "ups": "4.15", "wpb": "176785", "bsz": "17.1", "num_updates": "30500", "lr": "0.00493077", "gnorm": "0.613", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7868"}
2022-01-28 15:10:53 | INFO | train_inner | {"epoch": 20, "update": 19.549, "loss": "2.646", "ntokens": "173872", "nsentences": "17.52", "wps": "723131", "ups": "4.16", "wpb": "173872", "bsz": "17.5", "num_updates": "30600", "lr": "0.00493031", "gnorm": "0.58", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7893"}
2022-01-28 15:11:17 | INFO | train_inner | {"epoch": 20, "update": 19.613, "loss": "2.634", "ntokens": "175028", "nsentences": "16.48", "wps": "727655", "ups": "4.16", "wpb": "175028", "bsz": "16.5", "num_updates": "30700", "lr": "0.00492984", "gnorm": "0.621", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7917"}
2022-01-28 15:11:41 | INFO | train_inner | {"epoch": 20, "update": 19.677, "loss": "2.628", "ntokens": "176025", "nsentences": "17.44", "wps": "732240", "ups": "4.16", "wpb": "176025", "bsz": "17.4", "num_updates": "30800", "lr": "0.00492938", "gnorm": "0.572", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7941"}
2022-01-28 15:12:05 | INFO | train_inner | {"epoch": 20, "update": 19.741, "loss": "2.625", "ntokens": "174828", "nsentences": "16.24", "wps": "725726", "ups": "4.15", "wpb": "174828", "bsz": "16.2", "num_updates": "30900", "lr": "0.00492892", "gnorm": "0.605", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7965"}
2022-01-28 15:12:29 | INFO | train_inner | {"epoch": 20, "update": 19.805, "loss": "2.637", "ntokens": "175492", "nsentences": "16.72", "wps": "731044", "ups": "4.17", "wpb": "175492", "bsz": "16.7", "num_updates": "31000", "lr": "0.00492845", "gnorm": "0.611", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "7989"}
2022-01-28 15:12:53 | INFO | train_inner | {"epoch": 20, "update": 19.868, "loss": "2.618", "ntokens": "175548", "nsentences": "16.32", "wps": "729269", "ups": "4.15", "wpb": "175548", "bsz": "16.3", "num_updates": "31100", "lr": "0.00492798", "gnorm": "0.595", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8013"}
2022-01-28 15:13:17 | INFO | train_inner | {"epoch": 20, "update": 19.932, "loss": "2.622", "ntokens": "174619", "nsentences": "16.64", "wps": "725423", "ups": "4.15", "wpb": "174619", "bsz": "16.6", "num_updates": "31200", "lr": "0.00492751", "gnorm": "0.567", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8037"}
2022-01-28 15:13:41 | INFO | train_inner | {"epoch": 20, "update": 19.996, "loss": "2.636", "ntokens": "174857", "nsentences": "17.92", "wps": "721113", "ups": "4.12", "wpb": "174857", "bsz": "17.9", "num_updates": "31300", "lr": "0.00492704", "gnorm": "0.585", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8061"}
2022-01-28 15:13:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:13:47 | INFO | valid | {"epoch": 20, "valid_loss": "2.645", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.60593e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "31306", "valid_best_loss": "2.645"}
2022-01-28 15:13:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 31306 updates
2022-01-28 15:13:47 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:13:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:13:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 20 @ 31306 updates, score 2.645) (writing took 11.55380854010582 seconds)
2022-01-28 15:13:59 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-28 15:13:59 | INFO | train | {"epoch": 20, "train_loss": "2.635", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679621", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "31306", "train_lr": "0.00492701", "train_gnorm": "0.6", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "8079"}
2022-01-28 15:13:59 | INFO | fairseq.trainer | begin training epoch 21
2022-01-28 15:13:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:14:32 | INFO | train_inner | {"epoch": 21, "update": 20.06, "loss": "2.618", "ntokens": "175479", "nsentences": "17.36", "wps": "343678", "ups": "1.96", "wpb": "175479", "bsz": "17.4", "num_updates": "31400", "lr": "0.00492657", "gnorm": "0.614", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8112"}
2022-01-28 15:14:56 | INFO | train_inner | {"epoch": 21, "update": 20.124, "loss": "2.623", "ntokens": "173847", "nsentences": "16.72", "wps": "727296", "ups": "4.18", "wpb": "173847", "bsz": "16.7", "num_updates": "31500", "lr": "0.0049261", "gnorm": "0.613", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8136"}
2022-01-28 15:15:20 | INFO | train_inner | {"epoch": 21, "update": 20.188, "loss": "2.611", "ntokens": "174056", "nsentences": "16.64", "wps": "726094", "ups": "4.17", "wpb": "174056", "bsz": "16.6", "num_updates": "31600", "lr": "0.00492562", "gnorm": "0.601", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8160"}
2022-01-28 15:15:44 | INFO | train_inner | {"epoch": 21, "update": 20.252, "loss": "2.625", "ntokens": "175301", "nsentences": "16.48", "wps": "728315", "ups": "4.15", "wpb": "175301", "bsz": "16.5", "num_updates": "31700", "lr": "0.00492515", "gnorm": "0.591", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8184"}
2022-01-28 15:16:08 | INFO | train_inner | {"epoch": 21, "update": 20.315, "loss": "2.624", "ntokens": "174681", "nsentences": "17.06", "wps": "725986", "ups": "4.16", "wpb": "174681", "bsz": "17.1", "num_updates": "31800", "lr": "0.00492467", "gnorm": "0.624", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8208"}
2022-01-28 15:16:33 | INFO | train_inner | {"epoch": 21, "update": 20.379, "loss": "2.625", "ntokens": "175606", "nsentences": "17.28", "wps": "729311", "ups": "4.15", "wpb": "175606", "bsz": "17.3", "num_updates": "31900", "lr": "0.00492419", "gnorm": "0.6", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8232"}
2022-01-28 15:16:57 | INFO | train_inner | {"epoch": 21, "update": 20.443, "loss": "2.621", "ntokens": "175536", "nsentences": "16.72", "wps": "731766", "ups": "4.17", "wpb": "175536", "bsz": "16.7", "num_updates": "32000", "lr": "0.00492371", "gnorm": "0.582", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8256"}
2022-01-28 15:17:21 | INFO | train_inner | {"epoch": 21, "update": 20.507, "loss": "2.603", "ntokens": "175375", "nsentences": "16.72", "wps": "727756", "ups": "4.15", "wpb": "175375", "bsz": "16.7", "num_updates": "32100", "lr": "0.00492322", "gnorm": "0.617", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8280"}
2022-01-28 15:17:45 | INFO | train_inner | {"epoch": 21, "update": 20.571, "loss": "2.616", "ntokens": "175335", "nsentences": "17.12", "wps": "730272", "ups": "4.16", "wpb": "175335", "bsz": "17.1", "num_updates": "32200", "lr": "0.00492274", "gnorm": "0.597", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8305"}
2022-01-28 15:18:09 | INFO | train_inner | {"epoch": 21, "update": 20.635, "loss": "2.608", "ntokens": "175901", "nsentences": "16.96", "wps": "729495", "ups": "4.15", "wpb": "175901", "bsz": "17", "num_updates": "32300", "lr": "0.00492225", "gnorm": "0.565", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8329"}
2022-01-28 15:18:33 | INFO | train_inner | {"epoch": 21, "update": 20.699, "loss": "2.618", "ntokens": "174867", "nsentences": "16.72", "wps": "728035", "ups": "4.16", "wpb": "174867", "bsz": "16.7", "num_updates": "32400", "lr": "0.00492177", "gnorm": "0.624", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8353"}
2022-01-28 15:18:57 | INFO | train_inner | {"epoch": 21, "update": 20.762, "loss": "2.613", "ntokens": "175456", "nsentences": "17.04", "wps": "730573", "ups": "4.16", "wpb": "175456", "bsz": "17", "num_updates": "32500", "lr": "0.00492128", "gnorm": "0.617", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8377"}
2022-01-28 15:19:21 | INFO | train_inner | {"epoch": 21, "update": 20.826, "loss": "2.624", "ntokens": "176554", "nsentences": "17.2", "wps": "732860", "ups": "4.15", "wpb": "176554", "bsz": "17.2", "num_updates": "32600", "lr": "0.00492079", "gnorm": "0.612", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8401"}
2022-01-28 15:19:45 | INFO | train_inner | {"epoch": 21, "update": 20.89, "loss": "2.609", "ntokens": "174813", "nsentences": "16.24", "wps": "726850", "ups": "4.16", "wpb": "174813", "bsz": "16.2", "num_updates": "32700", "lr": "0.0049203", "gnorm": "0.594", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8425"}
2022-01-28 15:20:09 | INFO | train_inner | {"epoch": 21, "update": 20.954, "loss": "2.618", "ntokens": "175337", "nsentences": "17.04", "wps": "728712", "ups": "4.16", "wpb": "175337", "bsz": "17", "num_updates": "32800", "lr": "0.0049198", "gnorm": "0.617", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.9", "wall": "8449"}
2022-01-28 15:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:20:31 | INFO | valid | {"epoch": 21, "valid_loss": "2.646", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.48528e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "32872", "valid_best_loss": "2.645"}
2022-01-28 15:20:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 32872 updates
2022-01-28 15:20:31 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 15:20:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 15:20:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 21 @ 32872 updates, score 2.646) (writing took 4.270803338848054 seconds)
2022-01-28 15:20:36 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-28 15:20:36 | INFO | train | {"epoch": 21, "train_loss": "2.618", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "691950", "train_ups": "3.95", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "32872", "train_lr": "0.00491945", "train_gnorm": "0.607", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "8475"}
2022-01-28 15:20:36 | INFO | fairseq.trainer | begin training epoch 22
2022-01-28 15:20:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:20:53 | INFO | train_inner | {"epoch": 22, "update": 21.018, "loss": "2.621", "ntokens": "174767", "nsentences": "17.04", "wps": "398595", "ups": "2.28", "wpb": "174767", "bsz": "17", "num_updates": "32900", "lr": "0.00491931", "gnorm": "0.633", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8493"}
2022-01-28 15:21:17 | INFO | train_inner | {"epoch": 22, "update": 21.082, "loss": "2.601", "ntokens": "175445", "nsentences": "16.64", "wps": "734049", "ups": "4.18", "wpb": "175445", "bsz": "16.6", "num_updates": "33000", "lr": "0.00491881", "gnorm": "0.64", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8517"}
2022-01-28 15:21:41 | INFO | train_inner | {"epoch": 22, "update": 21.146, "loss": "2.599", "ntokens": "174477", "nsentences": "16.96", "wps": "728469", "ups": "4.18", "wpb": "174477", "bsz": "17", "num_updates": "33100", "lr": "0.00491831", "gnorm": "0.646", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8541"}
2022-01-28 15:22:05 | INFO | train_inner | {"epoch": 22, "update": 21.209, "loss": "2.609", "ntokens": "176144", "nsentences": "16.48", "wps": "732906", "ups": "4.16", "wpb": "176144", "bsz": "16.5", "num_updates": "33200", "lr": "0.00491782", "gnorm": "0.651", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8565"}
2022-01-28 15:22:29 | INFO | train_inner | {"epoch": 22, "update": 21.273, "loss": "2.602", "ntokens": "175464", "nsentences": "16.8", "wps": "729982", "ups": "4.16", "wpb": "175464", "bsz": "16.8", "num_updates": "33300", "lr": "0.00491731", "gnorm": "0.586", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8589"}
2022-01-28 15:22:53 | INFO | train_inner | {"epoch": 22, "update": 21.337, "loss": "2.592", "ntokens": "175637", "nsentences": "16.88", "wps": "730884", "ups": "4.16", "wpb": "175637", "bsz": "16.9", "num_updates": "33400", "lr": "0.00491681", "gnorm": "0.59", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8613"}
2022-01-28 15:23:17 | INFO | train_inner | {"epoch": 22, "update": 21.401, "loss": "2.607", "ntokens": "175954", "nsentences": "17.28", "wps": "731343", "ups": "4.16", "wpb": "175954", "bsz": "17.3", "num_updates": "33500", "lr": "0.00491631", "gnorm": "0.599", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8637"}
2022-01-28 15:23:41 | INFO | train_inner | {"epoch": 22, "update": 21.465, "loss": "2.611", "ntokens": "174675", "nsentences": "16.96", "wps": "726463", "ups": "4.16", "wpb": "174675", "bsz": "17", "num_updates": "33600", "lr": "0.0049158", "gnorm": "0.618", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8661"}
2022-01-28 15:24:05 | INFO | train_inner | {"epoch": 22, "update": 21.529, "loss": "2.586", "ntokens": "175800", "nsentences": "16.8", "wps": "731198", "ups": "4.16", "wpb": "175800", "bsz": "16.8", "num_updates": "33700", "lr": "0.0049153", "gnorm": "0.579", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8685"}
2022-01-28 15:24:29 | INFO | train_inner | {"epoch": 22, "update": 21.593, "loss": "2.609", "ntokens": "176473", "nsentences": "16.72", "wps": "733824", "ups": "4.16", "wpb": "176473", "bsz": "16.7", "num_updates": "33800", "lr": "0.00491479", "gnorm": "0.627", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8709"}
2022-01-28 15:24:53 | INFO | train_inner | {"epoch": 22, "update": 21.656, "loss": "2.59", "ntokens": "175307", "nsentences": "16.96", "wps": "728676", "ups": "4.16", "wpb": "175307", "bsz": "17", "num_updates": "33900", "lr": "0.00491428", "gnorm": "0.566", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8733"}
2022-01-28 15:25:17 | INFO | train_inner | {"epoch": 22, "update": 21.72, "loss": "2.601", "ntokens": "174097", "nsentences": "17.04", "wps": "726180", "ups": "4.17", "wpb": "174097", "bsz": "17", "num_updates": "34000", "lr": "0.00491377", "gnorm": "0.62", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "8757"}
2022-01-28 15:25:41 | INFO | train_inner | {"epoch": 22, "update": 21.784, "loss": "2.608", "ntokens": "172782", "nsentences": "16.5", "wps": "720461", "ups": "4.17", "wpb": "172782", "bsz": "16.5", "num_updates": "34100", "lr": "0.00491326", "gnorm": "0.627", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8781"}
2022-01-28 15:26:05 | INFO | train_inner | {"epoch": 22, "update": 21.848, "loss": "2.599", "ntokens": "173874", "nsentences": "16.96", "wps": "722228", "ups": "4.15", "wpb": "173874", "bsz": "17", "num_updates": "34200", "lr": "0.00491274", "gnorm": "0.579", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8805"}
2022-01-28 15:26:29 | INFO | train_inner | {"epoch": 22, "update": 21.912, "loss": "2.595", "ntokens": "176261", "nsentences": "17.28", "wps": "731937", "ups": "4.15", "wpb": "176261", "bsz": "17.3", "num_updates": "34300", "lr": "0.00491223", "gnorm": "0.588", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8829"}
2022-01-28 15:26:54 | INFO | train_inner | {"epoch": 22, "update": 21.976, "loss": "2.586", "ntokens": "175244", "nsentences": "16.88", "wps": "727462", "ups": "4.15", "wpb": "175244", "bsz": "16.9", "num_updates": "34400", "lr": "0.00491171", "gnorm": "0.613", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8853"}
2022-01-28 15:27:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:27:07 | INFO | valid | {"epoch": 22, "valid_loss": "2.608", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.67162e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "34438", "valid_best_loss": "2.608"}
2022-01-28 15:27:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 34438 updates
2022-01-28 15:27:07 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:27:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:27:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 22 @ 34438 updates, score 2.608) (writing took 11.694847372360528 seconds)
2022-01-28 15:27:19 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-28 15:27:19 | INFO | train | {"epoch": 22, "train_loss": "2.6", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679880", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "34438", "train_lr": "0.00491151", "train_gnorm": "0.608", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "8879"}
2022-01-28 15:27:19 | INFO | fairseq.trainer | begin training epoch 23
2022-01-28 15:27:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:27:45 | INFO | train_inner | {"epoch": 23, "update": 22.04, "loss": "2.601", "ntokens": "175364", "nsentences": "17.04", "wps": "343028", "ups": "1.96", "wpb": "175364", "bsz": "17", "num_updates": "34500", "lr": "0.00491119", "gnorm": "0.563", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8904"}
2022-01-28 15:28:09 | INFO | train_inner | {"epoch": 23, "update": 22.103, "loss": "2.591", "ntokens": "174959", "nsentences": "17.2", "wps": "730557", "ups": "4.18", "wpb": "174959", "bsz": "17.2", "num_updates": "34600", "lr": "0.00491067", "gnorm": "0.594", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8928"}
2022-01-28 15:28:33 | INFO | train_inner | {"epoch": 23, "update": 22.167, "loss": "2.581", "ntokens": "174929", "nsentences": "17.36", "wps": "732127", "ups": "4.19", "wpb": "174929", "bsz": "17.4", "num_updates": "34700", "lr": "0.00491015", "gnorm": "0.6", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8952"}
2022-01-28 15:28:57 | INFO | train_inner | {"epoch": 23, "update": 22.231, "loss": "2.585", "ntokens": "176328", "nsentences": "17.04", "wps": "733700", "ups": "4.16", "wpb": "176328", "bsz": "17", "num_updates": "34800", "lr": "0.00490963", "gnorm": "0.612", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "8976"}
2022-01-28 15:29:21 | INFO | train_inner | {"epoch": 23, "update": 22.295, "loss": "2.572", "ntokens": "175695", "nsentences": "16.16", "wps": "730657", "ups": "4.16", "wpb": "175695", "bsz": "16.2", "num_updates": "34900", "lr": "0.0049091", "gnorm": "0.595", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9000"}
2022-01-28 15:29:45 | INFO | train_inner | {"epoch": 23, "update": 22.359, "loss": "2.578", "ntokens": "175530", "nsentences": "16.72", "wps": "732286", "ups": "4.17", "wpb": "175530", "bsz": "16.7", "num_updates": "35000", "lr": "0.00490858", "gnorm": "0.615", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9024"}
2022-01-28 15:30:09 | INFO | train_inner | {"epoch": 23, "update": 22.423, "loss": "2.578", "ntokens": "176020", "nsentences": "16.96", "wps": "729855", "ups": "4.15", "wpb": "176020", "bsz": "17", "num_updates": "35100", "lr": "0.00490805", "gnorm": "0.6", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9049"}
2022-01-28 15:30:33 | INFO | train_inner | {"epoch": 23, "update": 22.487, "loss": "2.597", "ntokens": "175708", "nsentences": "17.76", "wps": "733154", "ups": "4.17", "wpb": "175708", "bsz": "17.8", "num_updates": "35200", "lr": "0.00490752", "gnorm": "0.617", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.6", "wall": "9072"}
2022-01-28 15:30:57 | INFO | train_inner | {"epoch": 23, "update": 22.55, "loss": "2.594", "ntokens": "174211", "nsentences": "16.24", "wps": "725062", "ups": "4.16", "wpb": "174211", "bsz": "16.2", "num_updates": "35300", "lr": "0.00490699", "gnorm": "0.574", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9097"}
2022-01-28 15:31:21 | INFO | train_inner | {"epoch": 23, "update": 22.614, "loss": "2.593", "ntokens": "175297", "nsentences": "17.52", "wps": "729324", "ups": "4.16", "wpb": "175297", "bsz": "17.5", "num_updates": "35400", "lr": "0.00490646", "gnorm": "0.652", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9121"}
2022-01-28 15:31:45 | INFO | train_inner | {"epoch": 23, "update": 22.678, "loss": "2.576", "ntokens": "174040", "nsentences": "16.72", "wps": "727990", "ups": "4.18", "wpb": "174040", "bsz": "16.7", "num_updates": "35500", "lr": "0.00490592", "gnorm": "0.59", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9144"}
2022-01-28 15:32:09 | INFO | train_inner | {"epoch": 23, "update": 22.742, "loss": "2.571", "ntokens": "175830", "nsentences": "16.56", "wps": "729799", "ups": "4.15", "wpb": "175830", "bsz": "16.6", "num_updates": "35600", "lr": "0.00490539", "gnorm": "0.632", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9169"}
2022-01-28 15:32:33 | INFO | train_inner | {"epoch": 23, "update": 22.806, "loss": "2.573", "ntokens": "173864", "nsentences": "17.14", "wps": "725234", "ups": "4.17", "wpb": "173864", "bsz": "17.1", "num_updates": "35700", "lr": "0.00490485", "gnorm": "0.597", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9193"}
2022-01-28 15:32:57 | INFO | train_inner | {"epoch": 23, "update": 22.87, "loss": "2.589", "ntokens": "174926", "nsentences": "16.48", "wps": "727468", "ups": "4.16", "wpb": "174926", "bsz": "16.5", "num_updates": "35800", "lr": "0.00490431", "gnorm": "0.614", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "21", "wall": "9217"}
2022-01-28 15:33:21 | INFO | train_inner | {"epoch": 23, "update": 22.934, "loss": "2.587", "ntokens": "175042", "nsentences": "16.72", "wps": "729296", "ups": "4.17", "wpb": "175042", "bsz": "16.7", "num_updates": "35900", "lr": "0.00490377", "gnorm": "0.619", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9241"}
2022-01-28 15:33:45 | INFO | train_inner | {"epoch": 23, "update": 22.997, "loss": "2.578", "ntokens": "175407", "nsentences": "16.8", "wps": "721222", "ups": "4.11", "wpb": "175407", "bsz": "16.8", "num_updates": "36000", "lr": "0.00490323", "gnorm": "0.638", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9265"}
2022-01-28 15:33:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:33:51 | INFO | valid | {"epoch": 23, "valid_loss": "2.611", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.60767e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "36004", "valid_best_loss": "2.608"}
2022-01-28 15:33:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 36004 updates
2022-01-28 15:33:51 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 15:33:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 15:33:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 23 @ 36004 updates, score 2.611) (writing took 4.260686040855944 seconds)
2022-01-28 15:33:55 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-28 15:33:55 | INFO | train | {"epoch": 23, "train_loss": "2.583", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "692944", "train_ups": "3.96", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "36004", "train_lr": "0.00490321", "train_gnorm": "0.608", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "9275"}
2022-01-28 15:33:55 | INFO | fairseq.trainer | begin training epoch 24
2022-01-28 15:33:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:34:29 | INFO | train_inner | {"epoch": 24, "update": 23.061, "loss": "2.572", "ntokens": "174180", "nsentences": "16.64", "wps": "396531", "ups": "2.28", "wpb": "174180", "bsz": "16.6", "num_updates": "36100", "lr": "0.00490269", "gnorm": "0.585", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9309"}
2022-01-28 15:34:53 | INFO | train_inner | {"epoch": 24, "update": 23.125, "loss": "2.572", "ntokens": "175302", "nsentences": "16.8", "wps": "732886", "ups": "4.18", "wpb": "175302", "bsz": "16.8", "num_updates": "36200", "lr": "0.00490215", "gnorm": "0.598", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.8", "wall": "9333"}
2022-01-28 15:35:17 | INFO | train_inner | {"epoch": 24, "update": 23.189, "loss": "2.572", "ntokens": "174688", "nsentences": "16.96", "wps": "728938", "ups": "4.17", "wpb": "174688", "bsz": "17", "num_updates": "36300", "lr": "0.0049016", "gnorm": "0.586", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9357"}
2022-01-28 15:35:41 | INFO | train_inner | {"epoch": 24, "update": 23.253, "loss": "2.553", "ntokens": "176475", "nsentences": "16.56", "wps": "735827", "ups": "4.17", "wpb": "176475", "bsz": "16.6", "num_updates": "36400", "lr": "0.00490106", "gnorm": "0.561", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9381"}
2022-01-28 15:36:05 | INFO | train_inner | {"epoch": 24, "update": 23.317, "loss": "2.563", "ntokens": "174297", "nsentences": "17.06", "wps": "725784", "ups": "4.16", "wpb": "174297", "bsz": "17.1", "num_updates": "36500", "lr": "0.00490051", "gnorm": "0.632", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9405"}
2022-01-28 15:36:29 | INFO | train_inner | {"epoch": 24, "update": 23.381, "loss": "2.549", "ntokens": "174478", "nsentences": "16.8", "wps": "727393", "ups": "4.17", "wpb": "174478", "bsz": "16.8", "num_updates": "36600", "lr": "0.00489996", "gnorm": "0.56", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "9429"}
2022-01-28 15:36:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2022-01-28 15:36:53 | INFO | train_inner | {"epoch": 24, "update": 23.445, "loss": "2.556", "ntokens": "176188", "nsentences": "16.56", "wps": "725075", "ups": "4.12", "wpb": "176188", "bsz": "16.6", "num_updates": "36700", "lr": "0.00489941", "gnorm": "0.614", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9453"}
2022-01-28 15:37:17 | INFO | train_inner | {"epoch": 24, "update": 23.509, "loss": "2.558", "ntokens": "174580", "nsentences": "16.96", "wps": "726797", "ups": "4.16", "wpb": "174580", "bsz": "17", "num_updates": "36800", "lr": "0.00489885", "gnorm": "0.591", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9477"}
2022-01-28 15:37:41 | INFO | train_inner | {"epoch": 24, "update": 23.573, "loss": "2.548", "ntokens": "175974", "nsentences": "16.96", "wps": "732007", "ups": "4.16", "wpb": "175974", "bsz": "17", "num_updates": "36900", "lr": "0.0048983", "gnorm": "0.566", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9501"}
2022-01-28 15:38:05 | INFO | train_inner | {"epoch": 24, "update": 23.637, "loss": "2.536", "ntokens": "176694", "nsentences": "17.52", "wps": "733960", "ups": "4.15", "wpb": "176694", "bsz": "17.5", "num_updates": "37000", "lr": "0.00489774", "gnorm": "0.59", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9525"}
2022-01-28 15:38:29 | INFO | train_inner | {"epoch": 24, "update": 23.701, "loss": "2.54", "ntokens": "172827", "nsentences": "16.72", "wps": "721402", "ups": "4.17", "wpb": "172827", "bsz": "16.7", "num_updates": "37100", "lr": "0.00489719", "gnorm": "0.579", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9549"}
2022-01-28 15:38:53 | INFO | train_inner | {"epoch": 24, "update": 23.764, "loss": "2.519", "ntokens": "175175", "nsentences": "16.24", "wps": "728626", "ups": "4.16", "wpb": "175175", "bsz": "16.2", "num_updates": "37200", "lr": "0.00489663", "gnorm": "0.554", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9573"}
2022-01-28 15:39:18 | INFO | train_inner | {"epoch": 24, "update": 23.828, "loss": "2.53", "ntokens": "176446", "nsentences": "17.28", "wps": "732265", "ups": "4.15", "wpb": "176446", "bsz": "17.3", "num_updates": "37300", "lr": "0.00489607", "gnorm": "0.551", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9597"}
2022-01-28 15:39:42 | INFO | train_inner | {"epoch": 24, "update": 23.892, "loss": "2.535", "ntokens": "175611", "nsentences": "17.2", "wps": "730446", "ups": "4.16", "wpb": "175611", "bsz": "17.2", "num_updates": "37400", "lr": "0.00489551", "gnorm": "0.581", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9621"}
2022-01-28 15:40:06 | INFO | train_inner | {"epoch": 24, "update": 23.956, "loss": "2.537", "ntokens": "174631", "nsentences": "17.2", "wps": "726026", "ups": "4.16", "wpb": "174631", "bsz": "17.2", "num_updates": "37500", "lr": "0.00489494", "gnorm": "0.599", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.9", "wall": "9646"}
2022-01-28 15:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:40:27 | INFO | valid | {"epoch": 24, "valid_loss": "2.534", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.58906e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "37569", "valid_best_loss": "2.534"}
2022-01-28 15:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 37569 updates
2022-01-28 15:40:27 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:40:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:40:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 24 @ 37569 updates, score 2.534) (writing took 12.240759376436472 seconds)
2022-01-28 15:40:39 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-28 15:40:39 | INFO | train | {"epoch": 24, "train_loss": "2.549", "train_ntokens": "175181", "train_nsentences": "16.8907", "train_wps": "677818", "train_ups": "3.87", "train_wpb": "175181", "train_bsz": "16.9", "train_num_updates": "37569", "train_lr": "0.00489455", "train_gnorm": "0.582", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "9679"}
2022-01-28 15:40:39 | INFO | fairseq.trainer | begin training epoch 25
2022-01-28 15:40:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:40:58 | INFO | train_inner | {"epoch": 25, "update": 24.02, "loss": "2.523", "ntokens": "174660", "nsentences": "16.72", "wps": "334087", "ups": "1.91", "wpb": "174660", "bsz": "16.7", "num_updates": "37600", "lr": "0.00489438", "gnorm": "0.587", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9698"}
2022-01-28 15:41:22 | INFO | train_inner | {"epoch": 25, "update": 24.084, "loss": "2.508", "ntokens": "175801", "nsentences": "16.48", "wps": "737508", "ups": "4.2", "wpb": "175801", "bsz": "16.5", "num_updates": "37700", "lr": "0.00489381", "gnorm": "0.538", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21.4", "wall": "9722"}
2022-01-28 15:41:46 | INFO | train_inner | {"epoch": 25, "update": 24.148, "loss": "2.511", "ntokens": "175474", "nsentences": "16.72", "wps": "732018", "ups": "4.17", "wpb": "175474", "bsz": "16.7", "num_updates": "37800", "lr": "0.00489324", "gnorm": "0.569", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9746"}
2022-01-28 15:42:10 | INFO | train_inner | {"epoch": 25, "update": 24.211, "loss": "2.499", "ntokens": "175555", "nsentences": "16.64", "wps": "730849", "ups": "4.16", "wpb": "175555", "bsz": "16.6", "num_updates": "37900", "lr": "0.00489268", "gnorm": "0.542", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9770"}
2022-01-28 15:42:34 | INFO | train_inner | {"epoch": 25, "update": 24.275, "loss": "2.508", "ntokens": "175192", "nsentences": "17.28", "wps": "729067", "ups": "4.16", "wpb": "175192", "bsz": "17.3", "num_updates": "38000", "lr": "0.0048921", "gnorm": "0.557", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9794"}
2022-01-28 15:42:58 | INFO | train_inner | {"epoch": 25, "update": 24.339, "loss": "2.515", "ntokens": "176016", "nsentences": "16.8", "wps": "732311", "ups": "4.16", "wpb": "176016", "bsz": "16.8", "num_updates": "38100", "lr": "0.00489153", "gnorm": "0.561", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9818"}
2022-01-28 15:43:22 | INFO | train_inner | {"epoch": 25, "update": 24.403, "loss": "2.496", "ntokens": "175635", "nsentences": "16.72", "wps": "730423", "ups": "4.16", "wpb": "175635", "bsz": "16.7", "num_updates": "38200", "lr": "0.00489096", "gnorm": "0.546", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9842"}
2022-01-28 15:43:46 | INFO | train_inner | {"epoch": 25, "update": 24.467, "loss": "2.493", "ntokens": "174733", "nsentences": "16.4", "wps": "726898", "ups": "4.16", "wpb": "174733", "bsz": "16.4", "num_updates": "38300", "lr": "0.00489038", "gnorm": "0.532", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9866"}
2022-01-28 15:44:10 | INFO | train_inner | {"epoch": 25, "update": 24.531, "loss": "2.5", "ntokens": "175054", "nsentences": "16.72", "wps": "728871", "ups": "4.16", "wpb": "175054", "bsz": "16.7", "num_updates": "38400", "lr": "0.00488981", "gnorm": "0.522", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9890"}
2022-01-28 15:44:34 | INFO | train_inner | {"epoch": 25, "update": 24.595, "loss": "2.486", "ntokens": "175389", "nsentences": "16.96", "wps": "729216", "ups": "4.16", "wpb": "175389", "bsz": "17", "num_updates": "38500", "lr": "0.00488923", "gnorm": "0.575", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9914"}
2022-01-28 15:44:58 | INFO | train_inner | {"epoch": 25, "update": 24.658, "loss": "2.486", "ntokens": "174383", "nsentences": "16.48", "wps": "726283", "ups": "4.16", "wpb": "174383", "bsz": "16.5", "num_updates": "38600", "lr": "0.00488865", "gnorm": "0.527", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9938"}
2022-01-28 15:45:22 | INFO | train_inner | {"epoch": 25, "update": 24.722, "loss": "2.494", "ntokens": "173744", "nsentences": "16.82", "wps": "724145", "ups": "4.17", "wpb": "173744", "bsz": "16.8", "num_updates": "38700", "lr": "0.00488807", "gnorm": "0.562", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9962"}
2022-01-28 15:45:46 | INFO | train_inner | {"epoch": 25, "update": 24.786, "loss": "2.494", "ntokens": "175011", "nsentences": "17.52", "wps": "729962", "ups": "4.17", "wpb": "175011", "bsz": "17.5", "num_updates": "38800", "lr": "0.00488749", "gnorm": "0.547", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "9986"}
2022-01-28 15:46:10 | INFO | train_inner | {"epoch": 25, "update": 24.85, "loss": "2.497", "ntokens": "176521", "nsentences": "17.6", "wps": "733937", "ups": "4.16", "wpb": "176521", "bsz": "17.6", "num_updates": "38900", "lr": "0.0048869", "gnorm": "0.552", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20", "wall": "10010"}
2022-01-28 15:46:34 | INFO | train_inner | {"epoch": 25, "update": 24.914, "loss": "2.486", "ntokens": "175208", "nsentences": "17.36", "wps": "728824", "ups": "4.16", "wpb": "175208", "bsz": "17.4", "num_updates": "39000", "lr": "0.00488632", "gnorm": "0.539", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10034"}
2022-01-28 15:46:58 | INFO | train_inner | {"epoch": 25, "update": 24.978, "loss": "2.489", "ntokens": "175065", "nsentences": "16.72", "wps": "728652", "ups": "4.16", "wpb": "175065", "bsz": "16.7", "num_updates": "39100", "lr": "0.00488573", "gnorm": "0.557", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10058"}
2022-01-28 15:47:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:47:11 | INFO | valid | {"epoch": 25, "valid_loss": "2.52", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.703e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "39135", "valid_best_loss": "2.52"}
2022-01-28 15:47:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 39135 updates
2022-01-28 15:47:11 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:47:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:47:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 25 @ 39135 updates, score 2.52) (writing took 11.863674521446228 seconds)
2022-01-28 15:47:23 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-28 15:47:23 | INFO | train | {"epoch": 25, "train_loss": "2.497", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679381", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "39135", "train_lr": "0.00488553", "train_gnorm": "0.551", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "10083"}
2022-01-28 15:47:23 | INFO | fairseq.trainer | begin training epoch 26
2022-01-28 15:47:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:47:50 | INFO | train_inner | {"epoch": 26, "update": 25.042, "loss": "2.474", "ntokens": "175482", "nsentences": "17.12", "wps": "340342", "ups": "1.94", "wpb": "175482", "bsz": "17.1", "num_updates": "39200", "lr": "0.00488514", "gnorm": "0.546", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10110"}
2022-01-28 15:48:14 | INFO | train_inner | {"epoch": 26, "update": 25.105, "loss": "2.47", "ntokens": "175211", "nsentences": "17.36", "wps": "733418", "ups": "4.19", "wpb": "175211", "bsz": "17.4", "num_updates": "39300", "lr": "0.00488455", "gnorm": "0.527", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10134"}
2022-01-28 15:48:38 | INFO | train_inner | {"epoch": 26, "update": 25.169, "loss": "2.458", "ntokens": "175609", "nsentences": "17.04", "wps": "733643", "ups": "4.18", "wpb": "175609", "bsz": "17", "num_updates": "39400", "lr": "0.00488396", "gnorm": "0.506", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10157"}
2022-01-28 15:49:02 | INFO | train_inner | {"epoch": 26, "update": 25.233, "loss": "2.463", "ntokens": "174782", "nsentences": "16.56", "wps": "728446", "ups": "4.17", "wpb": "174782", "bsz": "16.6", "num_updates": "39500", "lr": "0.00488337", "gnorm": "0.541", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10182"}
2022-01-28 15:49:26 | INFO | train_inner | {"epoch": 26, "update": 25.297, "loss": "2.464", "ntokens": "175595", "nsentences": "17.2", "wps": "730098", "ups": "4.16", "wpb": "175595", "bsz": "17.2", "num_updates": "39600", "lr": "0.00488278", "gnorm": "0.506", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10206"}
2022-01-28 15:49:50 | INFO | train_inner | {"epoch": 26, "update": 25.361, "loss": "2.474", "ntokens": "175820", "nsentences": "16.72", "wps": "731282", "ups": "4.16", "wpb": "175820", "bsz": "16.7", "num_updates": "39700", "lr": "0.00488218", "gnorm": "0.533", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10230"}
2022-01-28 15:50:14 | INFO | train_inner | {"epoch": 26, "update": 25.425, "loss": "2.464", "ntokens": "175978", "nsentences": "16.48", "wps": "732453", "ups": "4.16", "wpb": "175978", "bsz": "16.5", "num_updates": "39800", "lr": "0.00488158", "gnorm": "0.524", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10254"}
2022-01-28 15:50:38 | INFO | train_inner | {"epoch": 26, "update": 25.489, "loss": "2.456", "ntokens": "173865", "nsentences": "16.8", "wps": "725730", "ups": "4.17", "wpb": "173865", "bsz": "16.8", "num_updates": "39900", "lr": "0.00488098", "gnorm": "0.526", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10278"}
2022-01-28 15:51:02 | INFO | train_inner | {"epoch": 26, "update": 25.552, "loss": "2.469", "ntokens": "176627", "nsentences": "16.88", "wps": "734074", "ups": "4.16", "wpb": "176627", "bsz": "16.9", "num_updates": "40000", "lr": "0.00488038", "gnorm": "0.497", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21", "wall": "10302"}
2022-01-28 15:51:26 | INFO | train_inner | {"epoch": 26, "update": 25.616, "loss": "2.458", "ntokens": "174294", "nsentences": "16.96", "wps": "726897", "ups": "4.17", "wpb": "174294", "bsz": "17", "num_updates": "40100", "lr": "0.00487978", "gnorm": "0.542", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10326"}
2022-01-28 15:51:50 | INFO | train_inner | {"epoch": 26, "update": 25.68, "loss": "2.474", "ntokens": "177127", "nsentences": "17.92", "wps": "734869", "ups": "4.15", "wpb": "177127", "bsz": "17.9", "num_updates": "40200", "lr": "0.00487918", "gnorm": "0.552", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10350"}
2022-01-28 15:52:14 | INFO | train_inner | {"epoch": 26, "update": 25.744, "loss": "2.472", "ntokens": "175259", "nsentences": "16.88", "wps": "731351", "ups": "4.17", "wpb": "175259", "bsz": "16.9", "num_updates": "40300", "lr": "0.00487858", "gnorm": "0.525", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10374"}
2022-01-28 15:52:38 | INFO | train_inner | {"epoch": 26, "update": 25.808, "loss": "2.468", "ntokens": "174600", "nsentences": "16.4", "wps": "725423", "ups": "4.15", "wpb": "174600", "bsz": "16.4", "num_updates": "40400", "lr": "0.00487797", "gnorm": "0.491", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10398"}
2022-01-28 15:53:02 | INFO | train_inner | {"epoch": 26, "update": 25.872, "loss": "2.448", "ntokens": "175170", "nsentences": "17.12", "wps": "729854", "ups": "4.17", "wpb": "175170", "bsz": "17.1", "num_updates": "40500", "lr": "0.00487736", "gnorm": "0.545", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10422"}
2022-01-28 15:53:26 | INFO | train_inner | {"epoch": 26, "update": 25.936, "loss": "2.461", "ntokens": "175492", "nsentences": "16.48", "wps": "730316", "ups": "4.16", "wpb": "175492", "bsz": "16.5", "num_updates": "40600", "lr": "0.00487675", "gnorm": "0.521", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10446"}
2022-01-28 15:53:50 | INFO | train_inner | {"epoch": 26, "update": 25.999, "loss": "2.454", "ntokens": "171711", "nsentences": "16.5", "wps": "712791", "ups": "4.15", "wpb": "171711", "bsz": "16.5", "num_updates": "40700", "lr": "0.00487614", "gnorm": "0.531", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10470"}
2022-01-28 15:53:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:53:55 | INFO | valid | {"epoch": 26, "valid_loss": "2.464", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.71404e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "40701", "valid_best_loss": "2.464"}
2022-01-28 15:53:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 40701 updates
2022-01-28 15:53:55 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:53:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 15:54:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 26 @ 40701 updates, score 2.464) (writing took 12.582525124773383 seconds)
2022-01-28 15:54:08 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-28 15:54:08 | INFO | train | {"epoch": 26, "train_loss": "2.464", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "678455", "train_ups": "3.87", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "40701", "train_lr": "0.00487614", "train_gnorm": "0.525", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "10487"}
2022-01-28 15:54:08 | INFO | fairseq.trainer | begin training epoch 27
2022-01-28 15:54:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:54:42 | INFO | train_inner | {"epoch": 27, "update": 26.063, "loss": "2.434", "ntokens": "175376", "nsentences": "16.4", "wps": "337014", "ups": "1.92", "wpb": "175376", "bsz": "16.4", "num_updates": "40800", "lr": "0.00487553", "gnorm": "0.516", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10522"}
2022-01-28 15:55:06 | INFO | train_inner | {"epoch": 27, "update": 26.127, "loss": "2.438", "ntokens": "174816", "nsentences": "16.4", "wps": "731136", "ups": "4.18", "wpb": "174816", "bsz": "16.4", "num_updates": "40900", "lr": "0.00487492", "gnorm": "0.516", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10546"}
2022-01-28 15:55:30 | INFO | train_inner | {"epoch": 27, "update": 26.191, "loss": "2.435", "ntokens": "176142", "nsentences": "17.6", "wps": "736326", "ups": "4.18", "wpb": "176142", "bsz": "17.6", "num_updates": "41000", "lr": "0.0048743", "gnorm": "0.512", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10570"}
2022-01-28 15:55:54 | INFO | train_inner | {"epoch": 27, "update": 26.255, "loss": "2.438", "ntokens": "175486", "nsentences": "16.5", "wps": "730866", "ups": "4.16", "wpb": "175486", "bsz": "16.5", "num_updates": "41100", "lr": "0.00487369", "gnorm": "0.551", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.2", "wall": "10594"}
2022-01-28 15:56:18 | INFO | train_inner | {"epoch": 27, "update": 26.319, "loss": "2.445", "ntokens": "173993", "nsentences": "16.8", "wps": "723594", "ups": "4.16", "wpb": "173993", "bsz": "16.8", "num_updates": "41200", "lr": "0.00487307", "gnorm": "0.565", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10618"}
2022-01-28 15:56:42 | INFO | train_inner | {"epoch": 27, "update": 26.383, "loss": "2.437", "ntokens": "175839", "nsentences": "16.88", "wps": "731169", "ups": "4.16", "wpb": "175839", "bsz": "16.9", "num_updates": "41300", "lr": "0.00487245", "gnorm": "0.507", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10642"}
2022-01-28 15:57:06 | INFO | train_inner | {"epoch": 27, "update": 26.446, "loss": "2.435", "ntokens": "175152", "nsentences": "16.8", "wps": "729654", "ups": "4.17", "wpb": "175152", "bsz": "16.8", "num_updates": "41400", "lr": "0.00487183", "gnorm": "0.528", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10666"}
2022-01-28 15:57:30 | INFO | train_inner | {"epoch": 27, "update": 26.51, "loss": "2.443", "ntokens": "175842", "nsentences": "17.04", "wps": "732556", "ups": "4.17", "wpb": "175842", "bsz": "17", "num_updates": "41500", "lr": "0.00487121", "gnorm": "0.496", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10690"}
2022-01-28 15:57:54 | INFO | train_inner | {"epoch": 27, "update": 26.574, "loss": "2.439", "ntokens": "174114", "nsentences": "16.72", "wps": "724714", "ups": "4.16", "wpb": "174114", "bsz": "16.7", "num_updates": "41600", "lr": "0.00487058", "gnorm": "0.504", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10714"}
2022-01-28 15:58:18 | INFO | train_inner | {"epoch": 27, "update": 26.638, "loss": "2.455", "ntokens": "175307", "nsentences": "17.36", "wps": "728648", "ups": "4.16", "wpb": "175307", "bsz": "17.4", "num_updates": "41700", "lr": "0.00486996", "gnorm": "0.572", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.5", "wall": "10738"}
2022-01-28 15:58:42 | INFO | train_inner | {"epoch": 27, "update": 26.702, "loss": "2.443", "ntokens": "176622", "nsentences": "17.36", "wps": "733638", "ups": "4.15", "wpb": "176622", "bsz": "17.4", "num_updates": "41800", "lr": "0.00486933", "gnorm": "0.499", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10762"}
2022-01-28 15:59:06 | INFO | train_inner | {"epoch": 27, "update": 26.766, "loss": "2.438", "ntokens": "175244", "nsentences": "16.8", "wps": "728565", "ups": "4.16", "wpb": "175244", "bsz": "16.8", "num_updates": "41900", "lr": "0.0048687", "gnorm": "0.515", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10786"}
2022-01-28 15:59:31 | INFO | train_inner | {"epoch": 27, "update": 26.83, "loss": "2.442", "ntokens": "173924", "nsentences": "16.96", "wps": "723073", "ups": "4.16", "wpb": "173924", "bsz": "17", "num_updates": "42000", "lr": "0.00486807", "gnorm": "0.525", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10810"}
2022-01-28 15:59:55 | INFO | train_inner | {"epoch": 27, "update": 26.893, "loss": "2.439", "ntokens": "175186", "nsentences": "16.72", "wps": "729364", "ups": "4.16", "wpb": "175186", "bsz": "16.7", "num_updates": "42100", "lr": "0.00486744", "gnorm": "0.568", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10834"}
2022-01-28 16:00:19 | INFO | train_inner | {"epoch": 27, "update": 26.957, "loss": "2.428", "ntokens": "175158", "nsentences": "17.12", "wps": "727393", "ups": "4.15", "wpb": "175158", "bsz": "17.1", "num_updates": "42200", "lr": "0.00486681", "gnorm": "0.518", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10858"}
2022-01-28 16:00:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:00:39 | INFO | valid | {"epoch": 27, "valid_loss": "2.445", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.6961e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "42267", "valid_best_loss": "2.445"}
2022-01-28 16:00:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 42267 updates
2022-01-28 16:00:39 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:00:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:00:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 27 @ 42267 updates, score 2.445) (writing took 11.725325103849173 seconds)
2022-01-28 16:00:51 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-28 16:00:51 | INFO | train | {"epoch": 27, "train_loss": "2.439", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679705", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "42267", "train_lr": "0.00486639", "train_gnorm": "0.525", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "10891"}
2022-01-28 16:00:51 | INFO | fairseq.trainer | begin training epoch 28
2022-01-28 16:00:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:01:10 | INFO | train_inner | {"epoch": 28, "update": 27.021, "loss": "2.418", "ntokens": "174638", "nsentences": "16.56", "wps": "340870", "ups": "1.95", "wpb": "174638", "bsz": "16.6", "num_updates": "42300", "lr": "0.00486618", "gnorm": "0.494", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10910"}
2022-01-28 16:01:34 | INFO | train_inner | {"epoch": 28, "update": 27.085, "loss": "2.407", "ntokens": "173748", "nsentences": "17.04", "wps": "730390", "ups": "4.2", "wpb": "173748", "bsz": "17", "num_updates": "42400", "lr": "0.00486554", "gnorm": "0.517", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10934"}
2022-01-28 16:01:58 | INFO | train_inner | {"epoch": 28, "update": 27.149, "loss": "2.425", "ntokens": "175758", "nsentences": "16.88", "wps": "733433", "ups": "4.17", "wpb": "175758", "bsz": "16.9", "num_updates": "42500", "lr": "0.00486491", "gnorm": "0.532", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10957"}
2022-01-28 16:02:22 | INFO | train_inner | {"epoch": 28, "update": 27.213, "loss": "2.429", "ntokens": "175693", "nsentences": "16.96", "wps": "734688", "ups": "4.18", "wpb": "175693", "bsz": "17", "num_updates": "42600", "lr": "0.00486427", "gnorm": "0.51", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "10981"}
2022-01-28 16:02:46 | INFO | train_inner | {"epoch": 28, "update": 27.277, "loss": "2.42", "ntokens": "176159", "nsentences": "17.2", "wps": "734675", "ups": "4.17", "wpb": "176159", "bsz": "17.2", "num_updates": "42700", "lr": "0.00486363", "gnorm": "0.549", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11005"}
2022-01-28 16:03:10 | INFO | train_inner | {"epoch": 28, "update": 27.34, "loss": "2.421", "ntokens": "176159", "nsentences": "17.12", "wps": "731694", "ups": "4.15", "wpb": "176159", "bsz": "17.1", "num_updates": "42800", "lr": "0.00486299", "gnorm": "0.511", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11029"}
2022-01-28 16:03:34 | INFO | train_inner | {"epoch": 28, "update": 27.404, "loss": "2.399", "ntokens": "175498", "nsentences": "16.48", "wps": "728765", "ups": "4.15", "wpb": "175498", "bsz": "16.5", "num_updates": "42900", "lr": "0.00486234", "gnorm": "0.515", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11054"}
2022-01-28 16:03:58 | INFO | train_inner | {"epoch": 28, "update": 27.468, "loss": "2.411", "ntokens": "175321", "nsentences": "16.8", "wps": "728788", "ups": "4.16", "wpb": "175321", "bsz": "16.8", "num_updates": "43000", "lr": "0.0048617", "gnorm": "0.516", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11078"}
2022-01-28 16:04:22 | INFO | train_inner | {"epoch": 28, "update": 27.532, "loss": "2.427", "ntokens": "174032", "nsentences": "16.48", "wps": "726987", "ups": "4.18", "wpb": "174032", "bsz": "16.5", "num_updates": "43100", "lr": "0.00486105", "gnorm": "0.543", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11102"}
2022-01-28 16:04:46 | INFO | train_inner | {"epoch": 28, "update": 27.596, "loss": "2.413", "ntokens": "175752", "nsentences": "16.88", "wps": "731973", "ups": "4.16", "wpb": "175752", "bsz": "16.9", "num_updates": "43200", "lr": "0.00486041", "gnorm": "0.463", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11126"}
2022-01-28 16:05:10 | INFO | train_inner | {"epoch": 28, "update": 27.66, "loss": "2.41", "ntokens": "175700", "nsentences": "17.28", "wps": "730348", "ups": "4.16", "wpb": "175700", "bsz": "17.3", "num_updates": "43300", "lr": "0.00485976", "gnorm": "0.518", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11150"}
2022-01-28 16:05:34 | INFO | train_inner | {"epoch": 28, "update": 27.723, "loss": "2.413", "ntokens": "175215", "nsentences": "16.88", "wps": "729450", "ups": "4.16", "wpb": "175215", "bsz": "16.9", "num_updates": "43400", "lr": "0.00485911", "gnorm": "0.533", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11174"}
2022-01-28 16:05:58 | INFO | train_inner | {"epoch": 28, "update": 27.787, "loss": "2.415", "ntokens": "175365", "nsentences": "16.4", "wps": "728185", "ups": "4.15", "wpb": "175365", "bsz": "16.4", "num_updates": "43500", "lr": "0.00485846", "gnorm": "0.52", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11198"}
2022-01-28 16:06:22 | INFO | train_inner | {"epoch": 28, "update": 27.851, "loss": "2.412", "ntokens": "173869", "nsentences": "17.38", "wps": "727949", "ups": "4.19", "wpb": "173869", "bsz": "17.4", "num_updates": "43600", "lr": "0.00485781", "gnorm": "0.484", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11222"}
2022-01-28 16:06:46 | INFO | train_inner | {"epoch": 28, "update": 27.915, "loss": "2.424", "ntokens": "174512", "nsentences": "16.8", "wps": "728752", "ups": "4.18", "wpb": "174512", "bsz": "16.8", "num_updates": "43700", "lr": "0.00485715", "gnorm": "0.503", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11246"}
2022-01-28 16:07:10 | INFO | train_inner | {"epoch": 28, "update": 27.979, "loss": "2.415", "ntokens": "174780", "nsentences": "17.2", "wps": "730001", "ups": "4.18", "wpb": "174780", "bsz": "17.2", "num_updates": "43800", "lr": "0.0048565", "gnorm": "0.503", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11270"}
2022-01-28 16:07:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:07:22 | INFO | valid | {"epoch": 28, "valid_loss": "2.431", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.77167e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "43833", "valid_best_loss": "2.431"}
2022-01-28 16:07:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 43833 updates
2022-01-28 16:07:22 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:07:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:07:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 28 @ 43833 updates, score 2.431) (writing took 11.379694300703704 seconds)
2022-01-28 16:07:34 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-28 16:07:34 | INFO | train | {"epoch": 28, "train_loss": "2.416", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "681409", "train_ups": "3.89", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "43833", "train_lr": "0.00485628", "train_gnorm": "0.515", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "11294"}
2022-01-28 16:07:34 | INFO | fairseq.trainer | begin training epoch 29
2022-01-28 16:07:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:08:01 | INFO | train_inner | {"epoch": 29, "update": 28.043, "loss": "2.398", "ntokens": "175689", "nsentences": "16.24", "wps": "342644", "ups": "1.95", "wpb": "175689", "bsz": "16.2", "num_updates": "43900", "lr": "0.00485584", "gnorm": "0.522", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11321"}
2022-01-28 16:08:25 | INFO | train_inner | {"epoch": 29, "update": 28.107, "loss": "2.387", "ntokens": "176389", "nsentences": "17.28", "wps": "738202", "ups": "4.19", "wpb": "176389", "bsz": "17.3", "num_updates": "44000", "lr": "0.00485518", "gnorm": "0.516", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11345"}
2022-01-28 16:08:49 | INFO | train_inner | {"epoch": 29, "update": 28.17, "loss": "2.396", "ntokens": "175215", "nsentences": "17.04", "wps": "733679", "ups": "4.19", "wpb": "175215", "bsz": "17", "num_updates": "44100", "lr": "0.00485452", "gnorm": "0.508", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11369"}
2022-01-28 16:09:13 | INFO | train_inner | {"epoch": 29, "update": 28.234, "loss": "2.41", "ntokens": "174553", "nsentences": "17.04", "wps": "730034", "ups": "4.18", "wpb": "174553", "bsz": "17", "num_updates": "44200", "lr": "0.00485386", "gnorm": "0.503", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11393"}
2022-01-28 16:09:37 | INFO | train_inner | {"epoch": 29, "update": 28.298, "loss": "2.396", "ntokens": "175277", "nsentences": "17.28", "wps": "730749", "ups": "4.17", "wpb": "175277", "bsz": "17.3", "num_updates": "44300", "lr": "0.0048532", "gnorm": "0.508", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11417"}
2022-01-28 16:10:01 | INFO | train_inner | {"epoch": 29, "update": 28.362, "loss": "2.407", "ntokens": "174565", "nsentences": "16.64", "wps": "726961", "ups": "4.16", "wpb": "174565", "bsz": "16.6", "num_updates": "44400", "lr": "0.00485253", "gnorm": "0.539", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11441"}
2022-01-28 16:10:25 | INFO | train_inner | {"epoch": 29, "update": 28.426, "loss": "2.403", "ntokens": "174219", "nsentences": "16.64", "wps": "724304", "ups": "4.16", "wpb": "174219", "bsz": "16.6", "num_updates": "44500", "lr": "0.00485187", "gnorm": "0.525", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11465"}
2022-01-28 16:10:49 | INFO | train_inner | {"epoch": 29, "update": 28.49, "loss": "2.394", "ntokens": "174067", "nsentences": "16.18", "wps": "724257", "ups": "4.16", "wpb": "174067", "bsz": "16.2", "num_updates": "44600", "lr": "0.0048512", "gnorm": "0.503", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11489"}
2022-01-28 16:11:13 | INFO | train_inner | {"epoch": 29, "update": 28.554, "loss": "2.386", "ntokens": "177135", "nsentences": "16.8", "wps": "737004", "ups": "4.16", "wpb": "177135", "bsz": "16.8", "num_updates": "44700", "lr": "0.00485053", "gnorm": "0.501", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11513"}
2022-01-28 16:11:37 | INFO | train_inner | {"epoch": 29, "update": 28.617, "loss": "2.398", "ntokens": "176439", "nsentences": "17.2", "wps": "733308", "ups": "4.16", "wpb": "176439", "bsz": "17.2", "num_updates": "44800", "lr": "0.00484986", "gnorm": "0.535", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "11537"}
2022-01-28 16:12:01 | INFO | train_inner | {"epoch": 29, "update": 28.681, "loss": "2.395", "ntokens": "174740", "nsentences": "16.88", "wps": "727891", "ups": "4.17", "wpb": "174740", "bsz": "16.9", "num_updates": "44900", "lr": "0.00484919", "gnorm": "0.518", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11561"}
2022-01-28 16:12:25 | INFO | train_inner | {"epoch": 29, "update": 28.745, "loss": "2.387", "ntokens": "174963", "nsentences": "16.96", "wps": "729791", "ups": "4.17", "wpb": "174963", "bsz": "17", "num_updates": "45000", "lr": "0.00484851", "gnorm": "0.501", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11585"}
2022-01-28 16:12:49 | INFO | train_inner | {"epoch": 29, "update": 28.809, "loss": "2.401", "ntokens": "174569", "nsentences": "17.2", "wps": "724814", "ups": "4.15", "wpb": "174569", "bsz": "17.2", "num_updates": "45100", "lr": "0.00484784", "gnorm": "0.511", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.6", "wall": "11609"}
2022-01-28 16:13:13 | INFO | train_inner | {"epoch": 29, "update": 28.873, "loss": "2.397", "ntokens": "175878", "nsentences": "17.04", "wps": "732789", "ups": "4.17", "wpb": "175878", "bsz": "17", "num_updates": "45200", "lr": "0.00484716", "gnorm": "0.47", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11633"}
2022-01-28 16:13:37 | INFO | train_inner | {"epoch": 29, "update": 28.937, "loss": "2.398", "ntokens": "173770", "nsentences": "16.64", "wps": "723121", "ups": "4.16", "wpb": "173770", "bsz": "16.6", "num_updates": "45300", "lr": "0.00484649", "gnorm": "0.511", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11657"}
2022-01-28 16:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:14:06 | INFO | valid | {"epoch": 29, "valid_loss": "2.42", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.62319e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "45399", "valid_best_loss": "2.42"}
2022-01-28 16:14:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 45399 updates
2022-01-28 16:14:06 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:14:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 29 @ 45399 updates, score 2.42) (writing took 11.765310952439904 seconds)
2022-01-28 16:14:17 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-28 16:14:17 | INFO | train | {"epoch": 29, "train_loss": "2.396", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679629", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "45399", "train_lr": "0.00484581", "train_gnorm": "0.511", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "11697"}
2022-01-28 16:14:18 | INFO | fairseq.trainer | begin training epoch 30
2022-01-28 16:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:14:29 | INFO | train_inner | {"epoch": 30, "update": 29.001, "loss": "2.39", "ntokens": "175712", "nsentences": "17.04", "wps": "340728", "ups": "1.94", "wpb": "175712", "bsz": "17", "num_updates": "45400", "lr": "0.00484581", "gnorm": "0.523", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11709"}
2022-01-28 16:14:53 | INFO | train_inner | {"epoch": 30, "update": 29.064, "loss": "2.382", "ntokens": "176217", "nsentences": "17.04", "wps": "740039", "ups": "4.2", "wpb": "176217", "bsz": "17", "num_updates": "45500", "lr": "0.00484513", "gnorm": "0.54", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.5", "wall": "11732"}
2022-01-28 16:15:17 | INFO | train_inner | {"epoch": 30, "update": 29.128, "loss": "2.371", "ntokens": "174978", "nsentences": "16.64", "wps": "731006", "ups": "4.18", "wpb": "174978", "bsz": "16.6", "num_updates": "45600", "lr": "0.00484444", "gnorm": "0.538", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11756"}
2022-01-28 16:15:41 | INFO | train_inner | {"epoch": 30, "update": 29.192, "loss": "2.382", "ntokens": "175143", "nsentences": "16.96", "wps": "729327", "ups": "4.16", "wpb": "175143", "bsz": "17", "num_updates": "45700", "lr": "0.00484376", "gnorm": "0.495", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11780"}
2022-01-28 16:16:05 | INFO | train_inner | {"epoch": 30, "update": 29.256, "loss": "2.376", "ntokens": "176334", "nsentences": "17.44", "wps": "734077", "ups": "4.16", "wpb": "176334", "bsz": "17.4", "num_updates": "45800", "lr": "0.00484308", "gnorm": "0.514", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11804"}
2022-01-28 16:16:29 | INFO | train_inner | {"epoch": 30, "update": 29.32, "loss": "2.377", "ntokens": "172970", "nsentences": "16.1", "wps": "718996", "ups": "4.16", "wpb": "172970", "bsz": "16.1", "num_updates": "45900", "lr": "0.00484239", "gnorm": "0.528", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11828"}
2022-01-28 16:16:53 | INFO | train_inner | {"epoch": 30, "update": 29.384, "loss": "2.372", "ntokens": "174540", "nsentences": "17.12", "wps": "725769", "ups": "4.16", "wpb": "174540", "bsz": "17.1", "num_updates": "46000", "lr": "0.0048417", "gnorm": "0.523", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11852"}
2022-01-28 16:17:17 | INFO | train_inner | {"epoch": 30, "update": 29.448, "loss": "2.38", "ntokens": "175056", "nsentences": "17.04", "wps": "726336", "ups": "4.15", "wpb": "175056", "bsz": "17", "num_updates": "46100", "lr": "0.00484101", "gnorm": "0.507", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.6", "wall": "11877"}
2022-01-28 16:17:41 | INFO | train_inner | {"epoch": 30, "update": 29.511, "loss": "2.377", "ntokens": "176034", "nsentences": "17.44", "wps": "731290", "ups": "4.15", "wpb": "176034", "bsz": "17.4", "num_updates": "46200", "lr": "0.00484032", "gnorm": "0.495", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11901"}
2022-01-28 16:18:05 | INFO | train_inner | {"epoch": 30, "update": 29.575, "loss": "2.381", "ntokens": "174399", "nsentences": "17.36", "wps": "723884", "ups": "4.15", "wpb": "174399", "bsz": "17.4", "num_updates": "46300", "lr": "0.00483963", "gnorm": "0.503", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11925"}
2022-01-28 16:18:29 | INFO | train_inner | {"epoch": 30, "update": 29.639, "loss": "2.386", "ntokens": "175665", "nsentences": "16.64", "wps": "729479", "ups": "4.15", "wpb": "175665", "bsz": "16.6", "num_updates": "46400", "lr": "0.00483894", "gnorm": "0.511", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11949"}
2022-01-28 16:18:53 | INFO | train_inner | {"epoch": 30, "update": 29.703, "loss": "2.374", "ntokens": "175557", "nsentences": "16.8", "wps": "729394", "ups": "4.15", "wpb": "175557", "bsz": "16.8", "num_updates": "46500", "lr": "0.00483824", "gnorm": "0.512", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "11973"}
2022-01-28 16:19:17 | INFO | train_inner | {"epoch": 30, "update": 29.767, "loss": "2.386", "ntokens": "174839", "nsentences": "16.8", "wps": "727067", "ups": "4.16", "wpb": "174839", "bsz": "16.8", "num_updates": "46600", "lr": "0.00483755", "gnorm": "0.482", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.9", "wall": "11997"}
2022-01-28 16:19:41 | INFO | train_inner | {"epoch": 30, "update": 29.831, "loss": "2.37", "ntokens": "175806", "nsentences": "16.32", "wps": "731570", "ups": "4.16", "wpb": "175806", "bsz": "16.3", "num_updates": "46700", "lr": "0.00483685", "gnorm": "0.501", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12021"}
2022-01-28 16:20:05 | INFO | train_inner | {"epoch": 30, "update": 29.895, "loss": "2.373", "ntokens": "176388", "nsentences": "16.64", "wps": "732268", "ups": "4.15", "wpb": "176388", "bsz": "16.6", "num_updates": "46800", "lr": "0.00483615", "gnorm": "0.484", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12045"}
2022-01-28 16:20:29 | INFO | train_inner | {"epoch": 30, "update": 29.958, "loss": "2.372", "ntokens": "174826", "nsentences": "16.56", "wps": "726801", "ups": "4.16", "wpb": "174826", "bsz": "16.6", "num_updates": "46900", "lr": "0.00483545", "gnorm": "0.502", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.5", "wall": "12069"}
2022-01-28 16:20:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:20:50 | INFO | valid | {"epoch": 30, "valid_loss": "2.399", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.55215e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "46965", "valid_best_loss": "2.399"}
2022-01-28 16:20:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 46965 updates
2022-01-28 16:20:50 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:21:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 30 @ 46965 updates, score 2.399) (writing took 11.485391699708998 seconds)
2022-01-28 16:21:01 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-28 16:21:01 | INFO | train | {"epoch": 30, "train_loss": "2.377", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679355", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "46965", "train_lr": "0.00483499", "train_gnorm": "0.509", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "12101"}
2022-01-28 16:21:01 | INFO | fairseq.trainer | begin training epoch 31
2022-01-28 16:21:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:21:21 | INFO | train_inner | {"epoch": 31, "update": 30.022, "loss": "2.367", "ntokens": "174362", "nsentences": "17.2", "wps": "338432", "ups": "1.94", "wpb": "174362", "bsz": "17.2", "num_updates": "47000", "lr": "0.00483475", "gnorm": "0.511", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12121"}
2022-01-28 16:21:45 | INFO | train_inner | {"epoch": 31, "update": 30.086, "loss": "2.361", "ntokens": "175060", "nsentences": "16.48", "wps": "735818", "ups": "4.2", "wpb": "175060", "bsz": "16.5", "num_updates": "47100", "lr": "0.00483404", "gnorm": "0.512", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12145"}
2022-01-28 16:22:09 | INFO | train_inner | {"epoch": 31, "update": 30.15, "loss": "2.353", "ntokens": "175403", "nsentences": "16.72", "wps": "732298", "ups": "4.17", "wpb": "175403", "bsz": "16.7", "num_updates": "47200", "lr": "0.00483334", "gnorm": "0.489", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12169"}
2022-01-28 16:22:33 | INFO | train_inner | {"epoch": 31, "update": 30.214, "loss": "2.353", "ntokens": "174085", "nsentences": "16.58", "wps": "724691", "ups": "4.16", "wpb": "174085", "bsz": "16.6", "num_updates": "47300", "lr": "0.00483263", "gnorm": "0.51", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12193"}
2022-01-28 16:22:57 | INFO | train_inner | {"epoch": 31, "update": 30.278, "loss": "2.368", "ntokens": "175691", "nsentences": "16.88", "wps": "732092", "ups": "4.17", "wpb": "175691", "bsz": "16.9", "num_updates": "47400", "lr": "0.00483192", "gnorm": "0.503", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12217"}
2022-01-28 16:23:21 | INFO | train_inner | {"epoch": 31, "update": 30.342, "loss": "2.377", "ntokens": "175625", "nsentences": "17.04", "wps": "731556", "ups": "4.17", "wpb": "175625", "bsz": "17", "num_updates": "47500", "lr": "0.00483121", "gnorm": "0.504", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12241"}
2022-01-28 16:23:45 | INFO | train_inner | {"epoch": 31, "update": 30.405, "loss": "2.364", "ntokens": "175420", "nsentences": "17.76", "wps": "730123", "ups": "4.16", "wpb": "175420", "bsz": "17.8", "num_updates": "47600", "lr": "0.0048305", "gnorm": "0.512", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12265"}
2022-01-28 16:24:09 | INFO | train_inner | {"epoch": 31, "update": 30.469, "loss": "2.36", "ntokens": "175828", "nsentences": "16.56", "wps": "730581", "ups": "4.16", "wpb": "175828", "bsz": "16.6", "num_updates": "47700", "lr": "0.00482979", "gnorm": "0.486", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12289"}
2022-01-28 16:24:33 | INFO | train_inner | {"epoch": 31, "update": 30.533, "loss": "2.371", "ntokens": "174984", "nsentences": "16.88", "wps": "728520", "ups": "4.16", "wpb": "174984", "bsz": "16.9", "num_updates": "47800", "lr": "0.00482908", "gnorm": "0.537", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12313"}
2022-01-28 16:24:57 | INFO | train_inner | {"epoch": 31, "update": 30.597, "loss": "2.365", "ntokens": "175125", "nsentences": "16.88", "wps": "726652", "ups": "4.15", "wpb": "175125", "bsz": "16.9", "num_updates": "47900", "lr": "0.00482836", "gnorm": "0.524", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12337"}
2022-01-28 16:25:21 | INFO | train_inner | {"epoch": 31, "update": 30.661, "loss": "2.367", "ntokens": "174246", "nsentences": "16.96", "wps": "726680", "ups": "4.17", "wpb": "174246", "bsz": "17", "num_updates": "48000", "lr": "0.00482765", "gnorm": "0.509", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12361"}
2022-01-28 16:25:45 | INFO | train_inner | {"epoch": 31, "update": 30.725, "loss": "2.365", "ntokens": "174230", "nsentences": "17.04", "wps": "725157", "ups": "4.16", "wpb": "174230", "bsz": "17", "num_updates": "48100", "lr": "0.00482693", "gnorm": "0.532", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12385"}
2022-01-28 16:26:09 | INFO | train_inner | {"epoch": 31, "update": 30.789, "loss": "2.362", "ntokens": "175861", "nsentences": "16.8", "wps": "730549", "ups": "4.15", "wpb": "175861", "bsz": "16.8", "num_updates": "48200", "lr": "0.00482621", "gnorm": "0.542", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12409"}
2022-01-28 16:26:33 | INFO | train_inner | {"epoch": 31, "update": 30.852, "loss": "2.352", "ntokens": "175846", "nsentences": "17.04", "wps": "731145", "ups": "4.16", "wpb": "175846", "bsz": "17", "num_updates": "48300", "lr": "0.00482549", "gnorm": "0.503", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12433"}
2022-01-28 16:26:57 | INFO | train_inner | {"epoch": 31, "update": 30.916, "loss": "2.368", "ntokens": "176158", "nsentences": "17.12", "wps": "732525", "ups": "4.16", "wpb": "176158", "bsz": "17.1", "num_updates": "48400", "lr": "0.00482477", "gnorm": "0.499", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12457"}
2022-01-28 16:27:21 | INFO | train_inner | {"epoch": 31, "update": 30.98, "loss": "2.375", "ntokens": "174203", "nsentences": "16.88", "wps": "725522", "ups": "4.16", "wpb": "174203", "bsz": "16.9", "num_updates": "48500", "lr": "0.00482404", "gnorm": "0.483", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12481"}
2022-01-28 16:27:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:27:33 | INFO | valid | {"epoch": 31, "valid_loss": "2.394", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.47135e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "48531", "valid_best_loss": "2.394"}
2022-01-28 16:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 48531 updates
2022-01-28 16:27:33 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:27:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:27:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 31 @ 48531 updates, score 2.394) (writing took 11.565145721659064 seconds)
2022-01-28 16:27:45 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-28 16:27:45 | INFO | train | {"epoch": 31, "train_loss": "2.364", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679440", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "48531", "train_lr": "0.00482382", "train_gnorm": "0.51", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "12505"}
2022-01-28 16:27:45 | INFO | fairseq.trainer | begin training epoch 32
2022-01-28 16:27:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:28:12 | INFO | train_inner | {"epoch": 32, "update": 31.044, "loss": "2.352", "ntokens": "174917", "nsentences": "16.8", "wps": "341484", "ups": "1.95", "wpb": "174917", "bsz": "16.8", "num_updates": "48600", "lr": "0.00482332", "gnorm": "0.499", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12532"}
2022-01-28 16:28:36 | INFO | train_inner | {"epoch": 32, "update": 31.108, "loss": "2.35", "ntokens": "174404", "nsentences": "16.88", "wps": "728978", "ups": "4.18", "wpb": "174404", "bsz": "16.9", "num_updates": "48700", "lr": "0.00482259", "gnorm": "0.501", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "12556"}
2022-01-28 16:29:00 | INFO | train_inner | {"epoch": 32, "update": 31.172, "loss": "2.34", "ntokens": "174464", "nsentences": "16.64", "wps": "728707", "ups": "4.18", "wpb": "174464", "bsz": "16.6", "num_updates": "48800", "lr": "0.00482186", "gnorm": "0.494", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12580"}
2022-01-28 16:29:24 | INFO | train_inner | {"epoch": 32, "update": 31.236, "loss": "2.343", "ntokens": "175028", "nsentences": "16.96", "wps": "729169", "ups": "4.17", "wpb": "175028", "bsz": "17", "num_updates": "48900", "lr": "0.00482113", "gnorm": "0.498", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.9", "wall": "12604"}
2022-01-28 16:29:48 | INFO | train_inner | {"epoch": 32, "update": 31.299, "loss": "2.335", "ntokens": "174591", "nsentences": "16.56", "wps": "729009", "ups": "4.18", "wpb": "174591", "bsz": "16.6", "num_updates": "49000", "lr": "0.0048204", "gnorm": "0.498", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12628"}
2022-01-28 16:30:12 | INFO | train_inner | {"epoch": 32, "update": 31.363, "loss": "2.355", "ntokens": "174387", "nsentences": "17.44", "wps": "727124", "ups": "4.17", "wpb": "174387", "bsz": "17.4", "num_updates": "49100", "lr": "0.00481967", "gnorm": "0.542", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "12652"}
2022-01-28 16:30:36 | INFO | train_inner | {"epoch": 32, "update": 31.427, "loss": "2.337", "ntokens": "175598", "nsentences": "16.72", "wps": "729866", "ups": "4.16", "wpb": "175598", "bsz": "16.7", "num_updates": "49200", "lr": "0.00481894", "gnorm": "0.515", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12676"}
2022-01-28 16:31:00 | INFO | train_inner | {"epoch": 32, "update": 31.491, "loss": "2.356", "ntokens": "175428", "nsentences": "16.96", "wps": "730874", "ups": "4.17", "wpb": "175428", "bsz": "17", "num_updates": "49300", "lr": "0.0048182", "gnorm": "0.508", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12700"}
2022-01-28 16:31:24 | INFO | train_inner | {"epoch": 32, "update": 31.555, "loss": "2.337", "ntokens": "174687", "nsentences": "16.74", "wps": "729190", "ups": "4.17", "wpb": "174686", "bsz": "16.7", "num_updates": "49400", "lr": "0.00481746", "gnorm": "0.502", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12724"}
2022-01-28 16:31:48 | INFO | train_inner | {"epoch": 32, "update": 31.619, "loss": "2.353", "ntokens": "175057", "nsentences": "16.8", "wps": "728393", "ups": "4.16", "wpb": "175057", "bsz": "16.8", "num_updates": "49500", "lr": "0.00481673", "gnorm": "0.484", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12748"}
2022-01-28 16:32:13 | INFO | train_inner | {"epoch": 32, "update": 31.683, "loss": "2.338", "ntokens": "175721", "nsentences": "17.04", "wps": "727535", "ups": "4.14", "wpb": "175721", "bsz": "17", "num_updates": "49600", "lr": "0.00481599", "gnorm": "0.523", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12772"}
2022-01-28 16:32:37 | INFO | train_inner | {"epoch": 32, "update": 31.746, "loss": "2.361", "ntokens": "175579", "nsentences": "17.36", "wps": "729946", "ups": "4.16", "wpb": "175579", "bsz": "17.4", "num_updates": "49700", "lr": "0.00481525", "gnorm": "0.513", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "21.2", "wall": "12796"}
2022-01-28 16:33:01 | INFO | train_inner | {"epoch": 32, "update": 31.81, "loss": "2.361", "ntokens": "175888", "nsentences": "17.12", "wps": "732566", "ups": "4.16", "wpb": "175888", "bsz": "17.1", "num_updates": "49800", "lr": "0.0048145", "gnorm": "0.522", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12820"}
2022-01-28 16:33:25 | INFO | train_inner | {"epoch": 32, "update": 31.874, "loss": "2.358", "ntokens": "175775", "nsentences": "17.2", "wps": "731159", "ups": "4.16", "wpb": "175775", "bsz": "17.2", "num_updates": "49900", "lr": "0.00481376", "gnorm": "0.458", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12845"}
2022-01-28 16:33:49 | INFO | train_inner | {"epoch": 32, "update": 31.938, "loss": "2.346", "ntokens": "175034", "nsentences": "16.16", "wps": "727188", "ups": "4.15", "wpb": "175034", "bsz": "16.2", "num_updates": "50000", "lr": "0.00481301", "gnorm": "0.489", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12869"}
2022-01-28 16:34:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:34:17 | INFO | valid | {"epoch": 32, "valid_loss": "2.398", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.61332e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "50097", "valid_best_loss": "2.394"}
2022-01-28 16:34:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 50097 updates
2022-01-28 16:34:17 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 16:34:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 16:34:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 32 @ 50097 updates, score 2.398) (writing took 4.286855733953416 seconds)
2022-01-28 16:34:21 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-28 16:34:21 | INFO | train | {"epoch": 32, "train_loss": "2.347", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "692628", "train_ups": "3.95", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "50097", "train_lr": "0.00481229", "train_gnorm": "0.502", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "12901"}
2022-01-28 16:34:21 | INFO | fairseq.trainer | begin training epoch 33
2022-01-28 16:34:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:34:33 | INFO | train_inner | {"epoch": 33, "update": 32.002, "loss": "2.336", "ntokens": "176168", "nsentences": "16.64", "wps": "398886", "ups": "2.26", "wpb": "176168", "bsz": "16.6", "num_updates": "50100", "lr": "0.00481227", "gnorm": "0.484", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12913"}
2022-01-28 16:34:57 | INFO | train_inner | {"epoch": 33, "update": 32.066, "loss": "2.331", "ntokens": "175245", "nsentences": "17.28", "wps": "733773", "ups": "4.19", "wpb": "175245", "bsz": "17.3", "num_updates": "50200", "lr": "0.00481152", "gnorm": "0.479", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12937"}
2022-01-28 16:35:21 | INFO | train_inner | {"epoch": 33, "update": 32.13, "loss": "2.339", "ntokens": "175066", "nsentences": "16.72", "wps": "732655", "ups": "4.19", "wpb": "175066", "bsz": "16.7", "num_updates": "50300", "lr": "0.00481077", "gnorm": "0.523", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12961"}
2022-01-28 16:35:45 | INFO | train_inner | {"epoch": 33, "update": 32.193, "loss": "2.34", "ntokens": "174214", "nsentences": "17.3", "wps": "727958", "ups": "4.18", "wpb": "174214", "bsz": "17.3", "num_updates": "50400", "lr": "0.00481002", "gnorm": "0.503", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "12985"}
2022-01-28 16:36:09 | INFO | train_inner | {"epoch": 33, "update": 32.257, "loss": "2.346", "ntokens": "176246", "nsentences": "16.72", "wps": "734314", "ups": "4.17", "wpb": "176246", "bsz": "16.7", "num_updates": "50500", "lr": "0.00480927", "gnorm": "0.522", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13009"}
2022-01-28 16:36:33 | INFO | train_inner | {"epoch": 33, "update": 32.321, "loss": "2.334", "ntokens": "174270", "nsentences": "17.2", "wps": "730113", "ups": "4.19", "wpb": "174270", "bsz": "17.2", "num_updates": "50600", "lr": "0.00480851", "gnorm": "0.501", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13032"}
2022-01-28 16:36:57 | INFO | train_inner | {"epoch": 33, "update": 32.385, "loss": "2.341", "ntokens": "173321", "nsentences": "16.88", "wps": "720049", "ups": "4.15", "wpb": "173321", "bsz": "16.9", "num_updates": "50700", "lr": "0.00480776", "gnorm": "0.514", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13056"}
2022-01-28 16:37:21 | INFO | train_inner | {"epoch": 33, "update": 32.449, "loss": "2.333", "ntokens": "174129", "nsentences": "16.88", "wps": "725015", "ups": "4.16", "wpb": "174129", "bsz": "16.9", "num_updates": "50800", "lr": "0.004807", "gnorm": "0.487", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13081"}
2022-01-28 16:37:45 | INFO | train_inner | {"epoch": 33, "update": 32.513, "loss": "2.33", "ntokens": "176605", "nsentences": "16.4", "wps": "734077", "ups": "4.16", "wpb": "176605", "bsz": "16.4", "num_updates": "50900", "lr": "0.00480624", "gnorm": "0.505", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13105"}
2022-01-28 16:38:09 | INFO | train_inner | {"epoch": 33, "update": 32.577, "loss": "2.322", "ntokens": "175978", "nsentences": "16.48", "wps": "731630", "ups": "4.16", "wpb": "175978", "bsz": "16.5", "num_updates": "51000", "lr": "0.00480548", "gnorm": "0.481", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13129"}
2022-01-28 16:38:33 | INFO | train_inner | {"epoch": 33, "update": 32.64, "loss": "2.33", "ntokens": "176062", "nsentences": "17.2", "wps": "731338", "ups": "4.15", "wpb": "176062", "bsz": "17.2", "num_updates": "51100", "lr": "0.00480472", "gnorm": "0.503", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13153"}
2022-01-28 16:38:57 | INFO | train_inner | {"epoch": 33, "update": 32.704, "loss": "2.327", "ntokens": "176028", "nsentences": "16.48", "wps": "730506", "ups": "4.15", "wpb": "176028", "bsz": "16.5", "num_updates": "51200", "lr": "0.00480396", "gnorm": "0.476", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13177"}
2022-01-28 16:39:21 | INFO | train_inner | {"epoch": 33, "update": 32.768, "loss": "2.326", "ntokens": "174540", "nsentences": "17.28", "wps": "724691", "ups": "4.15", "wpb": "174540", "bsz": "17.3", "num_updates": "51300", "lr": "0.0048032", "gnorm": "0.474", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13201"}
2022-01-28 16:39:45 | INFO | train_inner | {"epoch": 33, "update": 32.832, "loss": "2.323", "ntokens": "174570", "nsentences": "16.72", "wps": "728713", "ups": "4.17", "wpb": "174570", "bsz": "16.7", "num_updates": "51400", "lr": "0.00480243", "gnorm": "0.528", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13225"}
2022-01-28 16:40:09 | INFO | train_inner | {"epoch": 33, "update": 32.896, "loss": "2.334", "ntokens": "175253", "nsentences": "17.04", "wps": "729361", "ups": "4.16", "wpb": "175253", "bsz": "17", "num_updates": "51500", "lr": "0.00480166", "gnorm": "0.511", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13249"}
2022-01-28 16:40:33 | INFO | train_inner | {"epoch": 33, "update": 32.96, "loss": "2.329", "ntokens": "176097", "nsentences": "17.12", "wps": "732600", "ups": "4.16", "wpb": "176097", "bsz": "17.1", "num_updates": "51600", "lr": "0.0048009", "gnorm": "0.49", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "13273"}
2022-01-28 16:40:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:40:53 | INFO | valid | {"epoch": 33, "valid_loss": "2.38", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.4886e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "51663", "valid_best_loss": "2.38"}
2022-01-28 16:40:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 51663 updates
2022-01-28 16:40:53 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:40:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 33 @ 51663 updates, score 2.38) (writing took 12.334780252538621 seconds)
2022-01-28 16:41:06 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-28 16:41:06 | INFO | train | {"epoch": 33, "train_loss": "2.332", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "678449", "train_ups": "3.87", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "51663", "train_lr": "0.00480041", "train_gnorm": "0.499", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "13305"}
2022-01-28 16:41:06 | INFO | fairseq.trainer | begin training epoch 34
2022-01-28 16:41:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:41:25 | INFO | train_inner | {"epoch": 34, "update": 33.024, "loss": "2.317", "ntokens": "175412", "nsentences": "16.4", "wps": "335553", "ups": "1.91", "wpb": "175412", "bsz": "16.4", "num_updates": "51700", "lr": "0.00480013", "gnorm": "0.496", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13325"}
2022-01-28 16:41:49 | INFO | train_inner | {"epoch": 34, "update": 33.087, "loss": "2.318", "ntokens": "173223", "nsentences": "17.44", "wps": "725718", "ups": "4.19", "wpb": "173223", "bsz": "17.4", "num_updates": "51800", "lr": "0.00479936", "gnorm": "0.504", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13349"}
2022-01-28 16:42:13 | INFO | train_inner | {"epoch": 34, "update": 33.151, "loss": "2.325", "ntokens": "176498", "nsentences": "17.36", "wps": "737404", "ups": "4.18", "wpb": "176498", "bsz": "17.4", "num_updates": "51900", "lr": "0.00479858", "gnorm": "0.486", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13373"}
2022-01-28 16:42:37 | INFO | train_inner | {"epoch": 34, "update": 33.215, "loss": "2.329", "ntokens": "174093", "nsentences": "17.68", "wps": "726688", "ups": "4.17", "wpb": "174093", "bsz": "17.7", "num_updates": "52000", "lr": "0.00479781", "gnorm": "0.486", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13397"}
2022-01-28 16:43:01 | INFO | train_inner | {"epoch": 34, "update": 33.279, "loss": "2.312", "ntokens": "174694", "nsentences": "16.96", "wps": "730392", "ups": "4.18", "wpb": "174694", "bsz": "17", "num_updates": "52100", "lr": "0.00479703", "gnorm": "0.492", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13421"}
2022-01-28 16:43:25 | INFO | train_inner | {"epoch": 34, "update": 33.343, "loss": "2.327", "ntokens": "176689", "nsentences": "16.8", "wps": "734805", "ups": "4.16", "wpb": "176689", "bsz": "16.8", "num_updates": "52200", "lr": "0.00479626", "gnorm": "0.538", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13445"}
2022-01-28 16:43:49 | INFO | train_inner | {"epoch": 34, "update": 33.407, "loss": "2.318", "ntokens": "176079", "nsentences": "17.28", "wps": "733081", "ups": "4.16", "wpb": "176079", "bsz": "17.3", "num_updates": "52300", "lr": "0.00479548", "gnorm": "0.491", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13469"}
2022-01-28 16:44:13 | INFO | train_inner | {"epoch": 34, "update": 33.471, "loss": "2.305", "ntokens": "174795", "nsentences": "16.64", "wps": "727255", "ups": "4.16", "wpb": "174795", "bsz": "16.6", "num_updates": "52400", "lr": "0.0047947", "gnorm": "0.547", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13493"}
2022-01-28 16:44:37 | INFO | train_inner | {"epoch": 34, "update": 33.534, "loss": "2.321", "ntokens": "174946", "nsentences": "16.08", "wps": "727405", "ups": "4.16", "wpb": "174946", "bsz": "16.1", "num_updates": "52500", "lr": "0.00479392", "gnorm": "0.509", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "13517"}
2022-01-28 16:45:01 | INFO | train_inner | {"epoch": 34, "update": 33.598, "loss": "2.324", "ntokens": "175256", "nsentences": "16.64", "wps": "729347", "ups": "4.16", "wpb": "175256", "bsz": "16.6", "num_updates": "52600", "lr": "0.00479314", "gnorm": "0.511", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13541"}
2022-01-28 16:45:25 | INFO | train_inner | {"epoch": 34, "update": 33.662, "loss": "2.31", "ntokens": "174813", "nsentences": "16.72", "wps": "727360", "ups": "4.16", "wpb": "174813", "bsz": "16.7", "num_updates": "52700", "lr": "0.00479235", "gnorm": "0.52", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13565"}
2022-01-28 16:45:49 | INFO | train_inner | {"epoch": 34, "update": 33.726, "loss": "2.317", "ntokens": "173830", "nsentences": "16.72", "wps": "723265", "ups": "4.16", "wpb": "173830", "bsz": "16.7", "num_updates": "52800", "lr": "0.00479157", "gnorm": "0.456", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13589"}
2022-01-28 16:46:14 | INFO | train_inner | {"epoch": 34, "update": 33.79, "loss": "2.317", "ntokens": "175827", "nsentences": "16.48", "wps": "729705", "ups": "4.15", "wpb": "175827", "bsz": "16.5", "num_updates": "52900", "lr": "0.00479078", "gnorm": "0.523", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13613"}
2022-01-28 16:46:38 | INFO | train_inner | {"epoch": 34, "update": 33.854, "loss": "2.32", "ntokens": "175268", "nsentences": "16.9", "wps": "728580", "ups": "4.16", "wpb": "175268", "bsz": "16.9", "num_updates": "53000", "lr": "0.00478999", "gnorm": "0.528", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "13637"}
2022-01-28 16:47:02 | INFO | train_inner | {"epoch": 34, "update": 33.918, "loss": "2.322", "ntokens": "175835", "nsentences": "17.2", "wps": "729821", "ups": "4.15", "wpb": "175835", "bsz": "17.2", "num_updates": "53100", "lr": "0.00478921", "gnorm": "0.492", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13662"}
2022-01-28 16:47:26 | INFO | train_inner | {"epoch": 34, "update": 33.981, "loss": "2.337", "ntokens": "175860", "nsentences": "16.56", "wps": "730052", "ups": "4.15", "wpb": "175860", "bsz": "16.6", "num_updates": "53200", "lr": "0.00478841", "gnorm": "0.525", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13686"}
2022-01-28 16:47:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:47:38 | INFO | valid | {"epoch": 34, "valid_loss": "2.368", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.62582e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "53229", "valid_best_loss": "2.368"}
2022-01-28 16:47:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 53229 updates
2022-01-28 16:47:38 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:47:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 34 @ 53229 updates, score 2.368) (writing took 11.741278816014528 seconds)
2022-01-28 16:47:49 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-28 16:47:49 | INFO | train | {"epoch": 34, "train_loss": "2.32", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679449", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "53229", "train_lr": "0.00478818", "train_gnorm": "0.508", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "13709"}
2022-01-28 16:47:49 | INFO | fairseq.trainer | begin training epoch 35
2022-01-28 16:47:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:48:17 | INFO | train_inner | {"epoch": 35, "update": 34.045, "loss": "2.311", "ntokens": "176005", "nsentences": "16.88", "wps": "343648", "ups": "1.95", "wpb": "176005", "bsz": "16.9", "num_updates": "53300", "lr": "0.00478762", "gnorm": "0.507", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13737"}
2022-01-28 16:48:41 | INFO | train_inner | {"epoch": 35, "update": 34.109, "loss": "2.306", "ntokens": "175918", "nsentences": "16.32", "wps": "736251", "ups": "4.19", "wpb": "175918", "bsz": "16.3", "num_updates": "53400", "lr": "0.00478683", "gnorm": "0.522", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13761"}
2022-01-28 16:49:05 | INFO | train_inner | {"epoch": 35, "update": 34.173, "loss": "2.314", "ntokens": "175806", "nsentences": "16.64", "wps": "732580", "ups": "4.17", "wpb": "175806", "bsz": "16.6", "num_updates": "53500", "lr": "0.00478603", "gnorm": "0.503", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13785"}
2022-01-28 16:49:29 | INFO | train_inner | {"epoch": 35, "update": 34.237, "loss": "2.31", "ntokens": "173951", "nsentences": "16.82", "wps": "727134", "ups": "4.18", "wpb": "173951", "bsz": "16.8", "num_updates": "53600", "lr": "0.00478524", "gnorm": "0.544", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13809"}
2022-01-28 16:49:53 | INFO | train_inner | {"epoch": 35, "update": 34.301, "loss": "2.293", "ntokens": "175565", "nsentences": "16.16", "wps": "730985", "ups": "4.16", "wpb": "175565", "bsz": "16.2", "num_updates": "53700", "lr": "0.00478444", "gnorm": "0.473", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13833"}
2022-01-28 16:50:17 | INFO | train_inner | {"epoch": 35, "update": 34.365, "loss": "2.321", "ntokens": "173662", "nsentences": "17.6", "wps": "723854", "ups": "4.17", "wpb": "173662", "bsz": "17.6", "num_updates": "53800", "lr": "0.00478364", "gnorm": "0.514", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13857"}
2022-01-28 16:50:41 | INFO | train_inner | {"epoch": 35, "update": 34.428, "loss": "2.304", "ntokens": "174668", "nsentences": "17.12", "wps": "727503", "ups": "4.17", "wpb": "174668", "bsz": "17.1", "num_updates": "53900", "lr": "0.00478284", "gnorm": "0.473", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.7", "wall": "13881"}
2022-01-28 16:51:05 | INFO | train_inner | {"epoch": 35, "update": 34.492, "loss": "2.298", "ntokens": "175203", "nsentences": "16.88", "wps": "729742", "ups": "4.17", "wpb": "175203", "bsz": "16.9", "num_updates": "54000", "lr": "0.00478204", "gnorm": "0.499", "loss_scale": "0.125", "train_wall": "24", "gb_free": "21.1", "wall": "13905"}
2022-01-28 16:51:29 | INFO | train_inner | {"epoch": 35, "update": 34.556, "loss": "2.306", "ntokens": "175323", "nsentences": "16.8", "wps": "730154", "ups": "4.16", "wpb": "175323", "bsz": "16.8", "num_updates": "54100", "lr": "0.00478123", "gnorm": "0.513", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13929"}
2022-01-28 16:51:53 | INFO | train_inner | {"epoch": 35, "update": 34.62, "loss": "2.312", "ntokens": "175609", "nsentences": "17.28", "wps": "730997", "ups": "4.16", "wpb": "175609", "bsz": "17.3", "num_updates": "54200", "lr": "0.00478043", "gnorm": "0.488", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13953"}
2022-01-28 16:52:17 | INFO | train_inner | {"epoch": 35, "update": 34.684, "loss": "2.308", "ntokens": "175149", "nsentences": "16.72", "wps": "729433", "ups": "4.16", "wpb": "175149", "bsz": "16.7", "num_updates": "54300", "lr": "0.00477962", "gnorm": "0.493", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "13977"}
2022-01-28 16:52:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2022-01-28 16:52:41 | INFO | train_inner | {"epoch": 35, "update": 34.748, "loss": "2.304", "ntokens": "175839", "nsentences": "17.68", "wps": "723663", "ups": "4.12", "wpb": "175839", "bsz": "17.7", "num_updates": "54400", "lr": "0.00477882", "gnorm": "0.48", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14001"}
2022-01-28 16:53:05 | INFO | train_inner | {"epoch": 35, "update": 34.812, "loss": "2.314", "ntokens": "173976", "nsentences": "16.48", "wps": "728145", "ups": "4.19", "wpb": "173976", "bsz": "16.5", "num_updates": "54500", "lr": "0.00477801", "gnorm": "0.507", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14025"}
2022-01-28 16:53:29 | INFO | train_inner | {"epoch": 35, "update": 34.876, "loss": "2.3", "ntokens": "175318", "nsentences": "16.08", "wps": "728838", "ups": "4.16", "wpb": "175318", "bsz": "16.1", "num_updates": "54600", "lr": "0.0047772", "gnorm": "0.5", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14049"}
2022-01-28 16:53:53 | INFO | train_inner | {"epoch": 35, "update": 34.94, "loss": "2.301", "ntokens": "175661", "nsentences": "17.52", "wps": "728367", "ups": "4.15", "wpb": "175661", "bsz": "17.5", "num_updates": "54700", "lr": "0.00477638", "gnorm": "0.507", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14073"}
2022-01-28 16:54:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:54:21 | INFO | valid | {"epoch": 35, "valid_loss": "2.354", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.69083e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "54794", "valid_best_loss": "2.354"}
2022-01-28 16:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 54794 updates
2022-01-28 16:54:21 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:54:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 16:54:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 35 @ 54794 updates, score 2.354) (writing took 11.320823614485562 seconds)
2022-01-28 16:54:32 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-28 16:54:32 | INFO | train | {"epoch": 35, "train_loss": "2.307", "train_ntokens": "175180", "train_nsentences": "16.8907", "train_wps": "680730", "train_ups": "3.89", "train_wpb": "175180", "train_bsz": "16.9", "train_num_updates": "54794", "train_lr": "0.00477562", "train_gnorm": "0.5", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "14112"}
2022-01-28 16:54:32 | INFO | fairseq.trainer | begin training epoch 36
2022-01-28 16:54:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:54:44 | INFO | train_inner | {"epoch": 36, "update": 35.004, "loss": "2.319", "ntokens": "175199", "nsentences": "17.28", "wps": "344058", "ups": "1.96", "wpb": "175199", "bsz": "17.3", "num_updates": "54800", "lr": "0.00477557", "gnorm": "0.484", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14124"}
2022-01-28 16:55:08 | INFO | train_inner | {"epoch": 36, "update": 35.068, "loss": "2.295", "ntokens": "175001", "nsentences": "16.72", "wps": "735606", "ups": "4.2", "wpb": "175001", "bsz": "16.7", "num_updates": "54900", "lr": "0.00477476", "gnorm": "0.525", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.5", "wall": "14148"}
2022-01-28 16:55:32 | INFO | train_inner | {"epoch": 36, "update": 35.132, "loss": "2.299", "ntokens": "174163", "nsentences": "17.28", "wps": "729239", "ups": "4.19", "wpb": "174163", "bsz": "17.3", "num_updates": "55000", "lr": "0.00477394", "gnorm": "0.506", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14172"}
2022-01-28 16:55:56 | INFO | train_inner | {"epoch": 36, "update": 35.195, "loss": "2.308", "ntokens": "173116", "nsentences": "17.12", "wps": "724283", "ups": "4.18", "wpb": "173116", "bsz": "17.1", "num_updates": "55100", "lr": "0.00477312", "gnorm": "0.493", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14196"}
2022-01-28 16:56:20 | INFO | train_inner | {"epoch": 36, "update": 35.259, "loss": "2.303", "ntokens": "173973", "nsentences": "16.8", "wps": "726780", "ups": "4.18", "wpb": "173973", "bsz": "16.8", "num_updates": "55200", "lr": "0.0047723", "gnorm": "0.527", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14220"}
2022-01-28 16:56:44 | INFO | train_inner | {"epoch": 36, "update": 35.323, "loss": "2.295", "ntokens": "175036", "nsentences": "16.4", "wps": "728282", "ups": "4.16", "wpb": "175036", "bsz": "16.4", "num_updates": "55300", "lr": "0.00477148", "gnorm": "0.526", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14244"}
2022-01-28 16:57:08 | INFO | train_inner | {"epoch": 36, "update": 35.387, "loss": "2.297", "ntokens": "176096", "nsentences": "17.36", "wps": "729664", "ups": "4.14", "wpb": "176096", "bsz": "17.4", "num_updates": "55400", "lr": "0.00477066", "gnorm": "0.504", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14268"}
2022-01-28 16:57:32 | INFO | train_inner | {"epoch": 36, "update": 35.451, "loss": "2.302", "ntokens": "175673", "nsentences": "16.88", "wps": "730267", "ups": "4.16", "wpb": "175673", "bsz": "16.9", "num_updates": "55500", "lr": "0.00476984", "gnorm": "0.506", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14292"}
2022-01-28 16:57:56 | INFO | train_inner | {"epoch": 36, "update": 35.515, "loss": "2.294", "ntokens": "174333", "nsentences": "16.66", "wps": "725516", "ups": "4.16", "wpb": "174333", "bsz": "16.7", "num_updates": "55600", "lr": "0.00476901", "gnorm": "0.504", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14316"}
2022-01-28 16:58:20 | INFO | train_inner | {"epoch": 36, "update": 35.579, "loss": "2.3", "ntokens": "176180", "nsentences": "16.96", "wps": "730623", "ups": "4.15", "wpb": "176180", "bsz": "17", "num_updates": "55700", "lr": "0.00476819", "gnorm": "0.48", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14340"}
2022-01-28 16:58:44 | INFO | train_inner | {"epoch": 36, "update": 35.642, "loss": "2.298", "ntokens": "175949", "nsentences": "16.8", "wps": "730147", "ups": "4.15", "wpb": "175949", "bsz": "16.8", "num_updates": "55800", "lr": "0.00476736", "gnorm": "0.491", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14364"}
2022-01-28 16:59:09 | INFO | train_inner | {"epoch": 36, "update": 35.706, "loss": "2.3", "ntokens": "176081", "nsentences": "16.64", "wps": "730012", "ups": "4.15", "wpb": "176081", "bsz": "16.6", "num_updates": "55900", "lr": "0.00476653", "gnorm": "0.501", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.8", "wall": "14388"}
2022-01-28 16:59:33 | INFO | train_inner | {"epoch": 36, "update": 35.77, "loss": "2.302", "ntokens": "173670", "nsentences": "16.8", "wps": "724170", "ups": "4.17", "wpb": "173670", "bsz": "16.8", "num_updates": "56000", "lr": "0.0047657", "gnorm": "0.498", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14412"}
2022-01-28 16:59:57 | INFO | train_inner | {"epoch": 36, "update": 35.834, "loss": "2.292", "ntokens": "176514", "nsentences": "16.8", "wps": "735812", "ups": "4.17", "wpb": "176514", "bsz": "16.8", "num_updates": "56100", "lr": "0.00476487", "gnorm": "0.483", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.5", "wall": "14436"}
2022-01-28 17:00:21 | INFO | train_inner | {"epoch": 36, "update": 35.898, "loss": "2.296", "ntokens": "175630", "nsentences": "17.04", "wps": "731005", "ups": "4.16", "wpb": "175630", "bsz": "17", "num_updates": "56200", "lr": "0.00476404", "gnorm": "0.499", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14460"}
2022-01-28 17:00:45 | INFO | train_inner | {"epoch": 36, "update": 35.962, "loss": "2.321", "ntokens": "175509", "nsentences": "16.8", "wps": "729780", "ups": "4.16", "wpb": "175509", "bsz": "16.8", "num_updates": "56300", "lr": "0.0047632", "gnorm": "0.517", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14484"}
2022-01-28 17:00:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:01:04 | INFO | valid | {"epoch": 36, "valid_loss": "2.327", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.4458e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "56360", "valid_best_loss": "2.327"}
2022-01-28 17:01:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 56360 updates
2022-01-28 17:01:04 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:01:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:01:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 36 @ 56360 updates, score 2.327) (writing took 11.591643112711608 seconds)
2022-01-28 17:01:16 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-28 17:01:16 | INFO | train | {"epoch": 36, "train_loss": "2.3", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679940", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "56360", "train_lr": "0.0047627", "train_gnorm": "0.504", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "14515"}
2022-01-28 17:01:16 | INFO | fairseq.trainer | begin training epoch 37
2022-01-28 17:01:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:01:36 | INFO | train_inner | {"epoch": 37, "update": 36.026, "loss": "2.289", "ntokens": "176293", "nsentences": "17.04", "wps": "344076", "ups": "1.95", "wpb": "176293", "bsz": "17", "num_updates": "56400", "lr": "0.00476237", "gnorm": "0.493", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14536"}
2022-01-28 17:02:00 | INFO | train_inner | {"epoch": 37, "update": 36.089, "loss": "2.278", "ntokens": "175275", "nsentences": "16.64", "wps": "735135", "ups": "4.19", "wpb": "175275", "bsz": "16.6", "num_updates": "56500", "lr": "0.00476153", "gnorm": "0.483", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14560"}
2022-01-28 17:02:24 | INFO | train_inner | {"epoch": 37, "update": 36.153, "loss": "2.291", "ntokens": "174933", "nsentences": "16.96", "wps": "731058", "ups": "4.18", "wpb": "174933", "bsz": "17", "num_updates": "56600", "lr": "0.00476069", "gnorm": "0.539", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14584"}
2022-01-28 17:02:48 | INFO | train_inner | {"epoch": 37, "update": 36.217, "loss": "2.293", "ntokens": "173619", "nsentences": "17.12", "wps": "724244", "ups": "4.17", "wpb": "173619", "bsz": "17.1", "num_updates": "56700", "lr": "0.00475985", "gnorm": "0.516", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14607"}
2022-01-28 17:03:12 | INFO | train_inner | {"epoch": 37, "update": 36.281, "loss": "2.302", "ntokens": "176121", "nsentences": "17.04", "wps": "734795", "ups": "4.17", "wpb": "176121", "bsz": "17", "num_updates": "56800", "lr": "0.00475901", "gnorm": "0.504", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14631"}
2022-01-28 17:03:36 | INFO | train_inner | {"epoch": 37, "update": 36.345, "loss": "2.277", "ntokens": "175800", "nsentences": "16.64", "wps": "730813", "ups": "4.16", "wpb": "175800", "bsz": "16.6", "num_updates": "56900", "lr": "0.00475817", "gnorm": "0.493", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14656"}
2022-01-28 17:04:00 | INFO | train_inner | {"epoch": 37, "update": 36.409, "loss": "2.289", "ntokens": "176198", "nsentences": "16.88", "wps": "732748", "ups": "4.16", "wpb": "176198", "bsz": "16.9", "num_updates": "57000", "lr": "0.00475732", "gnorm": "0.555", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14680"}
2022-01-28 17:04:24 | INFO | train_inner | {"epoch": 37, "update": 36.473, "loss": "2.286", "ntokens": "173170", "nsentences": "17.44", "wps": "723831", "ups": "4.18", "wpb": "173170", "bsz": "17.4", "num_updates": "57100", "lr": "0.00475648", "gnorm": "0.507", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14704"}
2022-01-28 17:04:48 | INFO | train_inner | {"epoch": 37, "update": 36.536, "loss": "2.303", "ntokens": "173418", "nsentences": "17.12", "wps": "724379", "ups": "4.18", "wpb": "173418", "bsz": "17.1", "num_updates": "57200", "lr": "0.00475563", "gnorm": "0.51", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14727"}
2022-01-28 17:05:12 | INFO | train_inner | {"epoch": 37, "update": 36.6, "loss": "2.288", "ntokens": "175972", "nsentences": "16.88", "wps": "732086", "ups": "4.16", "wpb": "175972", "bsz": "16.9", "num_updates": "57300", "lr": "0.00475478", "gnorm": "0.503", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14752"}
2022-01-28 17:05:36 | INFO | train_inner | {"epoch": 37, "update": 36.664, "loss": "2.288", "ntokens": "176117", "nsentences": "17.12", "wps": "732569", "ups": "4.16", "wpb": "176117", "bsz": "17.1", "num_updates": "57400", "lr": "0.00475393", "gnorm": "0.505", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14776"}
2022-01-28 17:06:00 | INFO | train_inner | {"epoch": 37, "update": 36.728, "loss": "2.282", "ntokens": "175739", "nsentences": "16.4", "wps": "730057", "ups": "4.15", "wpb": "175739", "bsz": "16.4", "num_updates": "57500", "lr": "0.00475308", "gnorm": "0.513", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14800"}
2022-01-28 17:06:24 | INFO | train_inner | {"epoch": 37, "update": 36.792, "loss": "2.288", "ntokens": "175727", "nsentences": "17.04", "wps": "731026", "ups": "4.16", "wpb": "175727", "bsz": "17", "num_updates": "57600", "lr": "0.00475223", "gnorm": "0.479", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14824"}
2022-01-28 17:06:48 | INFO | train_inner | {"epoch": 37, "update": 36.856, "loss": "2.3", "ntokens": "175481", "nsentences": "17.12", "wps": "730222", "ups": "4.16", "wpb": "175481", "bsz": "17.1", "num_updates": "57700", "lr": "0.00475137", "gnorm": "0.532", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14848"}
2022-01-28 17:07:12 | INFO | train_inner | {"epoch": 37, "update": 36.92, "loss": "2.293", "ntokens": "174604", "nsentences": "16.9", "wps": "727056", "ups": "4.16", "wpb": "174604", "bsz": "16.9", "num_updates": "57800", "lr": "0.00475052", "gnorm": "0.495", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14872"}
2022-01-28 17:07:36 | INFO | train_inner | {"epoch": 37, "update": 36.983, "loss": "2.283", "ntokens": "174969", "nsentences": "16.16", "wps": "727605", "ups": "4.16", "wpb": "174969", "bsz": "16.2", "num_updates": "57900", "lr": "0.00474966", "gnorm": "0.501", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14896"}
2022-01-28 17:07:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:07:47 | INFO | valid | {"epoch": 37, "valid_loss": "2.34", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.48673e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "57926", "valid_best_loss": "2.327"}
2022-01-28 17:07:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 57926 updates
2022-01-28 17:07:47 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 17:07:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 17:07:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 37 @ 57926 updates, score 2.34) (writing took 4.258485331200063 seconds)
2022-01-28 17:07:51 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-28 17:07:51 | INFO | train | {"epoch": 37, "train_loss": "2.289", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "693091", "train_ups": "3.96", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "57926", "train_lr": "0.00474944", "train_gnorm": "0.508", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "14911"}
2022-01-28 17:07:51 | INFO | fairseq.trainer | begin training epoch 38
2022-01-28 17:07:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:08:20 | INFO | train_inner | {"epoch": 38, "update": 37.047, "loss": "2.271", "ntokens": "173517", "nsentences": "16.96", "wps": "393587", "ups": "2.27", "wpb": "173517", "bsz": "17", "num_updates": "58000", "lr": "0.0047488", "gnorm": "0.503", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14940"}
2022-01-28 17:08:44 | INFO | train_inner | {"epoch": 38, "update": 37.111, "loss": "2.266", "ntokens": "176182", "nsentences": "16.96", "wps": "736569", "ups": "4.18", "wpb": "176182", "bsz": "17", "num_updates": "58100", "lr": "0.00474794", "gnorm": "0.504", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14964"}
2022-01-28 17:09:08 | INFO | train_inner | {"epoch": 38, "update": 37.175, "loss": "2.276", "ntokens": "174733", "nsentences": "16.72", "wps": "729178", "ups": "4.17", "wpb": "174733", "bsz": "16.7", "num_updates": "58200", "lr": "0.00474708", "gnorm": "0.452", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "14988"}
2022-01-28 17:09:32 | INFO | train_inner | {"epoch": 38, "update": 37.239, "loss": "2.274", "ntokens": "175465", "nsentences": "16.72", "wps": "731269", "ups": "4.17", "wpb": "175465", "bsz": "16.7", "num_updates": "58300", "lr": "0.00474622", "gnorm": "0.532", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.5", "wall": "15012"}
2022-01-28 17:09:56 | INFO | train_inner | {"epoch": 38, "update": 37.303, "loss": "2.271", "ntokens": "174970", "nsentences": "16.88", "wps": "729025", "ups": "4.17", "wpb": "174970", "bsz": "16.9", "num_updates": "58400", "lr": "0.00474536", "gnorm": "0.518", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15036"}
2022-01-28 17:10:20 | INFO | train_inner | {"epoch": 38, "update": 37.367, "loss": "2.269", "ntokens": "174926", "nsentences": "17.04", "wps": "728171", "ups": "4.16", "wpb": "174926", "bsz": "17", "num_updates": "58500", "lr": "0.00474449", "gnorm": "0.483", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15060"}
2022-01-28 17:10:44 | INFO | train_inner | {"epoch": 38, "update": 37.43, "loss": "2.276", "ntokens": "174527", "nsentences": "16.64", "wps": "727705", "ups": "4.17", "wpb": "174527", "bsz": "16.6", "num_updates": "58600", "lr": "0.00474363", "gnorm": "0.478", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15084"}
2022-01-28 17:11:08 | INFO | train_inner | {"epoch": 38, "update": 37.494, "loss": "2.268", "ntokens": "175932", "nsentences": "17.04", "wps": "731116", "ups": "4.16", "wpb": "175932", "bsz": "17", "num_updates": "58700", "lr": "0.00474276", "gnorm": "0.487", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15108"}
2022-01-28 17:11:32 | INFO | train_inner | {"epoch": 38, "update": 37.558, "loss": "2.282", "ntokens": "175900", "nsentences": "17.04", "wps": "731741", "ups": "4.16", "wpb": "175900", "bsz": "17", "num_updates": "58800", "lr": "0.00474189", "gnorm": "0.484", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15132"}
2022-01-28 17:11:56 | INFO | train_inner | {"epoch": 38, "update": 37.622, "loss": "2.273", "ntokens": "174716", "nsentences": "17.12", "wps": "729910", "ups": "4.18", "wpb": "174716", "bsz": "17.1", "num_updates": "58900", "lr": "0.00474102", "gnorm": "0.49", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15156"}
2022-01-28 17:12:20 | INFO | train_inner | {"epoch": 38, "update": 37.686, "loss": "2.277", "ntokens": "175162", "nsentences": "16.8", "wps": "728707", "ups": "4.16", "wpb": "175162", "bsz": "16.8", "num_updates": "59000", "lr": "0.00474015", "gnorm": "0.505", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15180"}
2022-01-28 17:12:44 | INFO | train_inner | {"epoch": 38, "update": 37.75, "loss": "2.281", "ntokens": "175164", "nsentences": "17.44", "wps": "728508", "ups": "4.16", "wpb": "175164", "bsz": "17.4", "num_updates": "59100", "lr": "0.00473927", "gnorm": "0.505", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15204"}
2022-01-28 17:13:08 | INFO | train_inner | {"epoch": 38, "update": 37.814, "loss": "2.278", "ntokens": "174578", "nsentences": "16.56", "wps": "726759", "ups": "4.16", "wpb": "174578", "bsz": "16.6", "num_updates": "59200", "lr": "0.0047384", "gnorm": "0.495", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15228"}
2022-01-28 17:13:32 | INFO | train_inner | {"epoch": 38, "update": 37.877, "loss": "2.281", "ntokens": "175836", "nsentences": "17.28", "wps": "731934", "ups": "4.16", "wpb": "175836", "bsz": "17.3", "num_updates": "59300", "lr": "0.00473752", "gnorm": "0.466", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15252"}
2022-01-28 17:13:56 | INFO | train_inner | {"epoch": 38, "update": 37.941, "loss": "2.272", "ntokens": "175612", "nsentences": "16.48", "wps": "731726", "ups": "4.17", "wpb": "175612", "bsz": "16.5", "num_updates": "59400", "lr": "0.00473664", "gnorm": "0.499", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15276"}
2022-01-28 17:14:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:14:23 | INFO | valid | {"epoch": 38, "valid_loss": "2.32", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.60498e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "59492", "valid_best_loss": "2.32"}
2022-01-28 17:14:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 59492 updates
2022-01-28 17:14:23 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:14:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:14:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 38 @ 59492 updates, score 2.32) (writing took 11.272956402041018 seconds)
2022-01-28 17:14:34 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-28 17:14:34 | INFO | train | {"epoch": 38, "train_loss": "2.274", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680671", "train_ups": "3.89", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "59492", "train_lr": "0.00473584", "train_gnorm": "0.492", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.6", "train_wall": "15314"}
2022-01-28 17:14:34 | INFO | fairseq.trainer | begin training epoch 39
2022-01-28 17:14:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:14:47 | INFO | train_inner | {"epoch": 39, "update": 38.005, "loss": "2.269", "ntokens": "175222", "nsentences": "16.58", "wps": "344848", "ups": "1.97", "wpb": "175222", "bsz": "16.6", "num_updates": "59500", "lr": "0.00473577", "gnorm": "0.489", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15327"}
2022-01-28 17:15:11 | INFO | train_inner | {"epoch": 39, "update": 38.069, "loss": "2.248", "ntokens": "175793", "nsentences": "16.96", "wps": "738875", "ups": "4.2", "wpb": "175793", "bsz": "17", "num_updates": "59600", "lr": "0.00473489", "gnorm": "0.484", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15351"}
2022-01-28 17:15:35 | INFO | train_inner | {"epoch": 39, "update": 38.133, "loss": "2.263", "ntokens": "175225", "nsentences": "17.04", "wps": "733560", "ups": "4.19", "wpb": "175225", "bsz": "17", "num_updates": "59700", "lr": "0.004734", "gnorm": "0.495", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15375"}
2022-01-28 17:15:59 | INFO | train_inner | {"epoch": 39, "update": 38.197, "loss": "2.261", "ntokens": "174982", "nsentences": "17.2", "wps": "729081", "ups": "4.17", "wpb": "174982", "bsz": "17.2", "num_updates": "59800", "lr": "0.00473312", "gnorm": "0.503", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15399"}
2022-01-28 17:16:23 | INFO | train_inner | {"epoch": 39, "update": 38.261, "loss": "2.257", "ntokens": "174644", "nsentences": "16.56", "wps": "727144", "ups": "4.16", "wpb": "174644", "bsz": "16.6", "num_updates": "59900", "lr": "0.00473224", "gnorm": "0.478", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15423"}
2022-01-28 17:16:47 | INFO | train_inner | {"epoch": 39, "update": 38.324, "loss": "2.264", "ntokens": "175712", "nsentences": "16.48", "wps": "729447", "ups": "4.15", "wpb": "175712", "bsz": "16.5", "num_updates": "60000", "lr": "0.00473135", "gnorm": "0.511", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15447"}
2022-01-28 17:17:11 | INFO | train_inner | {"epoch": 39, "update": 38.388, "loss": "2.263", "ntokens": "175692", "nsentences": "16.96", "wps": "728646", "ups": "4.15", "wpb": "175692", "bsz": "17", "num_updates": "60100", "lr": "0.00473046", "gnorm": "0.525", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15471"}
2022-01-28 17:17:35 | INFO | train_inner | {"epoch": 39, "update": 38.452, "loss": "2.256", "ntokens": "175154", "nsentences": "16.88", "wps": "726858", "ups": "4.15", "wpb": "175154", "bsz": "16.9", "num_updates": "60200", "lr": "0.00472958", "gnorm": "0.474", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15495"}
2022-01-28 17:17:59 | INFO | train_inner | {"epoch": 39, "update": 38.516, "loss": "2.267", "ntokens": "175943", "nsentences": "17.44", "wps": "731163", "ups": "4.16", "wpb": "175943", "bsz": "17.4", "num_updates": "60300", "lr": "0.00472869", "gnorm": "0.47", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15519"}
2022-01-28 17:18:23 | INFO | train_inner | {"epoch": 39, "update": 38.58, "loss": "2.263", "ntokens": "174202", "nsentences": "16.48", "wps": "723600", "ups": "4.15", "wpb": "174202", "bsz": "16.5", "num_updates": "60400", "lr": "0.00472779", "gnorm": "0.466", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15543"}
2022-01-28 17:18:47 | INFO | train_inner | {"epoch": 39, "update": 38.644, "loss": "2.256", "ntokens": "175850", "nsentences": "16.48", "wps": "728637", "ups": "4.14", "wpb": "175850", "bsz": "16.5", "num_updates": "60500", "lr": "0.0047269", "gnorm": "0.507", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15567"}
2022-01-28 17:19:12 | INFO | train_inner | {"epoch": 39, "update": 38.708, "loss": "2.268", "ntokens": "174672", "nsentences": "17.12", "wps": "725697", "ups": "4.15", "wpb": "174672", "bsz": "17.1", "num_updates": "60600", "lr": "0.00472601", "gnorm": "0.513", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15591"}
2022-01-28 17:19:36 | INFO | train_inner | {"epoch": 39, "update": 38.771, "loss": "2.276", "ntokens": "176366", "nsentences": "17.44", "wps": "732836", "ups": "4.16", "wpb": "176366", "bsz": "17.4", "num_updates": "60700", "lr": "0.00472511", "gnorm": "0.492", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.3", "wall": "15615"}
2022-01-28 17:20:00 | INFO | train_inner | {"epoch": 39, "update": 38.835, "loss": "2.263", "ntokens": "176052", "nsentences": "16.56", "wps": "731600", "ups": "4.16", "wpb": "176052", "bsz": "16.6", "num_updates": "60800", "lr": "0.00472421", "gnorm": "0.509", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15640"}
2022-01-28 17:20:24 | INFO | train_inner | {"epoch": 39, "update": 38.899, "loss": "2.263", "ntokens": "174608", "nsentences": "17.12", "wps": "726267", "ups": "4.16", "wpb": "174608", "bsz": "17.1", "num_updates": "60900", "lr": "0.00472332", "gnorm": "0.469", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15664"}
2022-01-28 17:20:48 | INFO | train_inner | {"epoch": 39, "update": 38.963, "loss": "2.264", "ntokens": "173739", "nsentences": "16.66", "wps": "724111", "ups": "4.17", "wpb": "173738", "bsz": "16.7", "num_updates": "61000", "lr": "0.00472242", "gnorm": "0.471", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15688"}
2022-01-28 17:21:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:21:06 | INFO | valid | {"epoch": 39, "valid_loss": "2.33", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.66938e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "61058", "valid_best_loss": "2.32"}
2022-01-28 17:21:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 61058 updates
2022-01-28 17:21:06 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 17:21:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 17:21:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 39 @ 61058 updates, score 2.33) (writing took 4.254091304726899 seconds)
2022-01-28 17:21:11 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-28 17:21:11 | INFO | train | {"epoch": 39, "train_loss": "2.262", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "692514", "train_ups": "3.95", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "61058", "train_lr": "0.00472189", "train_gnorm": "0.493", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "15710"}
2022-01-28 17:21:11 | INFO | fairseq.trainer | begin training epoch 40
2022-01-28 17:21:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:21:31 | INFO | train_inner | {"epoch": 40, "update": 39.027, "loss": "2.264", "ntokens": "175608", "nsentences": "16.8", "wps": "401927", "ups": "2.29", "wpb": "175608", "bsz": "16.8", "num_updates": "61100", "lr": "0.00472152", "gnorm": "0.531", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15731"}
2022-01-28 17:21:55 | INFO | train_inner | {"epoch": 40, "update": 39.091, "loss": "2.257", "ntokens": "175359", "nsentences": "16.8", "wps": "734424", "ups": "4.19", "wpb": "175359", "bsz": "16.8", "num_updates": "61200", "lr": "0.00472061", "gnorm": "0.466", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.6", "wall": "15755"}
2022-01-28 17:22:19 | INFO | train_inner | {"epoch": 40, "update": 39.155, "loss": "2.251", "ntokens": "175210", "nsentences": "16.48", "wps": "731592", "ups": "4.18", "wpb": "175210", "bsz": "16.5", "num_updates": "61300", "lr": "0.00471971", "gnorm": "0.509", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15779"}
2022-01-28 17:22:43 | INFO | train_inner | {"epoch": 40, "update": 39.218, "loss": "2.24", "ntokens": "174584", "nsentences": "16.48", "wps": "727072", "ups": "4.16", "wpb": "174584", "bsz": "16.5", "num_updates": "61400", "lr": "0.00471881", "gnorm": "0.483", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15803"}
2022-01-28 17:23:07 | INFO | train_inner | {"epoch": 40, "update": 39.282, "loss": "2.246", "ntokens": "175629", "nsentences": "16.96", "wps": "731589", "ups": "4.17", "wpb": "175629", "bsz": "17", "num_updates": "61500", "lr": "0.0047179", "gnorm": "0.454", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15827"}
2022-01-28 17:23:31 | INFO | train_inner | {"epoch": 40, "update": 39.346, "loss": "2.261", "ntokens": "175349", "nsentences": "17.38", "wps": "728492", "ups": "4.15", "wpb": "175349", "bsz": "17.4", "num_updates": "61600", "lr": "0.00471699", "gnorm": "0.487", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15851"}
2022-01-28 17:23:55 | INFO | train_inner | {"epoch": 40, "update": 39.41, "loss": "2.244", "ntokens": "175812", "nsentences": "16.88", "wps": "731791", "ups": "4.16", "wpb": "175812", "bsz": "16.9", "num_updates": "61700", "lr": "0.00471608", "gnorm": "0.498", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15875"}
2022-01-28 17:24:20 | INFO | train_inner | {"epoch": 40, "update": 39.474, "loss": "2.245", "ntokens": "174727", "nsentences": "16.48", "wps": "726968", "ups": "4.16", "wpb": "174727", "bsz": "16.5", "num_updates": "61800", "lr": "0.00471517", "gnorm": "0.49", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.5", "wall": "15899"}
2022-01-28 17:24:43 | INFO | train_inner | {"epoch": 40, "update": 39.538, "loss": "2.251", "ntokens": "172656", "nsentences": "17.2", "wps": "721046", "ups": "4.18", "wpb": "172656", "bsz": "17.2", "num_updates": "61900", "lr": "0.00471426", "gnorm": "0.522", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.9", "wall": "15923"}
2022-01-28 17:25:08 | INFO | train_inner | {"epoch": 40, "update": 39.602, "loss": "2.249", "ntokens": "174626", "nsentences": "16.8", "wps": "726433", "ups": "4.16", "wpb": "174626", "bsz": "16.8", "num_updates": "62000", "lr": "0.00471335", "gnorm": "0.497", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15947"}
2022-01-28 17:25:32 | INFO | train_inner | {"epoch": 40, "update": 39.665, "loss": "2.257", "ntokens": "176053", "nsentences": "16.96", "wps": "732252", "ups": "4.16", "wpb": "176053", "bsz": "17", "num_updates": "62100", "lr": "0.00471243", "gnorm": "0.49", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15971"}
2022-01-28 17:25:56 | INFO | train_inner | {"epoch": 40, "update": 39.729, "loss": "2.23", "ntokens": "174452", "nsentences": "16.48", "wps": "725319", "ups": "4.16", "wpb": "174452", "bsz": "16.5", "num_updates": "62200", "lr": "0.00471152", "gnorm": "0.505", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "15995"}
2022-01-28 17:26:20 | INFO | train_inner | {"epoch": 40, "update": 39.793, "loss": "2.255", "ntokens": "176706", "nsentences": "17.68", "wps": "733400", "ups": "4.15", "wpb": "176706", "bsz": "17.7", "num_updates": "62300", "lr": "0.0047106", "gnorm": "0.502", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16020"}
2022-01-28 17:26:44 | INFO | train_inner | {"epoch": 40, "update": 39.857, "loss": "2.251", "ntokens": "175185", "nsentences": "17.36", "wps": "729911", "ups": "4.17", "wpb": "175185", "bsz": "17.4", "num_updates": "62400", "lr": "0.00470968", "gnorm": "0.504", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16044"}
2022-01-28 17:27:08 | INFO | train_inner | {"epoch": 40, "update": 39.921, "loss": "2.239", "ntokens": "175433", "nsentences": "16.64", "wps": "729642", "ups": "4.16", "wpb": "175433", "bsz": "16.6", "num_updates": "62500", "lr": "0.00470876", "gnorm": "0.471", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16068"}
2022-01-28 17:27:32 | INFO | train_inner | {"epoch": 40, "update": 39.985, "loss": "2.242", "ntokens": "175609", "nsentences": "16.88", "wps": "731340", "ups": "4.16", "wpb": "175609", "bsz": "16.9", "num_updates": "62600", "lr": "0.00470784", "gnorm": "0.485", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16092"}
2022-01-28 17:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:27:42 | INFO | valid | {"epoch": 40, "valid_loss": "2.29", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.63967e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "62624", "valid_best_loss": "2.29"}
2022-01-28 17:27:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 62624 updates
2022-01-28 17:27:42 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:27:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:27:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 40 @ 62624 updates, score 2.29) (writing took 12.113792445510626 seconds)
2022-01-28 17:27:54 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-28 17:27:54 | INFO | train | {"epoch": 40, "train_loss": "2.248", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679361", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "62624", "train_lr": "0.00470762", "train_gnorm": "0.492", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "16114"}
2022-01-28 17:27:54 | INFO | fairseq.trainer | begin training epoch 41
2022-01-28 17:27:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:28:23 | INFO | train_inner | {"epoch": 41, "update": 40.049, "loss": "2.243", "ntokens": "175099", "nsentences": "16.88", "wps": "340336", "ups": "1.94", "wpb": "175099", "bsz": "16.9", "num_updates": "62700", "lr": "0.00470691", "gnorm": "0.508", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16143"}
2022-01-28 17:28:47 | INFO | train_inner | {"epoch": 41, "update": 40.112, "loss": "2.224", "ntokens": "174584", "nsentences": "16.88", "wps": "729430", "ups": "4.18", "wpb": "174584", "bsz": "16.9", "num_updates": "62800", "lr": "0.00470599", "gnorm": "0.491", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16167"}
2022-01-28 17:29:11 | INFO | train_inner | {"epoch": 41, "update": 40.176, "loss": "2.222", "ntokens": "173888", "nsentences": "16.58", "wps": "724816", "ups": "4.17", "wpb": "173888", "bsz": "16.6", "num_updates": "62900", "lr": "0.00470506", "gnorm": "0.446", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16191"}
2022-01-28 17:29:35 | INFO | train_inner | {"epoch": 41, "update": 40.24, "loss": "2.226", "ntokens": "174888", "nsentences": "16.88", "wps": "729240", "ups": "4.17", "wpb": "174888", "bsz": "16.9", "num_updates": "63000", "lr": "0.00470414", "gnorm": "0.46", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16215"}
2022-01-28 17:29:59 | INFO | train_inner | {"epoch": 41, "update": 40.304, "loss": "2.231", "ntokens": "174742", "nsentences": "17.2", "wps": "728288", "ups": "4.17", "wpb": "174742", "bsz": "17.2", "num_updates": "63100", "lr": "0.00470321", "gnorm": "0.493", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16239"}
2022-01-28 17:30:23 | INFO | train_inner | {"epoch": 41, "update": 40.368, "loss": "2.225", "ntokens": "175382", "nsentences": "16.72", "wps": "729654", "ups": "4.16", "wpb": "175382", "bsz": "16.7", "num_updates": "63200", "lr": "0.00470228", "gnorm": "0.492", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16263"}
2022-01-28 17:30:47 | INFO | train_inner | {"epoch": 41, "update": 40.432, "loss": "2.226", "ntokens": "176728", "nsentences": "16.8", "wps": "733925", "ups": "4.15", "wpb": "176728", "bsz": "16.8", "num_updates": "63300", "lr": "0.00470135", "gnorm": "0.46", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16287"}
2022-01-28 17:31:11 | INFO | train_inner | {"epoch": 41, "update": 40.496, "loss": "2.236", "ntokens": "174891", "nsentences": "17.84", "wps": "727756", "ups": "4.16", "wpb": "174891", "bsz": "17.8", "num_updates": "63400", "lr": "0.00470042", "gnorm": "0.474", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16311"}
2022-01-28 17:31:35 | INFO | train_inner | {"epoch": 41, "update": 40.559, "loss": "2.235", "ntokens": "175864", "nsentences": "17.36", "wps": "731704", "ups": "4.16", "wpb": "175864", "bsz": "17.4", "num_updates": "63500", "lr": "0.00469948", "gnorm": "0.485", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "16335"}
2022-01-28 17:31:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2022-01-28 17:32:00 | INFO | train_inner | {"epoch": 41, "update": 40.624, "loss": "2.221", "ntokens": "175926", "nsentences": "16.64", "wps": "723660", "ups": "4.11", "wpb": "175926", "bsz": "16.6", "num_updates": "63600", "lr": "0.00469855", "gnorm": "0.465", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16360"}
2022-01-28 17:32:24 | INFO | train_inner | {"epoch": 41, "update": 40.688, "loss": "2.234", "ntokens": "175605", "nsentences": "17.04", "wps": "730233", "ups": "4.16", "wpb": "175605", "bsz": "17", "num_updates": "63700", "lr": "0.00469761", "gnorm": "0.459", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16384"}
2022-01-28 17:32:48 | INFO | train_inner | {"epoch": 41, "update": 40.752, "loss": "2.231", "ntokens": "175117", "nsentences": "16.32", "wps": "728308", "ups": "4.16", "wpb": "175117", "bsz": "16.3", "num_updates": "63800", "lr": "0.00469667", "gnorm": "0.505", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16408"}
2022-01-28 17:33:12 | INFO | train_inner | {"epoch": 41, "update": 40.815, "loss": "2.231", "ntokens": "175734", "nsentences": "16.4", "wps": "730174", "ups": "4.15", "wpb": "175734", "bsz": "16.4", "num_updates": "63900", "lr": "0.00469573", "gnorm": "0.471", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16432"}
2022-01-28 17:33:36 | INFO | train_inner | {"epoch": 41, "update": 40.879, "loss": "2.219", "ntokens": "174229", "nsentences": "16.56", "wps": "727856", "ups": "4.18", "wpb": "174229", "bsz": "16.6", "num_updates": "64000", "lr": "0.00469479", "gnorm": "0.484", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16456"}
2022-01-28 17:34:00 | INFO | train_inner | {"epoch": 41, "update": 40.943, "loss": "2.227", "ntokens": "174998", "nsentences": "17.52", "wps": "729555", "ups": "4.17", "wpb": "174998", "bsz": "17.5", "num_updates": "64100", "lr": "0.00469385", "gnorm": "0.463", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16480"}
2022-01-28 17:34:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:34:26 | INFO | valid | {"epoch": 41, "valid_loss": "2.269", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.4499e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "64189", "valid_best_loss": "2.269"}
2022-01-28 17:34:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 64189 updates
2022-01-28 17:34:26 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:34:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:34:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 41 @ 64189 updates, score 2.269) (writing took 11.457787618972361 seconds)
2022-01-28 17:34:38 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-28 17:34:38 | INFO | train | {"epoch": 41, "train_loss": "2.228", "train_ntokens": "175186", "train_nsentences": "16.8907", "train_wps": "679867", "train_ups": "3.88", "train_wpb": "175186", "train_bsz": "16.9", "train_num_updates": "64189", "train_lr": "0.00469301", "train_gnorm": "0.478", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "16517"}
2022-01-28 17:34:38 | INFO | fairseq.trainer | begin training epoch 42
2022-01-28 17:34:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:34:52 | INFO | train_inner | {"epoch": 42, "update": 41.007, "loss": "2.218", "ntokens": "174830", "nsentences": "16.56", "wps": "337320", "ups": "1.93", "wpb": "174830", "bsz": "16.6", "num_updates": "64200", "lr": "0.00469291", "gnorm": "0.497", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16532"}
2022-01-28 17:35:16 | INFO | train_inner | {"epoch": 42, "update": 41.071, "loss": "2.211", "ntokens": "175381", "nsentences": "17.2", "wps": "736414", "ups": "4.2", "wpb": "175381", "bsz": "17.2", "num_updates": "64300", "lr": "0.00469196", "gnorm": "0.46", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16555"}
2022-01-28 17:35:39 | INFO | train_inner | {"epoch": 42, "update": 41.135, "loss": "2.194", "ntokens": "175280", "nsentences": "16.96", "wps": "732187", "ups": "4.18", "wpb": "175280", "bsz": "17", "num_updates": "64400", "lr": "0.00469102", "gnorm": "0.459", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16579"}
2022-01-28 17:36:03 | INFO | train_inner | {"epoch": 42, "update": 41.199, "loss": "2.205", "ntokens": "176807", "nsentences": "16.64", "wps": "737357", "ups": "4.17", "wpb": "176807", "bsz": "16.6", "num_updates": "64500", "lr": "0.00469007", "gnorm": "0.464", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16603"}
2022-01-28 17:36:28 | INFO | train_inner | {"epoch": 42, "update": 41.262, "loss": "2.207", "ntokens": "176106", "nsentences": "16.48", "wps": "731752", "ups": "4.16", "wpb": "176106", "bsz": "16.5", "num_updates": "64600", "lr": "0.00468912", "gnorm": "0.468", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16627"}
2022-01-28 17:36:52 | INFO | train_inner | {"epoch": 42, "update": 41.326, "loss": "2.213", "ntokens": "173859", "nsentences": "16.56", "wps": "724227", "ups": "4.17", "wpb": "173859", "bsz": "16.6", "num_updates": "64700", "lr": "0.00468817", "gnorm": "0.529", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16651"}
2022-01-28 17:37:16 | INFO | train_inner | {"epoch": 42, "update": 41.39, "loss": "2.217", "ntokens": "174206", "nsentences": "17.04", "wps": "725259", "ups": "4.16", "wpb": "174206", "bsz": "17", "num_updates": "64800", "lr": "0.00468722", "gnorm": "0.465", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16675"}
2022-01-28 17:37:40 | INFO | train_inner | {"epoch": 42, "update": 41.454, "loss": "2.196", "ntokens": "176785", "nsentences": "16.88", "wps": "734705", "ups": "4.16", "wpb": "176785", "bsz": "16.9", "num_updates": "64900", "lr": "0.00468627", "gnorm": "0.461", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.9", "wall": "16699"}
2022-01-28 17:38:04 | INFO | train_inner | {"epoch": 42, "update": 41.518, "loss": "2.207", "ntokens": "173591", "nsentences": "16.9", "wps": "722915", "ups": "4.16", "wpb": "173591", "bsz": "16.9", "num_updates": "65000", "lr": "0.00468531", "gnorm": "0.472", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16723"}
2022-01-28 17:38:28 | INFO | train_inner | {"epoch": 42, "update": 41.582, "loss": "2.195", "ntokens": "176455", "nsentences": "16.96", "wps": "734806", "ups": "4.16", "wpb": "176455", "bsz": "17", "num_updates": "65100", "lr": "0.00468436", "gnorm": "0.458", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16748"}
2022-01-28 17:38:52 | INFO | train_inner | {"epoch": 42, "update": 41.646, "loss": "2.205", "ntokens": "175696", "nsentences": "16.8", "wps": "729801", "ups": "4.15", "wpb": "175696", "bsz": "16.8", "num_updates": "65200", "lr": "0.0046834", "gnorm": "0.515", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16772"}
2022-01-28 17:39:16 | INFO | train_inner | {"epoch": 42, "update": 41.709, "loss": "2.195", "ntokens": "174756", "nsentences": "16.64", "wps": "726778", "ups": "4.16", "wpb": "174756", "bsz": "16.6", "num_updates": "65300", "lr": "0.00468244", "gnorm": "0.487", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16796"}
2022-01-28 17:39:40 | INFO | train_inner | {"epoch": 42, "update": 41.773, "loss": "2.206", "ntokens": "174956", "nsentences": "16.48", "wps": "728694", "ups": "4.17", "wpb": "174956", "bsz": "16.5", "num_updates": "65400", "lr": "0.00468148", "gnorm": "0.465", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16820"}
2022-01-28 17:40:04 | INFO | train_inner | {"epoch": 42, "update": 41.837, "loss": "2.201", "ntokens": "173983", "nsentences": "18", "wps": "725148", "ups": "4.17", "wpb": "173983", "bsz": "18", "num_updates": "65500", "lr": "0.00468052", "gnorm": "0.459", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "16844"}
2022-01-28 17:40:28 | INFO | train_inner | {"epoch": 42, "update": 41.901, "loss": "2.199", "ntokens": "175339", "nsentences": "17.2", "wps": "730918", "ups": "4.17", "wpb": "175339", "bsz": "17.2", "num_updates": "65600", "lr": "0.00467956", "gnorm": "0.464", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16868"}
2022-01-28 17:40:52 | INFO | train_inner | {"epoch": 42, "update": 41.965, "loss": "2.203", "ntokens": "174960", "nsentences": "16.56", "wps": "726881", "ups": "4.15", "wpb": "174960", "bsz": "16.6", "num_updates": "65700", "lr": "0.0046786", "gnorm": "0.468", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16892"}
2022-01-28 17:41:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:41:10 | INFO | valid | {"epoch": 42, "valid_loss": "2.245", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.51753e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "65755", "valid_best_loss": "2.245"}
2022-01-28 17:41:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 65755 updates
2022-01-28 17:41:10 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:41:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:41:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 42 @ 65755 updates, score 2.245) (writing took 11.689206895418465 seconds)
2022-01-28 17:41:22 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-28 17:41:22 | INFO | train | {"epoch": 42, "train_loss": "2.203", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679147", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "65755", "train_lr": "0.00467807", "train_gnorm": "0.473", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "16921"}
2022-01-28 17:41:22 | INFO | fairseq.trainer | begin training epoch 43
2022-01-28 17:41:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:41:43 | INFO | train_inner | {"epoch": 43, "update": 42.029, "loss": "2.185", "ntokens": "175495", "nsentences": "16.88", "wps": "343093", "ups": "1.95", "wpb": "175495", "bsz": "16.9", "num_updates": "65800", "lr": "0.00467763", "gnorm": "0.472", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16943"}
2022-01-28 17:42:07 | INFO | train_inner | {"epoch": 43, "update": 42.093, "loss": "2.18", "ntokens": "176133", "nsentences": "17.6", "wps": "739056", "ups": "4.2", "wpb": "176133", "bsz": "17.6", "num_updates": "65900", "lr": "0.00467667", "gnorm": "0.465", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16967"}
2022-01-28 17:42:31 | INFO | train_inner | {"epoch": 43, "update": 42.156, "loss": "2.175", "ntokens": "173977", "nsentences": "16.48", "wps": "727165", "ups": "4.18", "wpb": "173977", "bsz": "16.5", "num_updates": "66000", "lr": "0.0046757", "gnorm": "0.483", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "16991"}
2022-01-28 17:42:55 | INFO | train_inner | {"epoch": 43, "update": 42.22, "loss": "2.175", "ntokens": "174621", "nsentences": "16.72", "wps": "728077", "ups": "4.17", "wpb": "174621", "bsz": "16.7", "num_updates": "66100", "lr": "0.00467473", "gnorm": "0.455", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17015"}
2022-01-28 17:43:19 | INFO | train_inner | {"epoch": 43, "update": 42.284, "loss": "2.189", "ntokens": "175519", "nsentences": "16.32", "wps": "731906", "ups": "4.17", "wpb": "175519", "bsz": "16.3", "num_updates": "66200", "lr": "0.00467376", "gnorm": "0.496", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17039"}
2022-01-28 17:43:43 | INFO | train_inner | {"epoch": 43, "update": 42.348, "loss": "2.189", "ntokens": "174519", "nsentences": "16.24", "wps": "726940", "ups": "4.17", "wpb": "174519", "bsz": "16.2", "num_updates": "66300", "lr": "0.00467279", "gnorm": "0.482", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17063"}
2022-01-28 17:44:07 | INFO | train_inner | {"epoch": 43, "update": 42.412, "loss": "2.204", "ntokens": "175221", "nsentences": "17.6", "wps": "728566", "ups": "4.16", "wpb": "175221", "bsz": "17.6", "num_updates": "66400", "lr": "0.00467181", "gnorm": "0.452", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17087"}
2022-01-28 17:44:31 | INFO | train_inner | {"epoch": 43, "update": 42.476, "loss": "2.195", "ntokens": "176653", "nsentences": "17.28", "wps": "734284", "ups": "4.16", "wpb": "176653", "bsz": "17.3", "num_updates": "66500", "lr": "0.00467084", "gnorm": "0.449", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17111"}
2022-01-28 17:44:55 | INFO | train_inner | {"epoch": 43, "update": 42.54, "loss": "2.186", "ntokens": "175287", "nsentences": "17.2", "wps": "731070", "ups": "4.17", "wpb": "175287", "bsz": "17.2", "num_updates": "66600", "lr": "0.00466986", "gnorm": "0.444", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.5", "wall": "17135"}
2022-01-28 17:45:19 | INFO | train_inner | {"epoch": 43, "update": 42.603, "loss": "2.181", "ntokens": "176905", "nsentences": "16.96", "wps": "735248", "ups": "4.16", "wpb": "176905", "bsz": "17", "num_updates": "66700", "lr": "0.00466889", "gnorm": "0.487", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17159"}
2022-01-28 17:45:43 | INFO | train_inner | {"epoch": 43, "update": 42.667, "loss": "2.166", "ntokens": "174247", "nsentences": "16.9", "wps": "726126", "ups": "4.17", "wpb": "174247", "bsz": "16.9", "num_updates": "66800", "lr": "0.00466791", "gnorm": "0.421", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17183"}
2022-01-28 17:46:07 | INFO | train_inner | {"epoch": 43, "update": 42.731, "loss": "2.171", "ntokens": "174544", "nsentences": "16.56", "wps": "727033", "ups": "4.17", "wpb": "174544", "bsz": "16.6", "num_updates": "66900", "lr": "0.00466693", "gnorm": "0.468", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17207"}
2022-01-28 17:46:31 | INFO | train_inner | {"epoch": 43, "update": 42.795, "loss": "2.175", "ntokens": "174945", "nsentences": "17.28", "wps": "728087", "ups": "4.16", "wpb": "174945", "bsz": "17.3", "num_updates": "67000", "lr": "0.00466595", "gnorm": "0.492", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17231"}
2022-01-28 17:46:55 | INFO | train_inner | {"epoch": 43, "update": 42.859, "loss": "2.165", "ntokens": "173865", "nsentences": "16.8", "wps": "723025", "ups": "4.16", "wpb": "173865", "bsz": "16.8", "num_updates": "67100", "lr": "0.00466497", "gnorm": "0.454", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "19.9", "wall": "17255"}
2022-01-28 17:47:19 | INFO | train_inner | {"epoch": 43, "update": 42.923, "loss": "2.17", "ntokens": "174938", "nsentences": "16.96", "wps": "729090", "ups": "4.17", "wpb": "174938", "bsz": "17", "num_updates": "67200", "lr": "0.00466398", "gnorm": "0.468", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17279"}
2022-01-28 17:47:43 | INFO | train_inner | {"epoch": 43, "update": 42.987, "loss": "2.15", "ntokens": "175917", "nsentences": "16.48", "wps": "731095", "ups": "4.16", "wpb": "175917", "bsz": "16.5", "num_updates": "67300", "lr": "0.004663", "gnorm": "0.494", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17303"}
2022-01-28 17:47:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:47:53 | INFO | valid | {"epoch": 43, "valid_loss": "2.195", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.70285e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "67321", "valid_best_loss": "2.195"}
2022-01-28 17:47:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 67321 updates
2022-01-28 17:47:53 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:47:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:48:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 43 @ 67321 updates, score 2.195) (writing took 11.919011401943862 seconds)
2022-01-28 17:48:05 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-28 17:48:05 | INFO | train | {"epoch": 43, "train_loss": "2.178", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680262", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "67321", "train_lr": "0.00466279", "train_gnorm": "0.466", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.6", "train_wall": "17325"}
2022-01-28 17:48:05 | INFO | fairseq.trainer | begin training epoch 44
2022-01-28 17:48:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:48:35 | INFO | train_inner | {"epoch": 44, "update": 43.05, "loss": "2.144", "ntokens": "175419", "nsentences": "16.74", "wps": "342326", "ups": "1.95", "wpb": "175419", "bsz": "16.7", "num_updates": "67400", "lr": "0.00466201", "gnorm": "0.453", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17354"}
2022-01-28 17:48:58 | INFO | train_inner | {"epoch": 44, "update": 43.114, "loss": "2.134", "ntokens": "174692", "nsentences": "16.48", "wps": "731833", "ups": "4.19", "wpb": "174692", "bsz": "16.5", "num_updates": "67500", "lr": "0.00466102", "gnorm": "0.466", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17378"}
2022-01-28 17:49:22 | INFO | train_inner | {"epoch": 44, "update": 43.178, "loss": "2.143", "ntokens": "176038", "nsentences": "16.88", "wps": "732799", "ups": "4.16", "wpb": "176038", "bsz": "16.9", "num_updates": "67600", "lr": "0.00466003", "gnorm": "0.463", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.2", "wall": "17402"}
2022-01-28 17:49:46 | INFO | train_inner | {"epoch": 44, "update": 43.242, "loss": "2.139", "ntokens": "174344", "nsentences": "17.28", "wps": "729123", "ups": "4.18", "wpb": "174344", "bsz": "17.3", "num_updates": "67700", "lr": "0.00465904", "gnorm": "0.456", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17426"}
2022-01-28 17:50:10 | INFO | train_inner | {"epoch": 44, "update": 43.306, "loss": "2.142", "ntokens": "173638", "nsentences": "16.64", "wps": "723251", "ups": "4.17", "wpb": "173638", "bsz": "16.6", "num_updates": "67800", "lr": "0.00465805", "gnorm": "0.478", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17450"}
2022-01-28 17:50:34 | INFO | train_inner | {"epoch": 44, "update": 43.37, "loss": "2.131", "ntokens": "175681", "nsentences": "17.52", "wps": "732014", "ups": "4.17", "wpb": "175681", "bsz": "17.5", "num_updates": "67900", "lr": "0.00465706", "gnorm": "0.456", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17474"}
2022-01-28 17:50:58 | INFO | train_inner | {"epoch": 44, "update": 43.434, "loss": "2.128", "ntokens": "175985", "nsentences": "16.88", "wps": "730725", "ups": "4.15", "wpb": "175985", "bsz": "16.9", "num_updates": "68000", "lr": "0.00465607", "gnorm": "0.479", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17498"}
2022-01-28 17:51:23 | INFO | train_inner | {"epoch": 44, "update": 43.497, "loss": "2.136", "ntokens": "174916", "nsentences": "16.8", "wps": "727421", "ups": "4.16", "wpb": "174916", "bsz": "16.8", "num_updates": "68100", "lr": "0.00465507", "gnorm": "0.472", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17522"}
2022-01-28 17:51:47 | INFO | train_inner | {"epoch": 44, "update": 43.561, "loss": "2.118", "ntokens": "176160", "nsentences": "16.72", "wps": "731792", "ups": "4.15", "wpb": "176160", "bsz": "16.7", "num_updates": "68200", "lr": "0.00465407", "gnorm": "0.455", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17546"}
2022-01-28 17:52:11 | INFO | train_inner | {"epoch": 44, "update": 43.625, "loss": "2.15", "ntokens": "174655", "nsentences": "17.44", "wps": "726724", "ups": "4.16", "wpb": "174655", "bsz": "17.4", "num_updates": "68300", "lr": "0.00465307", "gnorm": "0.47", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17570"}
2022-01-28 17:52:35 | INFO | train_inner | {"epoch": 44, "update": 43.689, "loss": "2.125", "ntokens": "175989", "nsentences": "17.2", "wps": "732192", "ups": "4.16", "wpb": "175989", "bsz": "17.2", "num_updates": "68400", "lr": "0.00465207", "gnorm": "0.447", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17594"}
2022-01-28 17:52:59 | INFO | train_inner | {"epoch": 44, "update": 43.753, "loss": "2.122", "ntokens": "173952", "nsentences": "16.4", "wps": "728233", "ups": "4.19", "wpb": "173952", "bsz": "16.4", "num_updates": "68500", "lr": "0.00465107", "gnorm": "0.435", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17618"}
2022-01-28 17:53:23 | INFO | train_inner | {"epoch": 44, "update": 43.817, "loss": "2.123", "ntokens": "175307", "nsentences": "16.48", "wps": "729990", "ups": "4.16", "wpb": "175307", "bsz": "16.5", "num_updates": "68600", "lr": "0.00465007", "gnorm": "0.472", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17642"}
2022-01-28 17:53:47 | INFO | train_inner | {"epoch": 44, "update": 43.881, "loss": "2.126", "ntokens": "174738", "nsentences": "16.48", "wps": "728540", "ups": "4.17", "wpb": "174738", "bsz": "16.5", "num_updates": "68700", "lr": "0.00464907", "gnorm": "0.506", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17666"}
2022-01-28 17:54:11 | INFO | train_inner | {"epoch": 44, "update": 43.944, "loss": "2.123", "ntokens": "174826", "nsentences": "17.12", "wps": "729346", "ups": "4.17", "wpb": "174826", "bsz": "17.1", "num_updates": "68800", "lr": "0.00464806", "gnorm": "0.459", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17690"}
2022-01-28 17:54:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:54:36 | INFO | valid | {"epoch": 44, "valid_loss": "2.164", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.68629e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "68887", "valid_best_loss": "2.164"}
2022-01-28 17:54:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 68887 updates
2022-01-28 17:54:36 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:54:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 17:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 44 @ 68887 updates, score 2.164) (writing took 11.290211440995336 seconds)
2022-01-28 17:54:48 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-28 17:54:48 | INFO | train | {"epoch": 44, "train_loss": "2.131", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "681320", "train_ups": "3.89", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "68887", "train_lr": "0.00464719", "train_gnorm": "0.463", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "17727"}
2022-01-28 17:54:48 | INFO | fairseq.trainer | begin training epoch 45
2022-01-28 17:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:55:01 | INFO | train_inner | {"epoch": 45, "update": 44.008, "loss": "2.122", "ntokens": "176290", "nsentences": "17.68", "wps": "346857", "ups": "1.97", "wpb": "176290", "bsz": "17.7", "num_updates": "68900", "lr": "0.00464706", "gnorm": "0.44", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17741"}
2022-01-28 17:55:25 | INFO | train_inner | {"epoch": 45, "update": 44.072, "loss": "2.118", "ntokens": "175962", "nsentences": "17.2", "wps": "739559", "ups": "4.2", "wpb": "175962", "bsz": "17.2", "num_updates": "69000", "lr": "0.00464605", "gnorm": "0.477", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17765"}
2022-01-28 17:55:49 | INFO | train_inner | {"epoch": 45, "update": 44.136, "loss": "2.105", "ntokens": "174704", "nsentences": "16.64", "wps": "729916", "ups": "4.18", "wpb": "174704", "bsz": "16.6", "num_updates": "69100", "lr": "0.00464504", "gnorm": "0.475", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.7", "wall": "17789"}
2022-01-28 17:56:13 | INFO | train_inner | {"epoch": 45, "update": 44.2, "loss": "2.111", "ntokens": "174710", "nsentences": "17.2", "wps": "728523", "ups": "4.17", "wpb": "174710", "bsz": "17.2", "num_updates": "69200", "lr": "0.00464403", "gnorm": "0.476", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20", "wall": "17813"}
2022-01-28 17:56:37 | INFO | train_inner | {"epoch": 45, "update": 44.264, "loss": "2.1", "ntokens": "174962", "nsentences": "16.48", "wps": "730201", "ups": "4.17", "wpb": "174962", "bsz": "16.5", "num_updates": "69300", "lr": "0.00464302", "gnorm": "0.453", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17837"}
2022-01-28 17:57:01 | INFO | train_inner | {"epoch": 45, "update": 44.328, "loss": "2.1", "ntokens": "176256", "nsentences": "17.68", "wps": "732725", "ups": "4.16", "wpb": "176256", "bsz": "17.7", "num_updates": "69400", "lr": "0.004642", "gnorm": "0.46", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17861"}
2022-01-28 17:57:25 | INFO | train_inner | {"epoch": 45, "update": 44.391, "loss": "2.114", "ntokens": "173857", "nsentences": "16.58", "wps": "725747", "ups": "4.17", "wpb": "173857", "bsz": "16.6", "num_updates": "69500", "lr": "0.00464099", "gnorm": "0.443", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17885"}
2022-01-28 17:57:49 | INFO | train_inner | {"epoch": 45, "update": 44.455, "loss": "2.12", "ntokens": "174845", "nsentences": "17.52", "wps": "726408", "ups": "4.15", "wpb": "174845", "bsz": "17.5", "num_updates": "69600", "lr": "0.00463997", "gnorm": "0.47", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.5", "wall": "17909"}
2022-01-28 17:58:13 | INFO | train_inner | {"epoch": 45, "update": 44.519, "loss": "2.102", "ntokens": "174729", "nsentences": "16.48", "wps": "727186", "ups": "4.16", "wpb": "174729", "bsz": "16.5", "num_updates": "69700", "lr": "0.00463896", "gnorm": "0.455", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17933"}
2022-01-28 17:58:37 | INFO | train_inner | {"epoch": 45, "update": 44.583, "loss": "2.092", "ntokens": "176724", "nsentences": "17.28", "wps": "734276", "ups": "4.15", "wpb": "176724", "bsz": "17.3", "num_updates": "69800", "lr": "0.00463794", "gnorm": "0.451", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "17957"}
2022-01-28 17:59:01 | INFO | train_inner | {"epoch": 45, "update": 44.647, "loss": "2.08", "ntokens": "176442", "nsentences": "16.32", "wps": "732776", "ups": "4.15", "wpb": "176442", "bsz": "16.3", "num_updates": "69900", "lr": "0.00463692", "gnorm": "0.465", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.8", "wall": "17981"}
2022-01-28 17:59:25 | INFO | train_inner | {"epoch": 45, "update": 44.711, "loss": "2.094", "ntokens": "174542", "nsentences": "16.24", "wps": "726351", "ups": "4.16", "wpb": "174542", "bsz": "16.2", "num_updates": "70000", "lr": "0.0046359", "gnorm": "0.466", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18005"}
2022-01-28 17:59:49 | INFO | train_inner | {"epoch": 45, "update": 44.775, "loss": "2.114", "ntokens": "174654", "nsentences": "16.56", "wps": "727904", "ups": "4.17", "wpb": "174654", "bsz": "16.6", "num_updates": "70100", "lr": "0.00463488", "gnorm": "0.477", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18029"}
2022-01-28 18:00:14 | INFO | train_inner | {"epoch": 45, "update": 44.838, "loss": "2.106", "ntokens": "175147", "nsentences": "17.52", "wps": "728498", "ups": "4.16", "wpb": "175147", "bsz": "17.5", "num_updates": "70200", "lr": "0.00463385", "gnorm": "0.428", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18053"}
2022-01-28 18:00:38 | INFO | train_inner | {"epoch": 45, "update": 44.902, "loss": "2.098", "ntokens": "175498", "nsentences": "16.8", "wps": "731885", "ups": "4.17", "wpb": "175498", "bsz": "16.8", "num_updates": "70300", "lr": "0.00463283", "gnorm": "0.45", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.2", "wall": "18077"}
2022-01-28 18:01:02 | INFO | train_inner | {"epoch": 45, "update": 44.966, "loss": "2.089", "ntokens": "174794", "nsentences": "16.56", "wps": "727022", "ups": "4.16", "wpb": "174794", "bsz": "16.6", "num_updates": "70400", "lr": "0.0046318", "gnorm": "0.435", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18101"}
2022-01-28 18:01:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:01:19 | INFO | valid | {"epoch": 45, "valid_loss": "2.152", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.74343e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "70453", "valid_best_loss": "2.152"}
2022-01-28 18:01:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 70453 updates
2022-01-28 18:01:19 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:01:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:01:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 45 @ 70453 updates, score 2.152) (writing took 12.441463205963373 seconds)
2022-01-28 18:01:31 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-28 18:01:31 | INFO | train | {"epoch": 45, "train_loss": "2.103", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679513", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "70453", "train_lr": "0.00463126", "train_gnorm": "0.459", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "18131"}
2022-01-28 18:01:31 | INFO | fairseq.trainer | begin training epoch 46
2022-01-28 18:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:01:53 | INFO | train_inner | {"epoch": 46, "update": 45.03, "loss": "2.086", "ntokens": "174680", "nsentences": "16.8", "wps": "336901", "ups": "1.93", "wpb": "174680", "bsz": "16.8", "num_updates": "70500", "lr": "0.00463077", "gnorm": "0.459", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18153"}
2022-01-28 18:02:17 | INFO | train_inner | {"epoch": 46, "update": 45.094, "loss": "2.085", "ntokens": "175177", "nsentences": "16.48", "wps": "734812", "ups": "4.19", "wpb": "175177", "bsz": "16.5", "num_updates": "70600", "lr": "0.00462975", "gnorm": "0.478", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18177"}
2022-01-28 18:02:41 | INFO | train_inner | {"epoch": 46, "update": 45.158, "loss": "2.078", "ntokens": "176037", "nsentences": "16.96", "wps": "734245", "ups": "4.17", "wpb": "176037", "bsz": "17", "num_updates": "70700", "lr": "0.00462872", "gnorm": "0.483", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18201"}
2022-01-28 18:03:05 | INFO | train_inner | {"epoch": 46, "update": 45.222, "loss": "2.077", "ntokens": "175348", "nsentences": "16.98", "wps": "730228", "ups": "4.16", "wpb": "175348", "bsz": "17", "num_updates": "70800", "lr": "0.00462768", "gnorm": "0.453", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18225"}
2022-01-28 18:03:29 | INFO | train_inner | {"epoch": 46, "update": 45.285, "loss": "2.081", "ntokens": "174806", "nsentences": "16.24", "wps": "730098", "ups": "4.18", "wpb": "174806", "bsz": "16.2", "num_updates": "70900", "lr": "0.00462665", "gnorm": "0.456", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18249"}
2022-01-28 18:03:53 | INFO | train_inner | {"epoch": 46, "update": 45.349, "loss": "2.072", "ntokens": "176049", "nsentences": "16.72", "wps": "732406", "ups": "4.16", "wpb": "176049", "bsz": "16.7", "num_updates": "71000", "lr": "0.00462562", "gnorm": "0.441", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "19.9", "wall": "18273"}
2022-01-28 18:04:17 | INFO | train_inner | {"epoch": 46, "update": 45.413, "loss": "2.083", "ntokens": "175026", "nsentences": "17.04", "wps": "729780", "ups": "4.17", "wpb": "175026", "bsz": "17", "num_updates": "71100", "lr": "0.00462458", "gnorm": "0.473", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18297"}
2022-01-28 18:04:41 | INFO | train_inner | {"epoch": 46, "update": 45.477, "loss": "2.086", "ntokens": "175001", "nsentences": "16.56", "wps": "730035", "ups": "4.17", "wpb": "175001", "bsz": "16.6", "num_updates": "71200", "lr": "0.00462355", "gnorm": "0.454", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18321"}
2022-01-28 18:05:05 | INFO | train_inner | {"epoch": 46, "update": 45.541, "loss": "2.081", "ntokens": "175133", "nsentences": "16.88", "wps": "728432", "ups": "4.16", "wpb": "175133", "bsz": "16.9", "num_updates": "71300", "lr": "0.00462251", "gnorm": "0.447", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20", "wall": "18345"}
2022-01-28 18:05:29 | INFO | train_inner | {"epoch": 46, "update": 45.605, "loss": "2.08", "ntokens": "174794", "nsentences": "16.72", "wps": "730738", "ups": "4.18", "wpb": "174794", "bsz": "16.7", "num_updates": "71400", "lr": "0.00462147", "gnorm": "0.46", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18369"}
2022-01-28 18:05:53 | INFO | train_inner | {"epoch": 46, "update": 45.669, "loss": "2.087", "ntokens": "175992", "nsentences": "16.88", "wps": "731132", "ups": "4.15", "wpb": "175992", "bsz": "16.9", "num_updates": "71500", "lr": "0.00462043", "gnorm": "0.447", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18393"}
2022-01-28 18:06:17 | INFO | train_inner | {"epoch": 46, "update": 45.732, "loss": "2.076", "ntokens": "175508", "nsentences": "16.72", "wps": "730542", "ups": "4.16", "wpb": "175508", "bsz": "16.7", "num_updates": "71600", "lr": "0.00461939", "gnorm": "0.454", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "18417"}
2022-01-28 18:06:41 | INFO | train_inner | {"epoch": 46, "update": 45.796, "loss": "2.092", "ntokens": "175661", "nsentences": "17.76", "wps": "729967", "ups": "4.16", "wpb": "175661", "bsz": "17.8", "num_updates": "71700", "lr": "0.00461834", "gnorm": "0.468", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "21", "wall": "18441"}
2022-01-28 18:07:05 | INFO | train_inner | {"epoch": 46, "update": 45.86, "loss": "2.072", "ntokens": "175020", "nsentences": "17.12", "wps": "730067", "ups": "4.17", "wpb": "175020", "bsz": "17.1", "num_updates": "71800", "lr": "0.0046173", "gnorm": "0.456", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18465"}
2022-01-28 18:07:29 | INFO | train_inner | {"epoch": 46, "update": 45.924, "loss": "2.085", "ntokens": "174620", "nsentences": "16.8", "wps": "725871", "ups": "4.16", "wpb": "174620", "bsz": "16.8", "num_updates": "71900", "lr": "0.00461625", "gnorm": "0.472", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18489"}
2022-01-28 18:07:54 | INFO | train_inner | {"epoch": 46, "update": 45.988, "loss": "2.086", "ntokens": "174060", "nsentences": "17.44", "wps": "723993", "ups": "4.16", "wpb": "174060", "bsz": "17.4", "num_updates": "72000", "lr": "0.0046152", "gnorm": "0.447", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18513"}
2022-01-28 18:07:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:08:03 | INFO | valid | {"epoch": 46, "valid_loss": "2.135", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.67637e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "72019", "valid_best_loss": "2.135"}
2022-01-28 18:08:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 72019 updates
2022-01-28 18:08:03 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:08:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 46 @ 72019 updates, score 2.135) (writing took 11.648308170959353 seconds)
2022-01-28 18:08:14 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-28 18:08:15 | INFO | train | {"epoch": 46, "train_loss": "2.082", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680358", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "72019", "train_lr": "0.00461501", "train_gnorm": "0.459", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "18534"}
2022-01-28 18:08:15 | INFO | fairseq.trainer | begin training epoch 47
2022-01-28 18:08:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:08:45 | INFO | train_inner | {"epoch": 47, "update": 46.052, "loss": "2.084", "ntokens": "175910", "nsentences": "17.2", "wps": "343563", "ups": "1.95", "wpb": "175910", "bsz": "17.2", "num_updates": "72100", "lr": "0.00461416", "gnorm": "0.473", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18565"}
2022-01-28 18:09:09 | INFO | train_inner | {"epoch": 47, "update": 46.116, "loss": "2.057", "ntokens": "175592", "nsentences": "16.8", "wps": "733800", "ups": "4.18", "wpb": "175592", "bsz": "16.8", "num_updates": "72200", "lr": "0.00461311", "gnorm": "0.439", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18588"}
2022-01-28 18:09:33 | INFO | train_inner | {"epoch": 47, "update": 46.179, "loss": "2.049", "ntokens": "175204", "nsentences": "16.32", "wps": "730436", "ups": "4.17", "wpb": "175204", "bsz": "16.3", "num_updates": "72300", "lr": "0.00461206", "gnorm": "0.463", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18612"}
2022-01-28 18:09:57 | INFO | train_inner | {"epoch": 47, "update": 46.243, "loss": "2.068", "ntokens": "172730", "nsentences": "16.56", "wps": "723281", "ups": "4.19", "wpb": "172730", "bsz": "16.6", "num_updates": "72400", "lr": "0.004611", "gnorm": "0.446", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18636"}
2022-01-28 18:10:21 | INFO | train_inner | {"epoch": 47, "update": 46.307, "loss": "2.063", "ntokens": "175135", "nsentences": "16.88", "wps": "731526", "ups": "4.18", "wpb": "175135", "bsz": "16.9", "num_updates": "72500", "lr": "0.00460995", "gnorm": "0.446", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18660"}
2022-01-28 18:10:45 | INFO | train_inner | {"epoch": 47, "update": 46.371, "loss": "2.072", "ntokens": "175568", "nsentences": "16.56", "wps": "730132", "ups": "4.16", "wpb": "175568", "bsz": "16.6", "num_updates": "72600", "lr": "0.00460889", "gnorm": "0.496", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18684"}
2022-01-28 18:11:09 | INFO | train_inner | {"epoch": 47, "update": 46.435, "loss": "2.056", "ntokens": "174775", "nsentences": "16.66", "wps": "728512", "ups": "4.17", "wpb": "174775", "bsz": "16.7", "num_updates": "72700", "lr": "0.00460784", "gnorm": "0.481", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18708"}
2022-01-28 18:11:33 | INFO | train_inner | {"epoch": 47, "update": 46.499, "loss": "2.073", "ntokens": "175458", "nsentences": "17.84", "wps": "730203", "ups": "4.16", "wpb": "175458", "bsz": "17.8", "num_updates": "72800", "lr": "0.00460678", "gnorm": "0.423", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18732"}
2022-01-28 18:11:57 | INFO | train_inner | {"epoch": 47, "update": 46.563, "loss": "2.077", "ntokens": "175660", "nsentences": "16.8", "wps": "728682", "ups": "4.15", "wpb": "175660", "bsz": "16.8", "num_updates": "72900", "lr": "0.00460572", "gnorm": "0.457", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18757"}
2022-01-28 18:12:21 | INFO | train_inner | {"epoch": 47, "update": 46.626, "loss": "2.053", "ntokens": "176293", "nsentences": "17.12", "wps": "730516", "ups": "4.14", "wpb": "176293", "bsz": "17.1", "num_updates": "73000", "lr": "0.00460466", "gnorm": "0.452", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18781"}
2022-01-28 18:12:45 | INFO | train_inner | {"epoch": 47, "update": 46.69, "loss": "2.062", "ntokens": "173837", "nsentences": "16.72", "wps": "723139", "ups": "4.16", "wpb": "173837", "bsz": "16.7", "num_updates": "73100", "lr": "0.0046036", "gnorm": "0.446", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18805"}
2022-01-28 18:13:09 | INFO | train_inner | {"epoch": 47, "update": 46.754, "loss": "2.079", "ntokens": "174467", "nsentences": "17.52", "wps": "726728", "ups": "4.17", "wpb": "174467", "bsz": "17.5", "num_updates": "73200", "lr": "0.00460254", "gnorm": "0.472", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18829"}
2022-01-28 18:13:33 | INFO | train_inner | {"epoch": 47, "update": 46.818, "loss": "2.069", "ntokens": "174963", "nsentences": "16.8", "wps": "727343", "ups": "4.16", "wpb": "174963", "bsz": "16.8", "num_updates": "73300", "lr": "0.00460147", "gnorm": "0.461", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18853"}
2022-01-28 18:13:57 | INFO | train_inner | {"epoch": 47, "update": 46.882, "loss": "2.083", "ntokens": "175819", "nsentences": "17.12", "wps": "731395", "ups": "4.16", "wpb": "175819", "bsz": "17.1", "num_updates": "73400", "lr": "0.00460041", "gnorm": "0.442", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18877"}
2022-01-28 18:14:21 | INFO | train_inner | {"epoch": 47, "update": 46.946, "loss": "2.061", "ntokens": "175667", "nsentences": "16.8", "wps": "730958", "ups": "4.16", "wpb": "175667", "bsz": "16.8", "num_updates": "73500", "lr": "0.00459934", "gnorm": "0.454", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18901"}
2022-01-28 18:14:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:14:46 | INFO | valid | {"epoch": 47, "valid_loss": "2.116", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.59587e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "73585", "valid_best_loss": "2.116"}
2022-01-28 18:14:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 73585 updates
2022-01-28 18:14:46 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:14:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 47 @ 73585 updates, score 2.116) (writing took 11.303304797038436 seconds)
2022-01-28 18:14:58 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-28 18:14:58 | INFO | train | {"epoch": 47, "train_loss": "2.066", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680660", "train_ups": "3.89", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "73585", "train_lr": "0.00459843", "train_gnorm": "0.456", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "18937"}
2022-01-28 18:14:58 | INFO | fairseq.trainer | begin training epoch 48
2022-01-28 18:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:15:12 | INFO | train_inner | {"epoch": 48, "update": 47.01, "loss": "2.053", "ntokens": "175778", "nsentences": "16.64", "wps": "343070", "ups": "1.95", "wpb": "175778", "bsz": "16.6", "num_updates": "73600", "lr": "0.00459827", "gnorm": "0.442", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "18952"}
2022-01-28 18:15:36 | INFO | train_inner | {"epoch": 48, "update": 47.073, "loss": "2.063", "ntokens": "173726", "nsentences": "16.8", "wps": "729988", "ups": "4.2", "wpb": "173726", "bsz": "16.8", "num_updates": "73700", "lr": "0.0045972", "gnorm": "0.467", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.9", "wall": "18976"}
2022-01-28 18:16:00 | INFO | train_inner | {"epoch": 48, "update": 47.137, "loss": "2.056", "ntokens": "175993", "nsentences": "16.96", "wps": "734578", "ups": "4.17", "wpb": "175993", "bsz": "17", "num_updates": "73800", "lr": "0.00459613", "gnorm": "0.45", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19000"}
2022-01-28 18:16:24 | INFO | train_inner | {"epoch": 48, "update": 47.201, "loss": "2.049", "ntokens": "174849", "nsentences": "16.64", "wps": "729929", "ups": "4.17", "wpb": "174849", "bsz": "16.6", "num_updates": "73900", "lr": "0.00459506", "gnorm": "0.444", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19024"}
2022-01-28 18:16:48 | INFO | train_inner | {"epoch": 48, "update": 47.265, "loss": "2.046", "ntokens": "177041", "nsentences": "17.52", "wps": "737244", "ups": "4.16", "wpb": "177041", "bsz": "17.5", "num_updates": "74000", "lr": "0.00459399", "gnorm": "0.47", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19048"}
2022-01-28 18:17:12 | INFO | train_inner | {"epoch": 48, "update": 47.329, "loss": "2.042", "ntokens": "175809", "nsentences": "16.64", "wps": "733084", "ups": "4.17", "wpb": "175809", "bsz": "16.6", "num_updates": "74100", "lr": "0.00459291", "gnorm": "0.442", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19072"}
2022-01-28 18:17:36 | INFO | train_inner | {"epoch": 48, "update": 47.393, "loss": "2.049", "ntokens": "175006", "nsentences": "17.12", "wps": "728277", "ups": "4.16", "wpb": "175006", "bsz": "17.1", "num_updates": "74200", "lr": "0.00459184", "gnorm": "0.476", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19096"}
2022-01-28 18:18:00 | INFO | train_inner | {"epoch": 48, "update": 47.457, "loss": "2.051", "ntokens": "175086", "nsentences": "17.7", "wps": "727749", "ups": "4.16", "wpb": "175086", "bsz": "17.7", "num_updates": "74300", "lr": "0.00459076", "gnorm": "0.448", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19120"}
2022-01-28 18:18:24 | INFO | train_inner | {"epoch": 48, "update": 47.52, "loss": "2.044", "ntokens": "176700", "nsentences": "16.48", "wps": "733059", "ups": "4.15", "wpb": "176700", "bsz": "16.5", "num_updates": "74400", "lr": "0.00458968", "gnorm": "0.425", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19144"}
2022-01-28 18:18:48 | INFO | train_inner | {"epoch": 48, "update": 47.584, "loss": "2.05", "ntokens": "175006", "nsentences": "16.4", "wps": "728805", "ups": "4.16", "wpb": "175006", "bsz": "16.4", "num_updates": "74500", "lr": "0.0045886", "gnorm": "0.419", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19168"}
2022-01-28 18:19:12 | INFO | train_inner | {"epoch": 48, "update": 47.648, "loss": "2.053", "ntokens": "174388", "nsentences": "16.56", "wps": "726892", "ups": "4.17", "wpb": "174388", "bsz": "16.6", "num_updates": "74600", "lr": "0.00458752", "gnorm": "0.473", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19192"}
2022-01-28 18:19:36 | INFO | train_inner | {"epoch": 48, "update": 47.712, "loss": "2.05", "ntokens": "175988", "nsentences": "17.04", "wps": "731683", "ups": "4.16", "wpb": "175988", "bsz": "17", "num_updates": "74700", "lr": "0.00458644", "gnorm": "0.459", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19216"}
2022-01-28 18:20:00 | INFO | train_inner | {"epoch": 48, "update": 47.776, "loss": "2.059", "ntokens": "174015", "nsentences": "17.04", "wps": "723054", "ups": "4.16", "wpb": "174015", "bsz": "17", "num_updates": "74800", "lr": "0.00458535", "gnorm": "0.466", "loss_scale": "0.125", "train_wall": "24", "gb_free": "21.4", "wall": "19240"}
2022-01-28 18:20:25 | INFO | train_inner | {"epoch": 48, "update": 47.84, "loss": "2.049", "ntokens": "174072", "nsentences": "16.88", "wps": "724721", "ups": "4.16", "wpb": "174072", "bsz": "16.9", "num_updates": "74900", "lr": "0.00458427", "gnorm": "0.453", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19264"}
2022-01-28 18:20:49 | INFO | train_inner | {"epoch": 48, "update": 47.904, "loss": "2.073", "ntokens": "174788", "nsentences": "17.44", "wps": "727791", "ups": "4.16", "wpb": "174788", "bsz": "17.4", "num_updates": "75000", "lr": "0.00458318", "gnorm": "0.436", "loss_scale": "0.125", "train_wall": "24", "gb_free": "21.1", "wall": "19288"}
2022-01-28 18:21:13 | INFO | train_inner | {"epoch": 48, "update": 47.967, "loss": "2.049", "ntokens": "175518", "nsentences": "16.56", "wps": "728582", "ups": "4.15", "wpb": "175518", "bsz": "16.6", "num_updates": "75100", "lr": "0.0045821", "gnorm": "0.439", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.6", "wall": "19312"}
2022-01-28 18:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:21:30 | INFO | valid | {"epoch": 48, "valid_loss": "2.103", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.41558e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "75151", "valid_best_loss": "2.103"}
2022-01-28 18:21:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 75151 updates
2022-01-28 18:21:30 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:21:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 48 @ 75151 updates, score 2.103) (writing took 11.977328164502978 seconds)
2022-01-28 18:21:42 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-28 18:21:42 | INFO | train | {"epoch": 48, "train_loss": "2.051", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "678896", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "75151", "train_lr": "0.00458154", "train_gnorm": "0.449", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "19341"}
2022-01-28 18:21:42 | INFO | fairseq.trainer | begin training epoch 49
2022-01-28 18:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:22:04 | INFO | train_inner | {"epoch": 49, "update": 48.031, "loss": "2.046", "ntokens": "174465", "nsentences": "16.4", "wps": "338521", "ups": "1.94", "wpb": "174465", "bsz": "16.4", "num_updates": "75200", "lr": "0.00458101", "gnorm": "0.432", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19364"}
2022-01-28 18:22:28 | INFO | train_inner | {"epoch": 49, "update": 48.095, "loss": "2.041", "ntokens": "176097", "nsentences": "17.6", "wps": "737489", "ups": "4.19", "wpb": "176097", "bsz": "17.6", "num_updates": "75300", "lr": "0.00457992", "gnorm": "0.451", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19388"}
2022-01-28 18:22:52 | INFO | train_inner | {"epoch": 49, "update": 48.159, "loss": "2.035", "ntokens": "173591", "nsentences": "16.64", "wps": "724149", "ups": "4.17", "wpb": "173591", "bsz": "16.6", "num_updates": "75400", "lr": "0.00457883", "gnorm": "0.407", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19412"}
2022-01-28 18:23:16 | INFO | train_inner | {"epoch": 49, "update": 48.223, "loss": "2.032", "ntokens": "175957", "nsentences": "16.64", "wps": "736325", "ups": "4.18", "wpb": "175957", "bsz": "16.6", "num_updates": "75500", "lr": "0.00457773", "gnorm": "0.436", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19436"}
2022-01-28 18:23:40 | INFO | train_inner | {"epoch": 49, "update": 48.287, "loss": "2.035", "ntokens": "175437", "nsentences": "16.56", "wps": "729564", "ups": "4.16", "wpb": "175437", "bsz": "16.6", "num_updates": "75600", "lr": "0.00457664", "gnorm": "0.494", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19460"}
2022-01-28 18:24:04 | INFO | train_inner | {"epoch": 49, "update": 48.351, "loss": "2.043", "ntokens": "175843", "nsentences": "17.68", "wps": "732236", "ups": "4.16", "wpb": "175843", "bsz": "17.7", "num_updates": "75700", "lr": "0.00457554", "gnorm": "0.47", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19484"}
2022-01-28 18:24:28 | INFO | train_inner | {"epoch": 49, "update": 48.414, "loss": "2.038", "ntokens": "173364", "nsentences": "16.5", "wps": "722372", "ups": "4.17", "wpb": "173364", "bsz": "16.5", "num_updates": "75800", "lr": "0.00457445", "gnorm": "0.456", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19508"}
2022-01-28 18:24:52 | INFO | train_inner | {"epoch": 49, "update": 48.478, "loss": "2.04", "ntokens": "175424", "nsentences": "16.72", "wps": "729573", "ups": "4.16", "wpb": "175424", "bsz": "16.7", "num_updates": "75900", "lr": "0.00457335", "gnorm": "0.435", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19532"}
2022-01-28 18:25:16 | INFO | train_inner | {"epoch": 49, "update": 48.542, "loss": "2.04", "ntokens": "174284", "nsentences": "16.56", "wps": "728604", "ups": "4.18", "wpb": "174284", "bsz": "16.6", "num_updates": "76000", "lr": "0.00457225", "gnorm": "0.439", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19556"}
2022-01-28 18:25:40 | INFO | train_inner | {"epoch": 49, "update": 48.606, "loss": "2.044", "ntokens": "175610", "nsentences": "17.04", "wps": "728217", "ups": "4.15", "wpb": "175610", "bsz": "17", "num_updates": "76100", "lr": "0.00457115", "gnorm": "0.463", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19580"}
2022-01-28 18:26:04 | INFO | train_inner | {"epoch": 49, "update": 48.67, "loss": "2.035", "ntokens": "176590", "nsentences": "16.8", "wps": "732702", "ups": "4.15", "wpb": "176590", "bsz": "16.8", "num_updates": "76200", "lr": "0.00457005", "gnorm": "0.415", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19604"}
2022-01-28 18:26:28 | INFO | train_inner | {"epoch": 49, "update": 48.734, "loss": "2.045", "ntokens": "175761", "nsentences": "16.88", "wps": "731664", "ups": "4.16", "wpb": "175761", "bsz": "16.9", "num_updates": "76300", "lr": "0.00456895", "gnorm": "0.449", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.6", "wall": "19628"}
2022-01-28 18:26:52 | INFO | train_inner | {"epoch": 49, "update": 48.798, "loss": "2.021", "ntokens": "175313", "nsentences": "17.12", "wps": "731160", "ups": "4.17", "wpb": "175313", "bsz": "17.1", "num_updates": "76400", "lr": "0.00456784", "gnorm": "0.452", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19652"}
2022-01-28 18:27:16 | INFO | train_inner | {"epoch": 49, "update": 48.861, "loss": "2.043", "ntokens": "174940", "nsentences": "16.88", "wps": "727122", "ups": "4.16", "wpb": "174940", "bsz": "16.9", "num_updates": "76500", "lr": "0.00456674", "gnorm": "0.468", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19676"}
2022-01-28 18:27:40 | INFO | train_inner | {"epoch": 49, "update": 48.925, "loss": "2.037", "ntokens": "174721", "nsentences": "16.8", "wps": "727230", "ups": "4.16", "wpb": "174721", "bsz": "16.8", "num_updates": "76600", "lr": "0.00456563", "gnorm": "0.433", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19700"}
2022-01-28 18:28:04 | INFO | train_inner | {"epoch": 49, "update": 48.989, "loss": "2.031", "ntokens": "174819", "nsentences": "16.96", "wps": "728720", "ups": "4.17", "wpb": "174819", "bsz": "17", "num_updates": "76700", "lr": "0.00456452", "gnorm": "0.437", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19724"}
2022-01-28 18:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:28:13 | INFO | valid | {"epoch": 49, "valid_loss": "2.091", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.44603e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "76717", "valid_best_loss": "2.091"}
2022-01-28 18:28:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 76717 updates
2022-01-28 18:28:13 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:28:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 49 @ 76717 updates, score 2.091) (writing took 12.182683297432959 seconds)
2022-01-28 18:28:26 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-28 18:28:26 | INFO | train | {"epoch": 49, "train_loss": "2.038", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679204", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "76717", "train_lr": "0.00456433", "train_gnorm": "0.447", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "19745"}
2022-01-28 18:28:26 | INFO | fairseq.trainer | begin training epoch 50
2022-01-28 18:28:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:28:56 | INFO | train_inner | {"epoch": 50, "update": 49.053, "loss": "2.024", "ntokens": "175704", "nsentences": "16.88", "wps": "339082", "ups": "1.93", "wpb": "175704", "bsz": "16.9", "num_updates": "76800", "lr": "0.00456341", "gnorm": "0.418", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19776"}
2022-01-28 18:29:20 | INFO | train_inner | {"epoch": 50, "update": 49.117, "loss": "2.006", "ntokens": "175983", "nsentences": "17.04", "wps": "736416", "ups": "4.18", "wpb": "175983", "bsz": "17", "num_updates": "76900", "lr": "0.0045623", "gnorm": "0.44", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19800"}
2022-01-28 18:29:44 | INFO | train_inner | {"epoch": 50, "update": 49.181, "loss": "2.029", "ntokens": "175324", "nsentences": "16.96", "wps": "732096", "ups": "4.18", "wpb": "175324", "bsz": "17", "num_updates": "77000", "lr": "0.00456119", "gnorm": "0.462", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19824"}
2022-01-28 18:30:08 | INFO | train_inner | {"epoch": 50, "update": 49.245, "loss": "2.025", "ntokens": "175181", "nsentences": "17.2", "wps": "729950", "ups": "4.17", "wpb": "175181", "bsz": "17.2", "num_updates": "77100", "lr": "0.00456008", "gnorm": "0.454", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.8", "wall": "19848"}
2022-01-28 18:30:32 | INFO | train_inner | {"epoch": 50, "update": 49.308, "loss": "2.028", "ntokens": "175250", "nsentences": "17.2", "wps": "729168", "ups": "4.16", "wpb": "175250", "bsz": "17.2", "num_updates": "77200", "lr": "0.00455896", "gnorm": "0.454", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19872"}
2022-01-28 18:30:56 | INFO | train_inner | {"epoch": 50, "update": 49.372, "loss": "2.018", "ntokens": "175623", "nsentences": "16.48", "wps": "731428", "ups": "4.16", "wpb": "175623", "bsz": "16.5", "num_updates": "77300", "lr": "0.00455785", "gnorm": "0.435", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19896"}
2022-01-28 18:31:20 | INFO | train_inner | {"epoch": 50, "update": 49.436, "loss": "2.043", "ntokens": "174403", "nsentences": "16.72", "wps": "726500", "ups": "4.17", "wpb": "174403", "bsz": "16.7", "num_updates": "77400", "lr": "0.00455673", "gnorm": "0.462", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19920"}
2022-01-28 18:31:44 | INFO | train_inner | {"epoch": 50, "update": 49.5, "loss": "2.019", "ntokens": "174299", "nsentences": "16.8", "wps": "724830", "ups": "4.16", "wpb": "174299", "bsz": "16.8", "num_updates": "77500", "lr": "0.00455561", "gnorm": "0.48", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19944"}
2022-01-28 18:32:08 | INFO | train_inner | {"epoch": 50, "update": 49.564, "loss": "2.042", "ntokens": "175594", "nsentences": "17.28", "wps": "729621", "ups": "4.16", "wpb": "175594", "bsz": "17.3", "num_updates": "77600", "lr": "0.00455449", "gnorm": "0.454", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19968"}
2022-01-28 18:32:32 | INFO | train_inner | {"epoch": 50, "update": 49.628, "loss": "2.035", "ntokens": "175079", "nsentences": "16.72", "wps": "726690", "ups": "4.15", "wpb": "175079", "bsz": "16.7", "num_updates": "77700", "lr": "0.00455337", "gnorm": "0.461", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "19992"}
2022-01-28 18:32:56 | INFO | train_inner | {"epoch": 50, "update": 49.692, "loss": "2.024", "ntokens": "175425", "nsentences": "16.8", "wps": "728198", "ups": "4.15", "wpb": "175425", "bsz": "16.8", "num_updates": "77800", "lr": "0.00455225", "gnorm": "0.43", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20016"}
2022-01-28 18:33:21 | INFO | train_inner | {"epoch": 50, "update": 49.755, "loss": "2.034", "ntokens": "176149", "nsentences": "17.04", "wps": "731545", "ups": "4.15", "wpb": "176149", "bsz": "17", "num_updates": "77900", "lr": "0.00455113", "gnorm": "0.438", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20040"}
2022-01-28 18:33:45 | INFO | train_inner | {"epoch": 50, "update": 49.819, "loss": "2.018", "ntokens": "174874", "nsentences": "16.08", "wps": "727540", "ups": "4.16", "wpb": "174874", "bsz": "16.1", "num_updates": "78000", "lr": "0.00455", "gnorm": "0.46", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20064"}
2022-01-28 18:34:09 | INFO | train_inner | {"epoch": 50, "update": 49.883, "loss": "2.026", "ntokens": "173922", "nsentences": "16.74", "wps": "721928", "ups": "4.15", "wpb": "173922", "bsz": "16.7", "num_updates": "78100", "lr": "0.00454888", "gnorm": "0.447", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20089"}
2022-01-28 18:34:33 | INFO | train_inner | {"epoch": 50, "update": 49.947, "loss": "2.026", "ntokens": "175481", "nsentences": "16.8", "wps": "730495", "ups": "4.16", "wpb": "175481", "bsz": "16.8", "num_updates": "78200", "lr": "0.00454775", "gnorm": "0.47", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20113"}
2022-01-28 18:34:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:34:57 | INFO | valid | {"epoch": 50, "valid_loss": "2.094", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.56863e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "78283", "valid_best_loss": "2.091"}
2022-01-28 18:34:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 78283 updates
2022-01-28 18:34:57 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 18:35:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 18:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 50 @ 78283 updates, score 2.094) (writing took 4.276281849481165 seconds)
2022-01-28 18:35:02 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-28 18:35:02 | INFO | train | {"epoch": 50, "train_loss": "2.027", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "692767", "train_ups": "3.95", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "78283", "train_lr": "0.00454681", "train_gnorm": "0.45", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "20141"}
2022-01-28 18:35:02 | INFO | fairseq.trainer | begin training epoch 51
2022-01-28 18:35:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:35:17 | INFO | train_inner | {"epoch": 51, "update": 50.011, "loss": "2.033", "ntokens": "175493", "nsentences": "17.44", "wps": "400213", "ups": "2.28", "wpb": "175493", "bsz": "17.4", "num_updates": "78300", "lr": "0.00454662", "gnorm": "0.425", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20156"}
2022-01-28 18:35:41 | INFO | train_inner | {"epoch": 51, "update": 50.075, "loss": "2.018", "ntokens": "176407", "nsentences": "16.8", "wps": "738387", "ups": "4.19", "wpb": "176407", "bsz": "16.8", "num_updates": "78400", "lr": "0.00454549", "gnorm": "0.443", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20180"}
2022-01-28 18:36:04 | INFO | train_inner | {"epoch": 51, "update": 50.139, "loss": "2.017", "ntokens": "174669", "nsentences": "16.74", "wps": "730616", "ups": "4.18", "wpb": "174669", "bsz": "16.7", "num_updates": "78500", "lr": "0.00454436", "gnorm": "0.467", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20204"}
2022-01-28 18:36:28 | INFO | train_inner | {"epoch": 51, "update": 50.202, "loss": "2.023", "ntokens": "174964", "nsentences": "16.88", "wps": "729723", "ups": "4.17", "wpb": "174964", "bsz": "16.9", "num_updates": "78600", "lr": "0.00454323", "gnorm": "0.479", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20228"}
2022-01-28 18:36:52 | INFO | train_inner | {"epoch": 51, "update": 50.266, "loss": "2.008", "ntokens": "176077", "nsentences": "16.72", "wps": "732920", "ups": "4.16", "wpb": "176077", "bsz": "16.7", "num_updates": "78700", "lr": "0.0045421", "gnorm": "0.421", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20252"}
2022-01-28 18:37:16 | INFO | train_inner | {"epoch": 51, "update": 50.33, "loss": "2.015", "ntokens": "175378", "nsentences": "16.8", "wps": "729754", "ups": "4.16", "wpb": "175378", "bsz": "16.8", "num_updates": "78800", "lr": "0.00454096", "gnorm": "0.449", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20276"}
2022-01-28 18:37:41 | INFO | train_inner | {"epoch": 51, "update": 50.394, "loss": "2.008", "ntokens": "176348", "nsentences": "16.96", "wps": "733457", "ups": "4.16", "wpb": "176348", "bsz": "17", "num_updates": "78900", "lr": "0.00453983", "gnorm": "0.433", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20300"}
2022-01-28 18:38:05 | INFO | train_inner | {"epoch": 51, "update": 50.458, "loss": "2.02", "ntokens": "173765", "nsentences": "16.72", "wps": "725173", "ups": "4.17", "wpb": "173765", "bsz": "16.7", "num_updates": "79000", "lr": "0.00453869", "gnorm": "0.454", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20324"}
2022-01-28 18:38:29 | INFO | train_inner | {"epoch": 51, "update": 50.522, "loss": "2.017", "ntokens": "176222", "nsentences": "17.2", "wps": "733155", "ups": "4.16", "wpb": "176222", "bsz": "17.2", "num_updates": "79100", "lr": "0.00453755", "gnorm": "0.435", "loss_scale": "0.125", "train_wall": "24", "gb_free": "19.9", "wall": "20348"}
2022-01-28 18:38:53 | INFO | train_inner | {"epoch": 51, "update": 50.586, "loss": "2.02", "ntokens": "174706", "nsentences": "16.48", "wps": "727412", "ups": "4.16", "wpb": "174706", "bsz": "16.5", "num_updates": "79200", "lr": "0.00453641", "gnorm": "0.435", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20372"}
2022-01-28 18:39:17 | INFO | train_inner | {"epoch": 51, "update": 50.649, "loss": "2.022", "ntokens": "174485", "nsentences": "17.2", "wps": "728596", "ups": "4.18", "wpb": "174485", "bsz": "17.2", "num_updates": "79300", "lr": "0.00453527", "gnorm": "0.452", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20396"}
2022-01-28 18:39:41 | INFO | train_inner | {"epoch": 51, "update": 50.713, "loss": "2.011", "ntokens": "176904", "nsentences": "17.12", "wps": "735071", "ups": "4.16", "wpb": "176904", "bsz": "17.1", "num_updates": "79400", "lr": "0.00453413", "gnorm": "0.447", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20420"}
2022-01-28 18:40:05 | INFO | train_inner | {"epoch": 51, "update": 50.777, "loss": "2.019", "ntokens": "175049", "nsentences": "16.8", "wps": "728399", "ups": "4.16", "wpb": "175049", "bsz": "16.8", "num_updates": "79500", "lr": "0.00453299", "gnorm": "0.451", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20444"}
2022-01-28 18:40:29 | INFO | train_inner | {"epoch": 51, "update": 50.841, "loss": "2.019", "ntokens": "173642", "nsentences": "17.04", "wps": "724377", "ups": "4.17", "wpb": "173642", "bsz": "17", "num_updates": "79600", "lr": "0.00453184", "gnorm": "0.412", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20468"}
2022-01-28 18:40:53 | INFO | train_inner | {"epoch": 51, "update": 50.905, "loss": "1.996", "ntokens": "175183", "nsentences": "16.56", "wps": "729264", "ups": "4.16", "wpb": "175183", "bsz": "16.6", "num_updates": "79700", "lr": "0.00453069", "gnorm": "0.452", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20492"}
2022-01-28 18:41:17 | INFO | train_inner | {"epoch": 51, "update": 50.969, "loss": "2.028", "ntokens": "174384", "nsentences": "17.12", "wps": "726963", "ups": "4.17", "wpb": "174384", "bsz": "17.1", "num_updates": "79800", "lr": "0.00452955", "gnorm": "0.465", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20516"}
2022-01-28 18:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:41:33 | INFO | valid | {"epoch": 51, "valid_loss": "2.081", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.66337e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "79849", "valid_best_loss": "2.081"}
2022-01-28 18:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 79849 updates
2022-01-28 18:41:33 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:41:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:41:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 51 @ 79849 updates, score 2.081) (writing took 11.572691329754889 seconds)
2022-01-28 18:41:45 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-28 18:41:45 | INFO | train | {"epoch": 51, "train_loss": "2.016", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680409", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "79849", "train_lr": "0.00452898", "train_gnorm": "0.446", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "20545"}
2022-01-28 18:41:45 | INFO | fairseq.trainer | begin training epoch 52
2022-01-28 18:41:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:42:08 | INFO | train_inner | {"epoch": 52, "update": 51.033, "loss": "1.991", "ntokens": "174652", "nsentences": "16.96", "wps": "339515", "ups": "1.94", "wpb": "174652", "bsz": "17", "num_updates": "79900", "lr": "0.0045284", "gnorm": "0.44", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "20568"}
2022-01-28 18:42:32 | INFO | train_inner | {"epoch": 52, "update": 51.096, "loss": "1.99", "ntokens": "175187", "nsentences": "16.56", "wps": "732922", "ups": "4.18", "wpb": "175187", "bsz": "16.6", "num_updates": "80000", "lr": "0.00452725", "gnorm": "0.402", "loss_scale": "0.25", "train_wall": "24", "gb_free": "21.3", "wall": "20592"}
2022-01-28 18:42:56 | INFO | train_inner | {"epoch": 52, "update": 51.16, "loss": "1.988", "ntokens": "175964", "nsentences": "16.56", "wps": "734416", "ups": "4.17", "wpb": "175964", "bsz": "16.6", "num_updates": "80100", "lr": "0.0045261", "gnorm": "0.43", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20616"}
2022-01-28 18:43:20 | INFO | train_inner | {"epoch": 52, "update": 51.224, "loss": "1.997", "ntokens": "176423", "nsentences": "16.56", "wps": "734428", "ups": "4.16", "wpb": "176423", "bsz": "16.6", "num_updates": "80200", "lr": "0.00452495", "gnorm": "0.425", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20640"}
2022-01-28 18:43:44 | INFO | train_inner | {"epoch": 52, "update": 51.288, "loss": "2", "ntokens": "172870", "nsentences": "17.04", "wps": "723468", "ups": "4.19", "wpb": "172870", "bsz": "17", "num_updates": "80300", "lr": "0.00452379", "gnorm": "0.464", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20664"}
2022-01-28 18:44:08 | INFO | train_inner | {"epoch": 52, "update": 51.352, "loss": "2.005", "ntokens": "174816", "nsentences": "16.64", "wps": "727875", "ups": "4.16", "wpb": "174816", "bsz": "16.6", "num_updates": "80400", "lr": "0.00452264", "gnorm": "0.474", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20688"}
2022-01-28 18:44:32 | INFO | train_inner | {"epoch": 52, "update": 51.416, "loss": "2.014", "ntokens": "175704", "nsentences": "17.36", "wps": "732762", "ups": "4.17", "wpb": "175704", "bsz": "17.4", "num_updates": "80500", "lr": "0.00452148", "gnorm": "0.44", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20712"}
2022-01-28 18:44:56 | INFO | train_inner | {"epoch": 52, "update": 51.48, "loss": "2.007", "ntokens": "176279", "nsentences": "16.72", "wps": "733621", "ups": "4.16", "wpb": "176279", "bsz": "16.7", "num_updates": "80600", "lr": "0.00452032", "gnorm": "0.432", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.3", "wall": "20736"}
2022-01-28 18:45:20 | INFO | train_inner | {"epoch": 52, "update": 51.543, "loss": "2.011", "ntokens": "175622", "nsentences": "17.6", "wps": "732303", "ups": "4.17", "wpb": "175622", "bsz": "17.6", "num_updates": "80700", "lr": "0.00451917", "gnorm": "0.488", "loss_scale": "0.25", "train_wall": "24", "gb_free": "21.1", "wall": "20760"}
2022-01-28 18:45:44 | INFO | train_inner | {"epoch": 52, "update": 51.607, "loss": "2.011", "ntokens": "174857", "nsentences": "16.56", "wps": "726868", "ups": "4.16", "wpb": "174857", "bsz": "16.6", "num_updates": "80800", "lr": "0.00451801", "gnorm": "0.479", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20784"}
2022-01-28 18:46:08 | INFO | train_inner | {"epoch": 52, "update": 51.671, "loss": "2.013", "ntokens": "174164", "nsentences": "16.66", "wps": "724839", "ups": "4.16", "wpb": "174164", "bsz": "16.7", "num_updates": "80900", "lr": "0.00451685", "gnorm": "0.432", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20808"}
2022-01-28 18:46:32 | INFO | train_inner | {"epoch": 52, "update": 51.735, "loss": "2.01", "ntokens": "174430", "nsentences": "17.04", "wps": "723989", "ups": "4.15", "wpb": "174430", "bsz": "17", "num_updates": "81000", "lr": "0.00451568", "gnorm": "0.446", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20832"}
2022-01-28 18:46:56 | INFO | train_inner | {"epoch": 52, "update": 51.799, "loss": "2.029", "ntokens": "175022", "nsentences": "17.28", "wps": "728678", "ups": "4.16", "wpb": "175022", "bsz": "17.3", "num_updates": "81100", "lr": "0.00451452", "gnorm": "0.425", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20856"}
2022-01-28 18:47:20 | INFO | train_inner | {"epoch": 52, "update": 51.863, "loss": "2.01", "ntokens": "175607", "nsentences": "16.88", "wps": "731183", "ups": "4.16", "wpb": "175607", "bsz": "16.9", "num_updates": "81200", "lr": "0.00451336", "gnorm": "0.481", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20880"}
2022-01-28 18:47:44 | INFO | train_inner | {"epoch": 52, "update": 51.927, "loss": "2.018", "ntokens": "174042", "nsentences": "18", "wps": "724792", "ups": "4.16", "wpb": "174042", "bsz": "18", "num_updates": "81300", "lr": "0.00451219", "gnorm": "0.451", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20904"}
2022-01-28 18:48:08 | INFO | train_inner | {"epoch": 52, "update": 51.99, "loss": "1.993", "ntokens": "176166", "nsentences": "16.16", "wps": "732084", "ups": "4.16", "wpb": "176166", "bsz": "16.2", "num_updates": "81400", "lr": "0.00451102", "gnorm": "0.424", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20928"}
2022-01-28 18:48:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:48:17 | INFO | valid | {"epoch": 52, "valid_loss": "2.057", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.74258e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "81415", "valid_best_loss": "2.057"}
2022-01-28 18:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 81415 updates
2022-01-28 18:48:17 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:48:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:48:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 52 @ 81415 updates, score 2.057) (writing took 11.147206526249647 seconds)
2022-01-28 18:48:28 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-28 18:48:28 | INFO | train | {"epoch": 52, "train_loss": "2.005", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680647", "train_ups": "3.89", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "81415", "train_lr": "0.00451085", "train_gnorm": "0.446", "train_loss_scale": "0.25", "train_train_wall": "374", "train_gb_free": "21.1", "train_wall": "20948"}
2022-01-28 18:48:28 | INFO | fairseq.trainer | begin training epoch 53
2022-01-28 18:48:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:48:59 | INFO | train_inner | {"epoch": 53, "update": 52.054, "loss": "1.995", "ntokens": "175586", "nsentences": "16.48", "wps": "346952", "ups": "1.98", "wpb": "175586", "bsz": "16.5", "num_updates": "81500", "lr": "0.00450985", "gnorm": "0.439", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "20979"}
2022-01-28 18:49:23 | INFO | train_inner | {"epoch": 53, "update": 52.118, "loss": "2.009", "ntokens": "176324", "nsentences": "16.88", "wps": "737239", "ups": "4.18", "wpb": "176324", "bsz": "16.9", "num_updates": "81600", "lr": "0.00450868", "gnorm": "0.451", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.5", "wall": "21003"}
2022-01-28 18:49:47 | INFO | train_inner | {"epoch": 53, "update": 52.182, "loss": "1.981", "ntokens": "176520", "nsentences": "17.2", "wps": "734345", "ups": "4.16", "wpb": "176520", "bsz": "17.2", "num_updates": "81700", "lr": "0.00450751", "gnorm": "0.465", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21027"}
2022-01-28 18:50:11 | INFO | train_inner | {"epoch": 53, "update": 52.246, "loss": "2.005", "ntokens": "175377", "nsentences": "17.44", "wps": "732903", "ups": "4.18", "wpb": "175377", "bsz": "17.4", "num_updates": "81800", "lr": "0.00450634", "gnorm": "0.437", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21051"}
2022-01-28 18:50:35 | INFO | train_inner | {"epoch": 53, "update": 52.31, "loss": "2", "ntokens": "175748", "nsentences": "16.64", "wps": "731831", "ups": "4.16", "wpb": "175748", "bsz": "16.6", "num_updates": "81900", "lr": "0.00450517", "gnorm": "0.454", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21075"}
2022-01-28 18:50:59 | INFO | train_inner | {"epoch": 53, "update": 52.374, "loss": "2.002", "ntokens": "174576", "nsentences": "16.8", "wps": "727119", "ups": "4.17", "wpb": "174576", "bsz": "16.8", "num_updates": "82000", "lr": "0.00450399", "gnorm": "0.41", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21099"}
2022-01-28 18:51:23 | INFO | train_inner | {"epoch": 53, "update": 52.437, "loss": "1.997", "ntokens": "174723", "nsentences": "16.72", "wps": "727439", "ups": "4.16", "wpb": "174723", "bsz": "16.7", "num_updates": "82100", "lr": "0.00450282", "gnorm": "0.428", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21123"}
2022-01-28 18:51:47 | INFO | train_inner | {"epoch": 53, "update": 52.501, "loss": "2", "ntokens": "175724", "nsentences": "17.52", "wps": "730769", "ups": "4.16", "wpb": "175724", "bsz": "17.5", "num_updates": "82200", "lr": "0.00450164", "gnorm": "0.431", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21147"}
2022-01-28 18:52:11 | INFO | train_inner | {"epoch": 53, "update": 52.565, "loss": "2.014", "ntokens": "175516", "nsentences": "17.36", "wps": "729149", "ups": "4.15", "wpb": "175516", "bsz": "17.4", "num_updates": "82300", "lr": "0.00450046", "gnorm": "0.477", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21171"}
2022-01-28 18:52:35 | INFO | train_inner | {"epoch": 53, "update": 52.629, "loss": "1.98", "ntokens": "175202", "nsentences": "16.56", "wps": "728615", "ups": "4.16", "wpb": "175202", "bsz": "16.6", "num_updates": "82400", "lr": "0.00449928", "gnorm": "0.473", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21195"}
2022-01-28 18:52:59 | INFO | train_inner | {"epoch": 53, "update": 52.693, "loss": "1.989", "ntokens": "173480", "nsentences": "16.48", "wps": "722343", "ups": "4.16", "wpb": "173480", "bsz": "16.5", "num_updates": "82500", "lr": "0.0044981", "gnorm": "0.443", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.7", "wall": "21219"}
2022-01-28 18:53:23 | INFO | train_inner | {"epoch": 53, "update": 52.757, "loss": "1.997", "ntokens": "173849", "nsentences": "16.42", "wps": "723671", "ups": "4.16", "wpb": "173849", "bsz": "16.4", "num_updates": "82600", "lr": "0.00449692", "gnorm": "0.443", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21243"}
2022-01-28 18:53:47 | INFO | train_inner | {"epoch": 53, "update": 52.821, "loss": "2", "ntokens": "175203", "nsentences": "17.2", "wps": "728047", "ups": "4.16", "wpb": "175203", "bsz": "17.2", "num_updates": "82700", "lr": "0.00449574", "gnorm": "0.428", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21267"}
2022-01-28 18:54:11 | INFO | train_inner | {"epoch": 53, "update": 52.884, "loss": "1.987", "ntokens": "173928", "nsentences": "16.72", "wps": "724866", "ups": "4.17", "wpb": "173928", "bsz": "16.7", "num_updates": "82800", "lr": "0.00449455", "gnorm": "0.433", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21291"}
2022-01-28 18:54:35 | INFO | train_inner | {"epoch": 53, "update": 52.948, "loss": "1.993", "ntokens": "176118", "nsentences": "16.48", "wps": "732203", "ups": "4.16", "wpb": "176118", "bsz": "16.5", "num_updates": "82900", "lr": "0.00449337", "gnorm": "0.437", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21315"}
2022-01-28 18:54:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:55:00 | INFO | valid | {"epoch": 53, "valid_loss": "2.05", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.64322e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "82981", "valid_best_loss": "2.05"}
2022-01-28 18:55:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 82981 updates
2022-01-28 18:55:00 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:55:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 18:55:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 53 @ 82981 updates, score 2.05) (writing took 11.348598405718803 seconds)
2022-01-28 18:55:11 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-28 18:55:11 | INFO | train | {"epoch": 53, "train_loss": "1.997", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680645", "train_ups": "3.89", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "82981", "train_lr": "0.0044924", "train_gnorm": "0.445", "train_loss_scale": "0.25", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "21351"}
2022-01-28 18:55:11 | INFO | fairseq.trainer | begin training epoch 54
2022-01-28 18:55:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:55:27 | INFO | train_inner | {"epoch": 54, "update": 53.012, "loss": "2.005", "ntokens": "175651", "nsentences": "17.52", "wps": "343290", "ups": "1.95", "wpb": "175651", "bsz": "17.5", "num_updates": "83000", "lr": "0.00449218", "gnorm": "0.472", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21366"}
2022-01-28 18:55:50 | INFO | train_inner | {"epoch": 54, "update": 53.076, "loss": "1.974", "ntokens": "175082", "nsentences": "16.32", "wps": "735585", "ups": "4.2", "wpb": "175082", "bsz": "16.3", "num_updates": "83100", "lr": "0.00449099", "gnorm": "0.447", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21390"}
2022-01-28 18:56:14 | INFO | train_inner | {"epoch": 54, "update": 53.14, "loss": "1.979", "ntokens": "175985", "nsentences": "16.72", "wps": "735029", "ups": "4.18", "wpb": "175985", "bsz": "16.7", "num_updates": "83200", "lr": "0.0044898", "gnorm": "0.427", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.1", "wall": "21414"}
2022-01-28 18:56:38 | INFO | train_inner | {"epoch": 54, "update": 53.204, "loss": "1.99", "ntokens": "175238", "nsentences": "16.88", "wps": "729352", "ups": "4.16", "wpb": "175238", "bsz": "16.9", "num_updates": "83300", "lr": "0.00448861", "gnorm": "0.429", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21438"}
2022-01-28 18:57:02 | INFO | train_inner | {"epoch": 54, "update": 53.268, "loss": "1.98", "ntokens": "174855", "nsentences": "17.28", "wps": "728399", "ups": "4.17", "wpb": "174855", "bsz": "17.3", "num_updates": "83400", "lr": "0.00448742", "gnorm": "0.437", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21462"}
2022-01-28 18:57:26 | INFO | train_inner | {"epoch": 54, "update": 53.331, "loss": "1.998", "ntokens": "175279", "nsentences": "17.36", "wps": "730359", "ups": "4.17", "wpb": "175279", "bsz": "17.4", "num_updates": "83500", "lr": "0.00448623", "gnorm": "0.453", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21486"}
2022-01-28 18:57:50 | INFO | train_inner | {"epoch": 54, "update": 53.395, "loss": "1.988", "ntokens": "175322", "nsentences": "17.12", "wps": "729335", "ups": "4.16", "wpb": "175322", "bsz": "17.1", "num_updates": "83600", "lr": "0.00448503", "gnorm": "0.469", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21510"}
2022-01-28 18:58:14 | INFO | train_inner | {"epoch": 54, "update": 53.459, "loss": "1.99", "ntokens": "174373", "nsentences": "17.28", "wps": "725992", "ups": "4.16", "wpb": "174373", "bsz": "17.3", "num_updates": "83700", "lr": "0.00448384", "gnorm": "0.46", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21534"}
2022-01-28 18:58:38 | INFO | train_inner | {"epoch": 54, "update": 53.523, "loss": "1.99", "ntokens": "174707", "nsentences": "16.98", "wps": "728405", "ups": "4.17", "wpb": "174707", "bsz": "17", "num_updates": "83800", "lr": "0.00448264", "gnorm": "0.448", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21558"}
2022-01-28 18:59:02 | INFO | train_inner | {"epoch": 54, "update": 53.587, "loss": "1.978", "ntokens": "174725", "nsentences": "16.72", "wps": "725898", "ups": "4.15", "wpb": "174725", "bsz": "16.7", "num_updates": "83900", "lr": "0.00448144", "gnorm": "0.427", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21582"}
2022-01-28 18:59:27 | INFO | train_inner | {"epoch": 54, "update": 53.651, "loss": "1.977", "ntokens": "175505", "nsentences": "16.56", "wps": "728870", "ups": "4.15", "wpb": "175505", "bsz": "16.6", "num_updates": "84000", "lr": "0.00448024", "gnorm": "0.441", "loss_scale": "0.25", "train_wall": "24", "gb_free": "20.4", "wall": "21606"}
2022-01-28 18:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2022-01-28 18:59:51 | INFO | train_inner | {"epoch": 54, "update": 53.715, "loss": "2.003", "ntokens": "175327", "nsentences": "17.04", "wps": "725018", "ups": "4.14", "wpb": "175327", "bsz": "17", "num_updates": "84100", "lr": "0.00447904", "gnorm": "0.433", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21631"}
2022-01-28 19:00:15 | INFO | train_inner | {"epoch": 54, "update": 53.779, "loss": "1.99", "ntokens": "174594", "nsentences": "16.64", "wps": "727379", "ups": "4.17", "wpb": "174594", "bsz": "16.6", "num_updates": "84200", "lr": "0.00447784", "gnorm": "0.44", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21655"}
2022-01-28 19:00:39 | INFO | train_inner | {"epoch": 54, "update": 53.843, "loss": "1.992", "ntokens": "175881", "nsentences": "16.64", "wps": "730130", "ups": "4.15", "wpb": "175881", "bsz": "16.6", "num_updates": "84300", "lr": "0.00447664", "gnorm": "0.453", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21679"}
2022-01-28 19:01:03 | INFO | train_inner | {"epoch": 54, "update": 53.907, "loss": "1.984", "ntokens": "173244", "nsentences": "17.04", "wps": "722152", "ups": "4.17", "wpb": "173244", "bsz": "17", "num_updates": "84400", "lr": "0.00447543", "gnorm": "0.452", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21703"}
2022-01-28 19:01:27 | INFO | train_inner | {"epoch": 54, "update": 53.971, "loss": "2", "ntokens": "177138", "nsentences": "16.8", "wps": "735396", "ups": "4.15", "wpb": "177138", "bsz": "16.8", "num_updates": "84500", "lr": "0.00447423", "gnorm": "0.428", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21727"}
2022-01-28 19:01:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:01:43 | INFO | valid | {"epoch": 54, "valid_loss": "2.055", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.65063e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "84546", "valid_best_loss": "2.05"}
2022-01-28 19:01:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 84546 updates
2022-01-28 19:01:43 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 19:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 19:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 54 @ 84546 updates, score 2.055) (writing took 4.27376898471266 seconds)
2022-01-28 19:01:47 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-28 19:01:47 | INFO | train | {"epoch": 54, "train_loss": "1.988", "train_ntokens": "175180", "train_nsentences": "16.8907", "train_wps": "691870", "train_ups": "3.95", "train_wpb": "175180", "train_bsz": "16.9", "train_num_updates": "84546", "train_lr": "0.00447367", "train_gnorm": "0.443", "train_loss_scale": "0.125", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "21747"}
2022-01-28 19:01:47 | INFO | fairseq.trainer | begin training epoch 55
2022-01-28 19:01:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:02:11 | INFO | train_inner | {"epoch": 55, "update": 54.034, "loss": "1.975", "ntokens": "174280", "nsentences": "16.72", "wps": "393625", "ups": "2.26", "wpb": "174280", "bsz": "16.7", "num_updates": "84600", "lr": "0.00447302", "gnorm": "0.436", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21771"}
2022-01-28 19:02:35 | INFO | train_inner | {"epoch": 55, "update": 54.098, "loss": "1.956", "ntokens": "175237", "nsentences": "17.04", "wps": "735034", "ups": "4.19", "wpb": "175237", "bsz": "17", "num_updates": "84700", "lr": "0.00447181", "gnorm": "0.467", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21795"}
2022-01-28 19:02:59 | INFO | train_inner | {"epoch": 55, "update": 54.162, "loss": "1.985", "ntokens": "174888", "nsentences": "16.4", "wps": "731222", "ups": "4.18", "wpb": "174888", "bsz": "16.4", "num_updates": "84800", "lr": "0.0044706", "gnorm": "0.446", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21819"}
2022-01-28 19:03:23 | INFO | train_inner | {"epoch": 55, "update": 54.226, "loss": "1.975", "ntokens": "175438", "nsentences": "17.76", "wps": "731021", "ups": "4.17", "wpb": "175438", "bsz": "17.8", "num_updates": "84900", "lr": "0.00446939", "gnorm": "0.429", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21843"}
2022-01-28 19:03:47 | INFO | train_inner | {"epoch": 55, "update": 54.29, "loss": "1.963", "ntokens": "175766", "nsentences": "17.28", "wps": "731868", "ups": "4.16", "wpb": "175766", "bsz": "17.3", "num_updates": "85000", "lr": "0.00446818", "gnorm": "0.437", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21867"}
2022-01-28 19:04:11 | INFO | train_inner | {"epoch": 55, "update": 54.354, "loss": "1.99", "ntokens": "175008", "nsentences": "17.52", "wps": "728532", "ups": "4.16", "wpb": "175008", "bsz": "17.5", "num_updates": "85100", "lr": "0.00446697", "gnorm": "0.456", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21891"}
2022-01-28 19:04:35 | INFO | train_inner | {"epoch": 55, "update": 54.418, "loss": "1.987", "ntokens": "174398", "nsentences": "16.82", "wps": "725125", "ups": "4.16", "wpb": "174398", "bsz": "16.8", "num_updates": "85200", "lr": "0.00446576", "gnorm": "0.459", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.3", "wall": "21915"}
2022-01-28 19:04:59 | INFO | train_inner | {"epoch": 55, "update": 54.481, "loss": "1.978", "ntokens": "176653", "nsentences": "16.4", "wps": "732183", "ups": "4.14", "wpb": "176653", "bsz": "16.4", "num_updates": "85300", "lr": "0.00446454", "gnorm": "0.45", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21939"}
2022-01-28 19:05:23 | INFO | train_inner | {"epoch": 55, "update": 54.545, "loss": "1.977", "ntokens": "174748", "nsentences": "16.72", "wps": "727227", "ups": "4.16", "wpb": "174748", "bsz": "16.7", "num_updates": "85400", "lr": "0.00446332", "gnorm": "0.439", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21963"}
2022-01-28 19:05:47 | INFO | train_inner | {"epoch": 55, "update": 54.609, "loss": "1.982", "ntokens": "174982", "nsentences": "16.64", "wps": "728977", "ups": "4.17", "wpb": "174982", "bsz": "16.6", "num_updates": "85500", "lr": "0.00446211", "gnorm": "0.451", "loss_scale": "0.125", "train_wall": "24", "gb_free": "20.4", "wall": "21987"}
2022-01-28 19:06:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2022-01-28 19:06:12 | INFO | train_inner | {"epoch": 55, "update": 54.674, "loss": "1.976", "ntokens": "175283", "nsentences": "16.48", "wps": "720726", "ups": "4.11", "wpb": "175283", "bsz": "16.5", "num_updates": "85600", "lr": "0.00446089", "gnorm": "0.444", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22011"}
2022-01-28 19:06:36 | INFO | train_inner | {"epoch": 55, "update": 54.738, "loss": "1.988", "ntokens": "175495", "nsentences": "17.04", "wps": "730375", "ups": "4.16", "wpb": "175495", "bsz": "17", "num_updates": "85700", "lr": "0.00445967", "gnorm": "0.452", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22036"}
2022-01-28 19:07:00 | INFO | train_inner | {"epoch": 55, "update": 54.801, "loss": "1.981", "ntokens": "175490", "nsentences": "16.56", "wps": "729886", "ups": "4.16", "wpb": "175490", "bsz": "16.6", "num_updates": "85800", "lr": "0.00445845", "gnorm": "0.428", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22060"}
2022-01-28 19:07:24 | INFO | train_inner | {"epoch": 55, "update": 54.865, "loss": "1.991", "ntokens": "175666", "nsentences": "16.56", "wps": "730910", "ups": "4.16", "wpb": "175666", "bsz": "16.6", "num_updates": "85900", "lr": "0.00445722", "gnorm": "0.468", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22084"}
2022-01-28 19:07:48 | INFO | train_inner | {"epoch": 55, "update": 54.929, "loss": "1.987", "ntokens": "175813", "nsentences": "16.8", "wps": "732175", "ups": "4.16", "wpb": "175813", "bsz": "16.8", "num_updates": "86000", "lr": "0.004456", "gnorm": "0.459", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22108"}
2022-01-28 19:08:12 | INFO | train_inner | {"epoch": 55, "update": 54.993, "loss": "2.006", "ntokens": "173785", "nsentences": "17.28", "wps": "726248", "ups": "4.18", "wpb": "173785", "bsz": "17.3", "num_updates": "86100", "lr": "0.00445478", "gnorm": "0.462", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22132"}
2022-01-28 19:08:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:08:19 | INFO | valid | {"epoch": 55, "valid_loss": "2.046", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.4698e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "86111", "valid_best_loss": "2.046"}
2022-01-28 19:08:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 86111 updates
2022-01-28 19:08:19 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:08:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:08:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 55 @ 86111 updates, score 2.046) (writing took 11.443233946338296 seconds)
2022-01-28 19:08:31 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-28 19:08:31 | INFO | train | {"epoch": 55, "train_loss": "1.981", "train_ntokens": "175180", "train_nsentences": "16.8907", "train_wps": "679454", "train_ups": "3.88", "train_wpb": "175180", "train_bsz": "16.9", "train_num_updates": "86111", "train_lr": "0.00445464", "train_gnorm": "0.45", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "22150"}
2022-01-28 19:08:31 | INFO | fairseq.trainer | begin training epoch 56
2022-01-28 19:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:09:03 | INFO | train_inner | {"epoch": 56, "update": 55.057, "loss": "1.96", "ntokens": "176111", "nsentences": "16.48", "wps": "343103", "ups": "1.95", "wpb": "176111", "bsz": "16.5", "num_updates": "86200", "lr": "0.00445355", "gnorm": "0.443", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22183"}
2022-01-28 19:09:27 | INFO | train_inner | {"epoch": 56, "update": 55.121, "loss": "1.955", "ntokens": "176201", "nsentences": "17.12", "wps": "736519", "ups": "4.18", "wpb": "176201", "bsz": "17.1", "num_updates": "86300", "lr": "0.00445232", "gnorm": "0.452", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22207"}
2022-01-28 19:09:51 | INFO | train_inner | {"epoch": 56, "update": 55.185, "loss": "1.963", "ntokens": "175155", "nsentences": "16.48", "wps": "731365", "ups": "4.18", "wpb": "175155", "bsz": "16.5", "num_updates": "86400", "lr": "0.00445109", "gnorm": "0.455", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22231"}
2022-01-28 19:10:15 | INFO | train_inner | {"epoch": 56, "update": 55.248, "loss": "1.972", "ntokens": "174643", "nsentences": "16.48", "wps": "728592", "ups": "4.17", "wpb": "174643", "bsz": "16.5", "num_updates": "86500", "lr": "0.00444986", "gnorm": "0.423", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22255"}
2022-01-28 19:10:39 | INFO | train_inner | {"epoch": 56, "update": 55.312, "loss": "1.973", "ntokens": "174461", "nsentences": "17.12", "wps": "727745", "ups": "4.17", "wpb": "174461", "bsz": "17.1", "num_updates": "86600", "lr": "0.00444863", "gnorm": "0.432", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22279"}
2022-01-28 19:11:03 | INFO | train_inner | {"epoch": 56, "update": 55.376, "loss": "1.991", "ntokens": "175736", "nsentences": "17.12", "wps": "732206", "ups": "4.17", "wpb": "175736", "bsz": "17.1", "num_updates": "86700", "lr": "0.0044474", "gnorm": "0.47", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22303"}
2022-01-28 19:11:27 | INFO | train_inner | {"epoch": 56, "update": 55.44, "loss": "1.963", "ntokens": "174399", "nsentences": "17.12", "wps": "726881", "ups": "4.17", "wpb": "174399", "bsz": "17.1", "num_updates": "86800", "lr": "0.00444617", "gnorm": "0.442", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22327"}
2022-01-28 19:11:51 | INFO | train_inner | {"epoch": 56, "update": 55.504, "loss": "1.977", "ntokens": "176078", "nsentences": "17.04", "wps": "731612", "ups": "4.16", "wpb": "176078", "bsz": "17", "num_updates": "86900", "lr": "0.00444493", "gnorm": "0.428", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22351"}
2022-01-28 19:12:15 | INFO | train_inner | {"epoch": 56, "update": 55.568, "loss": "1.983", "ntokens": "175043", "nsentences": "16.56", "wps": "728072", "ups": "4.16", "wpb": "175043", "bsz": "16.6", "num_updates": "87000", "lr": "0.0044437", "gnorm": "0.478", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22375"}
2022-01-28 19:12:39 | INFO | train_inner | {"epoch": 56, "update": 55.632, "loss": "1.968", "ntokens": "174269", "nsentences": "16.66", "wps": "726158", "ups": "4.17", "wpb": "174269", "bsz": "16.7", "num_updates": "87100", "lr": "0.00444246", "gnorm": "0.427", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22399"}
2022-01-28 19:13:03 | INFO | train_inner | {"epoch": 56, "update": 55.695, "loss": "1.993", "ntokens": "176102", "nsentences": "17.12", "wps": "732178", "ups": "4.16", "wpb": "176102", "bsz": "17.1", "num_updates": "87200", "lr": "0.00444122", "gnorm": "0.414", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22423"}
2022-01-28 19:13:27 | INFO | train_inner | {"epoch": 56, "update": 55.759, "loss": "1.972", "ntokens": "174662", "nsentences": "17.12", "wps": "728183", "ups": "4.17", "wpb": "174662", "bsz": "17.1", "num_updates": "87300", "lr": "0.00443998", "gnorm": "0.473", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22447"}
2022-01-28 19:13:51 | INFO | train_inner | {"epoch": 56, "update": 55.823, "loss": "1.978", "ntokens": "174810", "nsentences": "16.72", "wps": "726216", "ups": "4.15", "wpb": "174810", "bsz": "16.7", "num_updates": "87400", "lr": "0.00443874", "gnorm": "0.429", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "21.1", "wall": "22471"}
2022-01-28 19:14:15 | INFO | train_inner | {"epoch": 56, "update": 55.887, "loss": "1.982", "ntokens": "175052", "nsentences": "17.28", "wps": "729168", "ups": "4.17", "wpb": "175052", "bsz": "17.3", "num_updates": "87500", "lr": "0.0044375", "gnorm": "0.462", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22495"}
2022-01-28 19:14:39 | INFO | train_inner | {"epoch": 56, "update": 55.951, "loss": "1.971", "ntokens": "175577", "nsentences": "16.72", "wps": "732269", "ups": "4.17", "wpb": "175577", "bsz": "16.7", "num_updates": "87600", "lr": "0.00443626", "gnorm": "0.435", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22519"}
2022-01-28 19:14:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:15:02 | INFO | valid | {"epoch": 56, "valid_loss": "2.046", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.61731e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "87677", "valid_best_loss": "2.046"}
2022-01-28 19:15:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 87677 updates
2022-01-28 19:15:02 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 56 @ 87677 updates, score 2.046) (writing took 11.361684395000339 seconds)
2022-01-28 19:15:14 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-28 19:15:14 | INFO | train | {"epoch": 56, "train_loss": "1.973", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "680518", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "87677", "train_lr": "0.0044353", "train_gnorm": "0.444", "train_loss_scale": "0.0625", "train_train_wall": "374", "train_gb_free": "20.7", "train_wall": "22554"}
2022-01-28 19:15:14 | INFO | fairseq.trainer | begin training epoch 57
2022-01-28 19:15:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:15:31 | INFO | train_inner | {"epoch": 57, "update": 56.015, "loss": "1.97", "ntokens": "174022", "nsentences": "16.88", "wps": "337250", "ups": "1.94", "wpb": "174022", "bsz": "16.9", "num_updates": "87700", "lr": "0.00443501", "gnorm": "0.463", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22571"}
2022-01-28 19:15:55 | INFO | train_inner | {"epoch": 57, "update": 56.079, "loss": "1.967", "ntokens": "174964", "nsentences": "16.8", "wps": "734175", "ups": "4.2", "wpb": "174964", "bsz": "16.8", "num_updates": "87800", "lr": "0.00443377", "gnorm": "0.44", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22595"}
2022-01-28 19:16:19 | INFO | train_inner | {"epoch": 57, "update": 56.142, "loss": "1.97", "ntokens": "174135", "nsentences": "17.84", "wps": "730989", "ups": "4.2", "wpb": "174135", "bsz": "17.8", "num_updates": "87900", "lr": "0.00443252", "gnorm": "0.426", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.9", "wall": "22618"}
2022-01-28 19:16:43 | INFO | train_inner | {"epoch": 57, "update": 56.206, "loss": "1.963", "ntokens": "173588", "nsentences": "16.66", "wps": "724171", "ups": "4.17", "wpb": "173588", "bsz": "16.7", "num_updates": "88000", "lr": "0.00443128", "gnorm": "0.449", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22642"}
2022-01-28 19:17:07 | INFO | train_inner | {"epoch": 57, "update": 56.27, "loss": "1.959", "ntokens": "176277", "nsentences": "16.56", "wps": "734992", "ups": "4.17", "wpb": "176277", "bsz": "16.6", "num_updates": "88100", "lr": "0.00443003", "gnorm": "0.396", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22666"}
2022-01-28 19:17:31 | INFO | train_inner | {"epoch": 57, "update": 56.334, "loss": "1.961", "ntokens": "176753", "nsentences": "17.12", "wps": "736324", "ups": "4.17", "wpb": "176753", "bsz": "17.1", "num_updates": "88200", "lr": "0.00442878", "gnorm": "0.458", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22690"}
2022-01-28 19:17:55 | INFO | train_inner | {"epoch": 57, "update": 56.398, "loss": "1.964", "ntokens": "175903", "nsentences": "16.88", "wps": "732363", "ups": "4.16", "wpb": "175903", "bsz": "16.9", "num_updates": "88300", "lr": "0.00442752", "gnorm": "0.434", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22714"}
2022-01-28 19:18:19 | INFO | train_inner | {"epoch": 57, "update": 56.462, "loss": "1.96", "ntokens": "174616", "nsentences": "16.8", "wps": "728436", "ups": "4.17", "wpb": "174616", "bsz": "16.8", "num_updates": "88400", "lr": "0.00442627", "gnorm": "0.437", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22738"}
2022-01-28 19:18:42 | INFO | train_inner | {"epoch": 57, "update": 56.526, "loss": "1.98", "ntokens": "174857", "nsentences": "17.52", "wps": "732123", "ups": "4.19", "wpb": "174857", "bsz": "17.5", "num_updates": "88500", "lr": "0.00442502", "gnorm": "0.466", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22762"}
2022-01-28 19:19:06 | INFO | train_inner | {"epoch": 57, "update": 56.589, "loss": "1.963", "ntokens": "174679", "nsentences": "16.56", "wps": "727986", "ups": "4.17", "wpb": "174679", "bsz": "16.6", "num_updates": "88600", "lr": "0.00442376", "gnorm": "0.434", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22786"}
2022-01-28 19:19:30 | INFO | train_inner | {"epoch": 57, "update": 56.653, "loss": "1.966", "ntokens": "175899", "nsentences": "16.72", "wps": "732725", "ups": "4.17", "wpb": "175899", "bsz": "16.7", "num_updates": "88700", "lr": "0.00442251", "gnorm": "0.456", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22810"}
2022-01-28 19:19:54 | INFO | train_inner | {"epoch": 57, "update": 56.717, "loss": "1.963", "ntokens": "175986", "nsentences": "17.2", "wps": "732884", "ups": "4.16", "wpb": "175986", "bsz": "17.2", "num_updates": "88800", "lr": "0.00442125", "gnorm": "0.461", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22834"}
2022-01-28 19:20:19 | INFO | train_inner | {"epoch": 57, "update": 56.781, "loss": "1.968", "ntokens": "175724", "nsentences": "16.96", "wps": "731523", "ups": "4.16", "wpb": "175724", "bsz": "17", "num_updates": "88900", "lr": "0.00441999", "gnorm": "0.454", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22858"}
2022-01-28 19:20:43 | INFO | train_inner | {"epoch": 57, "update": 56.845, "loss": "1.96", "ntokens": "174468", "nsentences": "16.56", "wps": "727464", "ups": "4.17", "wpb": "174468", "bsz": "16.6", "num_updates": "89000", "lr": "0.00441873", "gnorm": "0.436", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22882"}
2022-01-28 19:21:07 | INFO | train_inner | {"epoch": 57, "update": 56.909, "loss": "1.976", "ntokens": "176048", "nsentences": "16.88", "wps": "732609", "ups": "4.16", "wpb": "176048", "bsz": "16.9", "num_updates": "89100", "lr": "0.00441747", "gnorm": "0.433", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22906"}
2022-01-28 19:21:31 | INFO | train_inner | {"epoch": 57, "update": 56.973, "loss": "1.973", "ntokens": "175242", "nsentences": "16.64", "wps": "730246", "ups": "4.17", "wpb": "175242", "bsz": "16.6", "num_updates": "89200", "lr": "0.00441621", "gnorm": "0.494", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22930"}
2022-01-28 19:21:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:21:46 | INFO | valid | {"epoch": 57, "valid_loss": "2.039", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.58265e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "89243", "valid_best_loss": "2.039"}
2022-01-28 19:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 89243 updates
2022-01-28 19:21:46 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:21:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:21:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 57 @ 89243 updates, score 2.039) (writing took 11.952869172208011 seconds)
2022-01-28 19:21:58 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-28 19:21:58 | INFO | train | {"epoch": 57, "train_loss": "1.966", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "679194", "train_ups": "3.88", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "89243", "train_lr": "0.00441567", "train_gnorm": "0.446", "train_loss_scale": "0.0625", "train_train_wall": "373", "train_gb_free": "20.8", "train_wall": "22958"}
2022-01-28 19:21:58 | INFO | fairseq.trainer | begin training epoch 58
2022-01-28 19:21:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:22:23 | INFO | train_inner | {"epoch": 58, "update": 57.036, "loss": "1.953", "ntokens": "174463", "nsentences": "16.72", "wps": "335638", "ups": "1.92", "wpb": "174463", "bsz": "16.7", "num_updates": "89300", "lr": "0.00441495", "gnorm": "0.459", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "22982"}
2022-01-28 19:22:46 | INFO | train_inner | {"epoch": 58, "update": 57.1, "loss": "1.963", "ntokens": "173965", "nsentences": "16.56", "wps": "730719", "ups": "4.2", "wpb": "173965", "bsz": "16.6", "num_updates": "89400", "lr": "0.00441368", "gnorm": "0.425", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23006"}
2022-01-28 19:23:10 | INFO | train_inner | {"epoch": 58, "update": 57.164, "loss": "1.961", "ntokens": "174396", "nsentences": "17.12", "wps": "729457", "ups": "4.18", "wpb": "174396", "bsz": "17.1", "num_updates": "89500", "lr": "0.00441242", "gnorm": "0.389", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23030"}
2022-01-28 19:23:34 | INFO | train_inner | {"epoch": 58, "update": 57.228, "loss": "1.965", "ntokens": "176097", "nsentences": "16.64", "wps": "731806", "ups": "4.16", "wpb": "176097", "bsz": "16.6", "num_updates": "89600", "lr": "0.00441115", "gnorm": "0.434", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23054"}
2022-01-28 19:23:58 | INFO | train_inner | {"epoch": 58, "update": 57.292, "loss": "1.964", "ntokens": "173442", "nsentences": "16.98", "wps": "725893", "ups": "4.19", "wpb": "173442", "bsz": "17", "num_updates": "89700", "lr": "0.00440988", "gnorm": "0.43", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23078"}
2022-01-28 19:24:22 | INFO | train_inner | {"epoch": 58, "update": 57.356, "loss": "1.944", "ntokens": "176648", "nsentences": "16.48", "wps": "735269", "ups": "4.16", "wpb": "176648", "bsz": "16.5", "num_updates": "89800", "lr": "0.00440861", "gnorm": "0.471", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23102"}
2022-01-28 19:24:46 | INFO | train_inner | {"epoch": 58, "update": 57.42, "loss": "1.957", "ntokens": "174251", "nsentences": "16.4", "wps": "725773", "ups": "4.17", "wpb": "174251", "bsz": "16.4", "num_updates": "89900", "lr": "0.00440734", "gnorm": "0.411", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23126"}
2022-01-28 19:25:10 | INFO | train_inner | {"epoch": 58, "update": 57.483, "loss": "1.958", "ntokens": "175343", "nsentences": "17.12", "wps": "729962", "ups": "4.16", "wpb": "175343", "bsz": "17.1", "num_updates": "90000", "lr": "0.00440607", "gnorm": "0.467", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23150"}
2022-01-28 19:25:34 | INFO | train_inner | {"epoch": 58, "update": 57.547, "loss": "1.96", "ntokens": "176153", "nsentences": "17.04", "wps": "733114", "ups": "4.16", "wpb": "176153", "bsz": "17", "num_updates": "90100", "lr": "0.0044048", "gnorm": "0.452", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23174"}
2022-01-28 19:25:58 | INFO | train_inner | {"epoch": 58, "update": 57.611, "loss": "1.953", "ntokens": "174965", "nsentences": "16.88", "wps": "729895", "ups": "4.17", "wpb": "174965", "bsz": "16.9", "num_updates": "90200", "lr": "0.00440353", "gnorm": "0.435", "loss_scale": "0.0625", "train_wall": "24", "gb_free": "20.4", "wall": "23198"}
2022-01-28 19:26:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2022-01-28 19:26:23 | INFO | train_inner | {"epoch": 58, "update": 57.676, "loss": "1.958", "ntokens": "175915", "nsentences": "17.6", "wps": "723528", "ups": "4.11", "wpb": "175915", "bsz": "17.6", "num_updates": "90300", "lr": "0.00440225", "gnorm": "0.461", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23222"}
2022-01-28 19:26:47 | INFO | train_inner | {"epoch": 58, "update": 57.739, "loss": "1.947", "ntokens": "174961", "nsentences": "16.96", "wps": "727515", "ups": "4.16", "wpb": "174961", "bsz": "17", "num_updates": "90400", "lr": "0.00440098", "gnorm": "0.42", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23247"}
2022-01-28 19:27:11 | INFO | train_inner | {"epoch": 58, "update": 57.803, "loss": "1.968", "ntokens": "176039", "nsentences": "17.12", "wps": "731545", "ups": "4.16", "wpb": "176039", "bsz": "17.1", "num_updates": "90500", "lr": "0.0043997", "gnorm": "0.426", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23271"}
2022-01-28 19:27:35 | INFO | train_inner | {"epoch": 58, "update": 57.867, "loss": "1.961", "ntokens": "174576", "nsentences": "16.56", "wps": "730167", "ups": "4.18", "wpb": "174576", "bsz": "16.6", "num_updates": "90600", "lr": "0.00439842", "gnorm": "0.456", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23295"}
2022-01-28 19:27:59 | INFO | train_inner | {"epoch": 58, "update": 57.931, "loss": "1.963", "ntokens": "174087", "nsentences": "17.12", "wps": "727026", "ups": "4.18", "wpb": "174087", "bsz": "17.1", "num_updates": "90700", "lr": "0.00439714", "gnorm": "0.414", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.2", "wall": "23318"}
2022-01-28 19:28:23 | INFO | train_inner | {"epoch": 58, "update": 57.995, "loss": "1.945", "ntokens": "176451", "nsentences": "16.88", "wps": "730835", "ups": "4.14", "wpb": "176451", "bsz": "16.9", "num_updates": "90800", "lr": "0.00439586", "gnorm": "0.438", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23343"}
2022-01-28 19:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:28:29 | INFO | valid | {"epoch": 58, "valid_loss": "2.037", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.70634e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "90808", "valid_best_loss": "2.037"}
2022-01-28 19:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 90808 updates
2022-01-28 19:28:29 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:28:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:28:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 58 @ 90808 updates, score 2.037) (writing took 12.860896461643279 seconds)
2022-01-28 19:28:42 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-28 19:28:42 | INFO | train | {"epoch": 58, "train_loss": "1.957", "train_ntokens": "175180", "train_nsentences": "16.8907", "train_wps": "678040", "train_ups": "3.87", "train_wpb": "175180", "train_bsz": "16.9", "train_num_updates": "90808", "train_lr": "0.00439576", "train_gnorm": "0.436", "train_loss_scale": "0.0312", "train_train_wall": "374", "train_gb_free": "20.4", "train_wall": "23362"}
2022-01-28 19:28:42 | INFO | fairseq.trainer | begin training epoch 59
2022-01-28 19:28:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:29:15 | INFO | train_inner | {"epoch": 59, "update": 58.059, "loss": "1.943", "ntokens": "174745", "nsentences": "16.64", "wps": "336706", "ups": "1.93", "wpb": "174745", "bsz": "16.6", "num_updates": "90900", "lr": "0.00439458", "gnorm": "0.44", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23395"}
2022-01-28 19:29:39 | INFO | train_inner | {"epoch": 59, "update": 58.123, "loss": "1.945", "ntokens": "175213", "nsentences": "16.8", "wps": "736075", "ups": "4.2", "wpb": "175213", "bsz": "16.8", "num_updates": "91000", "lr": "0.00439329", "gnorm": "0.437", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23418"}
2022-01-28 19:30:02 | INFO | train_inner | {"epoch": 59, "update": 58.186, "loss": "1.948", "ntokens": "174848", "nsentences": "16.88", "wps": "731150", "ups": "4.18", "wpb": "174848", "bsz": "16.9", "num_updates": "91100", "lr": "0.00439201", "gnorm": "0.426", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "21.3", "wall": "23442"}
2022-01-28 19:30:26 | INFO | train_inner | {"epoch": 59, "update": 58.25, "loss": "1.95", "ntokens": "175761", "nsentences": "17.36", "wps": "733670", "ups": "4.17", "wpb": "175761", "bsz": "17.4", "num_updates": "91200", "lr": "0.00439072", "gnorm": "0.447", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23466"}
2022-01-28 19:30:50 | INFO | train_inner | {"epoch": 59, "update": 58.314, "loss": "1.954", "ntokens": "174994", "nsentences": "17.28", "wps": "729670", "ups": "4.17", "wpb": "174994", "bsz": "17.3", "num_updates": "91300", "lr": "0.00438944", "gnorm": "0.409", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23490"}
2022-01-28 19:31:14 | INFO | train_inner | {"epoch": 59, "update": 58.378, "loss": "1.946", "ntokens": "174057", "nsentences": "16.56", "wps": "727382", "ups": "4.18", "wpb": "174057", "bsz": "16.6", "num_updates": "91400", "lr": "0.00438815", "gnorm": "0.459", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23514"}
2022-01-28 19:31:38 | INFO | train_inner | {"epoch": 59, "update": 58.442, "loss": "1.944", "ntokens": "174909", "nsentences": "16.88", "wps": "729353", "ups": "4.17", "wpb": "174909", "bsz": "16.9", "num_updates": "91500", "lr": "0.00438686", "gnorm": "0.455", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.1", "wall": "23538"}
2022-01-28 19:32:02 | INFO | train_inner | {"epoch": 59, "update": 58.506, "loss": "1.958", "ntokens": "175390", "nsentences": "16.74", "wps": "730654", "ups": "4.17", "wpb": "175390", "bsz": "16.7", "num_updates": "91600", "lr": "0.00438557", "gnorm": "0.443", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23562"}
2022-01-28 19:32:26 | INFO | train_inner | {"epoch": 59, "update": 58.57, "loss": "1.955", "ntokens": "174721", "nsentences": "16.4", "wps": "729735", "ups": "4.18", "wpb": "174721", "bsz": "16.4", "num_updates": "91700", "lr": "0.00438428", "gnorm": "0.436", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23586"}
2022-01-28 19:32:50 | INFO | train_inner | {"epoch": 59, "update": 58.633, "loss": "1.977", "ntokens": "175621", "nsentences": "17.92", "wps": "731868", "ups": "4.17", "wpb": "175621", "bsz": "17.9", "num_updates": "91800", "lr": "0.00438299", "gnorm": "0.468", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.5", "wall": "23610"}
2022-01-28 19:33:14 | INFO | train_inner | {"epoch": 59, "update": 58.697, "loss": "1.962", "ntokens": "175183", "nsentences": "16.56", "wps": "729187", "ups": "4.16", "wpb": "175183", "bsz": "16.6", "num_updates": "91900", "lr": "0.00438169", "gnorm": "0.446", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23634"}
2022-01-28 19:33:38 | INFO | train_inner | {"epoch": 59, "update": 58.761, "loss": "1.937", "ntokens": "175820", "nsentences": "17.12", "wps": "733598", "ups": "4.17", "wpb": "175820", "bsz": "17.1", "num_updates": "92000", "lr": "0.0043804", "gnorm": "0.407", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23658"}
2022-01-28 19:34:02 | INFO | train_inner | {"epoch": 59, "update": 58.825, "loss": "1.951", "ntokens": "175611", "nsentences": "17.04", "wps": "733630", "ups": "4.18", "wpb": "175611", "bsz": "17", "num_updates": "92100", "lr": "0.0043791", "gnorm": "0.441", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23682"}
2022-01-28 19:34:26 | INFO | train_inner | {"epoch": 59, "update": 58.889, "loss": "1.945", "ntokens": "175313", "nsentences": "16.56", "wps": "731060", "ups": "4.17", "wpb": "175313", "bsz": "16.6", "num_updates": "92200", "lr": "0.00437781", "gnorm": "0.45", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23706"}
2022-01-28 19:34:50 | INFO | train_inner | {"epoch": 59, "update": 58.953, "loss": "1.956", "ntokens": "175940", "nsentences": "16.72", "wps": "735308", "ups": "4.18", "wpb": "175940", "bsz": "16.7", "num_updates": "92300", "lr": "0.00437651", "gnorm": "0.448", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23730"}
2022-01-28 19:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:35:13 | INFO | valid | {"epoch": 59, "valid_loss": "2.026", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.51782e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "92374", "valid_best_loss": "2.026"}
2022-01-28 19:35:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 92374 updates
2022-01-28 19:35:13 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:35:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 59 @ 92374 updates, score 2.026) (writing took 15.369787308387458 seconds)
2022-01-28 19:35:28 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-28 19:35:28 | INFO | train | {"epoch": 59, "train_loss": "1.951", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "675268", "train_ups": "3.85", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "92374", "train_lr": "0.00437555", "train_gnorm": "0.44", "train_loss_scale": "0.0312", "train_train_wall": "373", "train_gb_free": "20.4", "train_wall": "23768"}
2022-01-28 19:35:28 | INFO | fairseq.trainer | begin training epoch 60
2022-01-28 19:35:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:35:46 | INFO | train_inner | {"epoch": 60, "update": 59.017, "loss": "1.95", "ntokens": "174338", "nsentences": "16.64", "wps": "314175", "ups": "1.8", "wpb": "174338", "bsz": "16.6", "num_updates": "92400", "lr": "0.00437521", "gnorm": "0.421", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23786"}
2022-01-28 19:36:09 | INFO | train_inner | {"epoch": 60, "update": 59.08, "loss": "1.958", "ntokens": "175156", "nsentences": "18", "wps": "737806", "ups": "4.21", "wpb": "175156", "bsz": "18", "num_updates": "92500", "lr": "0.00437391", "gnorm": "0.466", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23809"}
2022-01-28 19:36:33 | INFO | train_inner | {"epoch": 60, "update": 59.144, "loss": "1.932", "ntokens": "174816", "nsentences": "16.8", "wps": "731828", "ups": "4.19", "wpb": "174816", "bsz": "16.8", "num_updates": "92600", "lr": "0.00437261", "gnorm": "0.452", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23833"}
2022-01-28 19:36:57 | INFO | train_inner | {"epoch": 60, "update": 59.208, "loss": "1.936", "ntokens": "175730", "nsentences": "17.12", "wps": "733464", "ups": "4.17", "wpb": "175730", "bsz": "17.1", "num_updates": "92700", "lr": "0.0043713", "gnorm": "0.434", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23857"}
2022-01-28 19:37:21 | INFO | train_inner | {"epoch": 60, "update": 59.272, "loss": "1.932", "ntokens": "176927", "nsentences": "17.04", "wps": "737703", "ups": "4.17", "wpb": "176927", "bsz": "17", "num_updates": "92800", "lr": "0.00437", "gnorm": "0.433", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23881"}
2022-01-28 19:37:45 | INFO | train_inner | {"epoch": 60, "update": 59.336, "loss": "1.947", "ntokens": "175709", "nsentences": "16.72", "wps": "733116", "ups": "4.17", "wpb": "175709", "bsz": "16.7", "num_updates": "92900", "lr": "0.00436869", "gnorm": "0.445", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23905"}
2022-01-28 19:38:09 | INFO | train_inner | {"epoch": 60, "update": 59.4, "loss": "1.939", "ntokens": "175263", "nsentences": "16.48", "wps": "731576", "ups": "4.17", "wpb": "175263", "bsz": "16.5", "num_updates": "93000", "lr": "0.00436739", "gnorm": "0.434", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23929"}
2022-01-28 19:38:33 | INFO | train_inner | {"epoch": 60, "update": 59.464, "loss": "1.942", "ntokens": "175645", "nsentences": "16.4", "wps": "732099", "ups": "4.17", "wpb": "175645", "bsz": "16.4", "num_updates": "93100", "lr": "0.00436608", "gnorm": "0.444", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23953"}
2022-01-28 19:38:57 | INFO | train_inner | {"epoch": 60, "update": 59.527, "loss": "1.937", "ntokens": "174857", "nsentences": "16.4", "wps": "729972", "ups": "4.17", "wpb": "174857", "bsz": "16.4", "num_updates": "93200", "lr": "0.00436477", "gnorm": "0.42", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "23977"}
2022-01-28 19:39:21 | INFO | train_inner | {"epoch": 60, "update": 59.591, "loss": "1.945", "ntokens": "173450", "nsentences": "16.72", "wps": "724390", "ups": "4.18", "wpb": "173450", "bsz": "16.7", "num_updates": "93300", "lr": "0.00436346", "gnorm": "0.462", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "24001"}
2022-01-28 19:39:45 | INFO | train_inner | {"epoch": 60, "update": 59.655, "loss": "1.942", "ntokens": "175583", "nsentences": "16.72", "wps": "732132", "ups": "4.17", "wpb": "175583", "bsz": "16.7", "num_updates": "93400", "lr": "0.00436215", "gnorm": "0.474", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "24025"}
2022-01-28 19:40:09 | INFO | train_inner | {"epoch": 60, "update": 59.719, "loss": "1.947", "ntokens": "173880", "nsentences": "17.76", "wps": "724652", "ups": "4.17", "wpb": "173880", "bsz": "17.8", "num_updates": "93500", "lr": "0.00436084", "gnorm": "0.455", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "24049"}
2022-01-28 19:40:33 | INFO | train_inner | {"epoch": 60, "update": 59.783, "loss": "1.953", "ntokens": "173700", "nsentences": "16.74", "wps": "724870", "ups": "4.17", "wpb": "173700", "bsz": "16.7", "num_updates": "93600", "lr": "0.00435952", "gnorm": "0.473", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "24073"}
2022-01-28 19:40:57 | INFO | train_inner | {"epoch": 60, "update": 59.847, "loss": "1.96", "ntokens": "175441", "nsentences": "16.16", "wps": "731349", "ups": "4.17", "wpb": "175441", "bsz": "16.2", "num_updates": "93700", "lr": "0.00435821", "gnorm": "0.415", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "24097"}
2022-01-28 19:41:21 | INFO | train_inner | {"epoch": 60, "update": 59.911, "loss": "1.954", "ntokens": "176031", "nsentences": "17.28", "wps": "733551", "ups": "4.17", "wpb": "176031", "bsz": "17.3", "num_updates": "93800", "lr": "0.00435689", "gnorm": "0.456", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.7", "wall": "24121"}
2022-01-28 19:41:45 | INFO | train_inner | {"epoch": 60, "update": 59.974, "loss": "1.953", "ntokens": "176640", "nsentences": "17.2", "wps": "734859", "ups": "4.16", "wpb": "176640", "bsz": "17.2", "num_updates": "93900", "lr": "0.00435558", "gnorm": "0.437", "loss_scale": "0.0312", "train_wall": "24", "gb_free": "20.4", "wall": "24145"}
2022-01-28 19:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2022-01-28 19:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:41:59 | INFO | valid | {"epoch": 60, "valid_loss": "2.02", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.68747e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "93939", "valid_best_loss": "2.02"}
2022-01-28 19:41:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 93939 updates
2022-01-28 19:41:59 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:42:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:42:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 60 @ 93939 updates, score 2.02) (writing took 11.302571790292859 seconds)
2022-01-28 19:42:11 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-28 19:42:11 | INFO | train | {"epoch": 60, "train_loss": "1.946", "train_ntokens": "175180", "train_nsentences": "16.8907", "train_wps": "681349", "train_ups": "3.89", "train_wpb": "175180", "train_bsz": "16.9", "train_num_updates": "93939", "train_lr": "0.00435506", "train_gnorm": "0.446", "train_loss_scale": "0.0156", "train_train_wall": "373", "train_gb_free": "20.4", "train_wall": "24171"}
2022-01-28 19:42:11 | INFO | fairseq.trainer | begin training epoch 61
2022-01-28 19:42:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:42:36 | INFO | train_inner | {"epoch": 61, "update": 60.039, "loss": "1.934", "ntokens": "175325", "nsentences": "16.48", "wps": "343906", "ups": "1.96", "wpb": "175325", "bsz": "16.5", "num_updates": "94000", "lr": "0.00435426", "gnorm": "0.417", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "24196"}
2022-01-28 19:43:00 | INFO | train_inner | {"epoch": 61, "update": 60.103, "loss": "1.934", "ntokens": "174842", "nsentences": "17.04", "wps": "733936", "ups": "4.2", "wpb": "174842", "bsz": "17", "num_updates": "94100", "lr": "0.00435294", "gnorm": "0.445", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "24220"}
2022-01-28 19:43:24 | INFO | train_inner | {"epoch": 61, "update": 60.167, "loss": "1.928", "ntokens": "175271", "nsentences": "16.48", "wps": "732637", "ups": "4.18", "wpb": "175271", "bsz": "16.5", "num_updates": "94200", "lr": "0.00435162", "gnorm": "0.456", "loss_scale": "0.0156", "train_wall": "24", "gb_free": "20.4", "wall": "24244"}
2022-01-28 19:43:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125
2022-01-28 19:43:48 | INFO | train_inner | {"epoch": 61, "update": 60.231, "loss": "1.942", "ntokens": "176021", "nsentences": "16.88", "wps": "730232", "ups": "4.15", "wpb": "176021", "bsz": "16.9", "num_updates": "94300", "lr": "0.0043503", "gnorm": "0.425", "loss_scale": "0.0078", "train_wall": "24", "gb_free": "20.4", "wall": "24268"}
2022-01-28 19:44:12 | INFO | train_inner | {"epoch": 61, "update": 60.295, "loss": "1.948", "ntokens": "176145", "nsentences": "16.88", "wps": "735406", "ups": "4.18", "wpb": "176145", "bsz": "16.9", "num_updates": "94400", "lr": "0.00434898", "gnorm": "0.488", "loss_scale": "0.0078", "train_wall": "24", "gb_free": "20.4", "wall": "24292"}
2022-01-28 19:44:36 | INFO | train_inner | {"epoch": 61, "update": 60.359, "loss": "1.941", "ntokens": "173995", "nsentences": "16.48", "wps": "725840", "ups": "4.17", "wpb": "173995", "bsz": "16.5", "num_updates": "94500", "lr": "0.00434765", "gnorm": "0.453", "loss_scale": "0.0078", "train_wall": "24", "gb_free": "20.3", "wall": "24316"}
2022-01-28 19:45:00 | INFO | train_inner | {"epoch": 61, "update": 60.423, "loss": "1.946", "ntokens": "174604", "nsentences": "17.12", "wps": "730222", "ups": "4.18", "wpb": "174604", "bsz": "17.1", "num_updates": "94600", "lr": "0.00434633", "gnorm": "0.45", "loss_scale": "0.0078", "train_wall": "24", "gb_free": "20.4", "wall": "24340"}
2022-01-28 19:45:24 | INFO | train_inner | {"epoch": 61, "update": 60.487, "loss": "1.961", "ntokens": "176414", "nsentences": "16.8", "wps": "733737", "ups": "4.16", "wpb": "176414", "bsz": "16.8", "num_updates": "94700", "lr": "0.004345", "gnorm": "0.432", "loss_scale": "0.0078", "train_wall": "24", "gb_free": "20.4", "wall": "24364"}
2022-01-28 19:45:48 | INFO | train_inner | {"epoch": 61, "update": 60.55, "loss": "1.939", "ntokens": "175565", "nsentences": "16.4", "wps": "732079", "ups": "4.17", "wpb": "175565", "bsz": "16.4", "num_updates": "94800", "lr": "0.00434368", "gnorm": "0.438", "loss_scale": "0.0078", "train_wall": "24", "gb_free": "20.4", "wall": "24388"}
2022-01-28 19:46:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00390625
2022-01-28 19:46:12 | INFO | train_inner | {"epoch": 61, "update": 60.615, "loss": "1.945", "ntokens": "174600", "nsentences": "17.04", "wps": "720949", "ups": "4.13", "wpb": "174600", "bsz": "17", "num_updates": "94900", "lr": "0.00434235", "gnorm": "0.45", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24412"}
2022-01-28 19:46:36 | INFO | train_inner | {"epoch": 61, "update": 60.679, "loss": "1.932", "ntokens": "176032", "nsentences": "17.52", "wps": "734368", "ups": "4.17", "wpb": "176032", "bsz": "17.5", "num_updates": "95000", "lr": "0.00434102", "gnorm": "0.406", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24436"}
2022-01-28 19:47:00 | INFO | train_inner | {"epoch": 61, "update": 60.743, "loss": "1.946", "ntokens": "176174", "nsentences": "17.2", "wps": "735874", "ups": "4.18", "wpb": "176174", "bsz": "17.2", "num_updates": "95100", "lr": "0.00433969", "gnorm": "0.421", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24460"}
2022-01-28 19:47:24 | INFO | train_inner | {"epoch": 61, "update": 60.807, "loss": "1.939", "ntokens": "174635", "nsentences": "17.06", "wps": "727716", "ups": "4.17", "wpb": "174635", "bsz": "17.1", "num_updates": "95200", "lr": "0.00433836", "gnorm": "0.43", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24484"}
2022-01-28 19:47:48 | INFO | train_inner | {"epoch": 61, "update": 60.87, "loss": "1.917", "ntokens": "173761", "nsentences": "16.48", "wps": "727357", "ups": "4.19", "wpb": "173761", "bsz": "16.5", "num_updates": "95300", "lr": "0.00433702", "gnorm": "0.453", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24508"}
2022-01-28 19:48:12 | INFO | train_inner | {"epoch": 61, "update": 60.934, "loss": "1.942", "ntokens": "174754", "nsentences": "17.6", "wps": "731178", "ups": "4.18", "wpb": "174754", "bsz": "17.6", "num_updates": "95400", "lr": "0.00433569", "gnorm": "0.415", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24532"}
2022-01-28 19:48:36 | INFO | train_inner | {"epoch": 61, "update": 60.998, "loss": "1.953", "ntokens": "174020", "nsentences": "16.56", "wps": "722402", "ups": "4.15", "wpb": "174020", "bsz": "16.6", "num_updates": "95500", "lr": "0.00433435", "gnorm": "0.457", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24556"}
2022-01-28 19:48:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:48:41 | INFO | valid | {"epoch": 61, "valid_loss": "2.015", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.59458e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "95503", "valid_best_loss": "2.015"}
2022-01-28 19:48:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 95503 updates
2022-01-28 19:48:41 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:48:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:48:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 61 @ 95503 updates, score 2.015) (writing took 11.391288944520056 seconds)
2022-01-28 19:48:53 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-28 19:48:53 | INFO | train | {"epoch": 61, "train_loss": "1.94", "train_ntokens": "175178", "train_nsentences": "16.8913", "train_wps": "681733", "train_ups": "3.89", "train_wpb": "175178", "train_bsz": "16.9", "train_num_updates": "95503", "train_lr": "0.00433431", "train_gnorm": "0.44", "train_loss_scale": "0.0039", "train_train_wall": "373", "train_gb_free": "20", "train_wall": "24572"}
2022-01-28 19:48:53 | INFO | fairseq.trainer | begin training epoch 62
2022-01-28 19:48:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:49:27 | INFO | train_inner | {"epoch": 62, "update": 61.062, "loss": "1.926", "ntokens": "175183", "nsentences": "17.52", "wps": "345856", "ups": "1.97", "wpb": "175183", "bsz": "17.5", "num_updates": "95600", "lr": "0.00433302", "gnorm": "0.43", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24607"}
2022-01-28 19:49:51 | INFO | train_inner | {"epoch": 62, "update": 61.126, "loss": "1.916", "ntokens": "176195", "nsentences": "16.56", "wps": "740366", "ups": "4.2", "wpb": "176195", "bsz": "16.6", "num_updates": "95700", "lr": "0.00433168", "gnorm": "0.435", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24630"}
2022-01-28 19:50:14 | INFO | train_inner | {"epoch": 62, "update": 61.19, "loss": "1.932", "ntokens": "176687", "nsentences": "16.64", "wps": "739379", "ups": "4.18", "wpb": "176687", "bsz": "16.6", "num_updates": "95800", "lr": "0.00433034", "gnorm": "0.455", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24654"}
2022-01-28 19:50:38 | INFO | train_inner | {"epoch": 62, "update": 61.254, "loss": "1.932", "ntokens": "174657", "nsentences": "17.12", "wps": "730120", "ups": "4.18", "wpb": "174657", "bsz": "17.1", "num_updates": "95900", "lr": "0.004329", "gnorm": "0.416", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24678"}
2022-01-28 19:51:02 | INFO | train_inner | {"epoch": 62, "update": 61.317, "loss": "1.935", "ntokens": "174539", "nsentences": "16.66", "wps": "728389", "ups": "4.17", "wpb": "174539", "bsz": "16.7", "num_updates": "96000", "lr": "0.00432766", "gnorm": "0.445", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24702"}
2022-01-28 19:51:26 | INFO | train_inner | {"epoch": 62, "update": 61.381, "loss": "1.935", "ntokens": "175284", "nsentences": "16.96", "wps": "732331", "ups": "4.18", "wpb": "175284", "bsz": "17", "num_updates": "96100", "lr": "0.00432632", "gnorm": "0.447", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24726"}
2022-01-28 19:51:50 | INFO | train_inner | {"epoch": 62, "update": 61.445, "loss": "1.928", "ntokens": "175865", "nsentences": "16.56", "wps": "734539", "ups": "4.18", "wpb": "175865", "bsz": "16.6", "num_updates": "96200", "lr": "0.00432498", "gnorm": "0.468", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24750"}
2022-01-28 19:52:14 | INFO | train_inner | {"epoch": 62, "update": 61.509, "loss": "1.921", "ntokens": "176279", "nsentences": "16.4", "wps": "735729", "ups": "4.17", "wpb": "176279", "bsz": "16.4", "num_updates": "96300", "lr": "0.00432363", "gnorm": "0.454", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24774"}
2022-01-28 19:52:38 | INFO | train_inner | {"epoch": 62, "update": 61.573, "loss": "1.932", "ntokens": "172904", "nsentences": "16.48", "wps": "722072", "ups": "4.18", "wpb": "172904", "bsz": "16.5", "num_updates": "96400", "lr": "0.00432229", "gnorm": "0.483", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24798"}
2022-01-28 19:53:02 | INFO | train_inner | {"epoch": 62, "update": 61.637, "loss": "1.92", "ntokens": "175554", "nsentences": "17.04", "wps": "734203", "ups": "4.18", "wpb": "175554", "bsz": "17", "num_updates": "96500", "lr": "0.00432094", "gnorm": "0.432", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24822"}
2022-01-28 19:53:26 | INFO | train_inner | {"epoch": 62, "update": 61.701, "loss": "1.929", "ntokens": "174066", "nsentences": "16.24", "wps": "728264", "ups": "4.18", "wpb": "174066", "bsz": "16.2", "num_updates": "96600", "lr": "0.00431959", "gnorm": "0.412", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24846"}
2022-01-28 19:53:50 | INFO | train_inner | {"epoch": 62, "update": 61.764, "loss": "1.938", "ntokens": "175290", "nsentences": "16.88", "wps": "734431", "ups": "4.19", "wpb": "175290", "bsz": "16.9", "num_updates": "96700", "lr": "0.00431825", "gnorm": "0.434", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24870"}
2022-01-28 19:54:14 | INFO | train_inner | {"epoch": 62, "update": 61.828, "loss": "1.945", "ntokens": "175139", "nsentences": "18.24", "wps": "731661", "ups": "4.18", "wpb": "175139", "bsz": "18.2", "num_updates": "96800", "lr": "0.0043169", "gnorm": "0.45", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24894"}
2022-01-28 19:54:38 | INFO | train_inner | {"epoch": 62, "update": 61.892, "loss": "1.949", "ntokens": "175697", "nsentences": "17.2", "wps": "732634", "ups": "4.17", "wpb": "175697", "bsz": "17.2", "num_updates": "96900", "lr": "0.00431555", "gnorm": "0.431", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24918"}
2022-01-28 19:55:02 | INFO | train_inner | {"epoch": 62, "update": 61.956, "loss": "1.93", "ntokens": "174541", "nsentences": "17.12", "wps": "730477", "ups": "4.19", "wpb": "174541", "bsz": "17.1", "num_updates": "97000", "lr": "0.00431419", "gnorm": "0.426", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24942"}
2022-01-28 19:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:55:23 | INFO | valid | {"epoch": 62, "valid_loss": "2.011", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.73903e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "97069", "valid_best_loss": "2.011"}
2022-01-28 19:55:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 97069 updates
2022-01-28 19:55:23 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:55:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 19:55:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 62 @ 97069 updates, score 2.011) (writing took 11.168238666839898 seconds)
2022-01-28 19:55:34 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-28 19:55:34 | INFO | train | {"epoch": 62, "train_loss": "1.932", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "683531", "train_ups": "3.9", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "97069", "train_lr": "0.00431326", "train_gnorm": "0.441", "train_loss_scale": "0.0039", "train_train_wall": "372", "train_gb_free": "20.4", "train_wall": "24974"}
2022-01-28 19:55:34 | INFO | fairseq.trainer | begin training epoch 63
2022-01-28 19:55:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:55:52 | INFO | train_inner | {"epoch": 63, "update": 62.02, "loss": "1.94", "ntokens": "174500", "nsentences": "16.48", "wps": "345169", "ups": "1.98", "wpb": "174500", "bsz": "16.5", "num_updates": "97100", "lr": "0.00431284", "gnorm": "0.422", "loss_scale": "0.0039", "train_wall": "24", "gb_free": "20.4", "wall": "24992"}
2022-01-28 19:55:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.001953125
2022-01-28 19:56:16 | INFO | train_inner | {"epoch": 63, "update": 62.084, "loss": "1.927", "ntokens": "175762", "nsentences": "16.64", "wps": "733279", "ups": "4.17", "wpb": "175762", "bsz": "16.6", "num_updates": "97200", "lr": "0.00431149", "gnorm": "0.442", "loss_scale": "0.002", "train_wall": "24", "gb_free": "20.4", "wall": "25016"}
2022-01-28 19:56:40 | INFO | train_inner | {"epoch": 63, "update": 62.148, "loss": "1.912", "ntokens": "175385", "nsentences": "16.56", "wps": "735991", "ups": "4.2", "wpb": "175385", "bsz": "16.6", "num_updates": "97300", "lr": "0.00431013", "gnorm": "0.438", "loss_scale": "0.002", "train_wall": "24", "gb_free": "20.4", "wall": "25040"}
2022-01-28 19:57:04 | INFO | train_inner | {"epoch": 63, "update": 62.212, "loss": "1.924", "ntokens": "175068", "nsentences": "17.2", "wps": "734592", "ups": "4.2", "wpb": "175068", "bsz": "17.2", "num_updates": "97400", "lr": "0.00430877", "gnorm": "0.456", "loss_scale": "0.002", "train_wall": "24", "gb_free": "20", "wall": "25064"}
2022-01-28 19:57:28 | INFO | train_inner | {"epoch": 63, "update": 62.276, "loss": "1.903", "ntokens": "173231", "nsentences": "16.64", "wps": "724901", "ups": "4.18", "wpb": "173231", "bsz": "16.6", "num_updates": "97500", "lr": "0.00430742", "gnorm": "0.429", "loss_scale": "0.002", "train_wall": "24", "gb_free": "20.9", "wall": "25088"}
2022-01-28 19:57:52 | INFO | train_inner | {"epoch": 63, "update": 62.34, "loss": "1.931", "ntokens": "175418", "nsentences": "16.98", "wps": "732311", "ups": "4.17", "wpb": "175418", "bsz": "17", "num_updates": "97600", "lr": "0.00430606", "gnorm": "0.454", "loss_scale": "0.002", "train_wall": "24", "gb_free": "20.4", "wall": "25112"}
2022-01-28 19:58:16 | INFO | train_inner | {"epoch": 63, "update": 62.404, "loss": "1.936", "ntokens": "174642", "nsentences": "16.88", "wps": "729578", "ups": "4.18", "wpb": "174642", "bsz": "16.9", "num_updates": "97700", "lr": "0.0043047", "gnorm": "0.451", "loss_scale": "0.002", "train_wall": "24", "gb_free": "20.4", "wall": "25136"}
2022-01-28 19:58:40 | INFO | train_inner | {"epoch": 63, "update": 62.467, "loss": "1.92", "ntokens": "176639", "nsentences": "17.12", "wps": "738312", "ups": "4.18", "wpb": "176639", "bsz": "17.1", "num_updates": "97800", "lr": "0.00430334", "gnorm": "0.427", "loss_scale": "0.002", "train_wall": "24", "gb_free": "20.4", "wall": "25159"}
2022-01-28 19:59:04 | INFO | train_inner | {"epoch": 63, "update": 62.531, "loss": "1.925", "ntokens": "176592", "nsentences": "16.8", "wps": "737344", "ups": "4.18", "wpb": "176592", "bsz": "16.8", "num_updates": "97900", "lr": "0.00430198", "gnorm": "0.415", "loss_scale": "0.002", "train_wall": "24", "gb_free": "20.4", "wall": "25183"}
2022-01-28 19:59:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0009765625
2022-01-28 19:59:28 | INFO | train_inner | {"epoch": 63, "update": 62.596, "loss": "1.933", "ntokens": "173406", "nsentences": "16.96", "wps": "720109", "ups": "4.15", "wpb": "173406", "bsz": "17", "num_updates": "98000", "lr": "0.00430061", "gnorm": "0.441", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25208"}
2022-01-28 19:59:52 | INFO | train_inner | {"epoch": 63, "update": 62.66, "loss": "1.938", "ntokens": "175109", "nsentences": "16.24", "wps": "732880", "ups": "4.19", "wpb": "175109", "bsz": "16.2", "num_updates": "98100", "lr": "0.00429925", "gnorm": "0.486", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25231"}
2022-01-28 20:00:16 | INFO | train_inner | {"epoch": 63, "update": 62.723, "loss": "1.931", "ntokens": "175012", "nsentences": "17.2", "wps": "733505", "ups": "4.19", "wpb": "175012", "bsz": "17.2", "num_updates": "98200", "lr": "0.00429788", "gnorm": "0.458", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25255"}
2022-01-28 20:00:39 | INFO | train_inner | {"epoch": 63, "update": 62.787, "loss": "1.925", "ntokens": "175894", "nsentences": "17.12", "wps": "736992", "ups": "4.19", "wpb": "175894", "bsz": "17.1", "num_updates": "98300", "lr": "0.00429652", "gnorm": "0.436", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25279"}
2022-01-28 20:01:03 | INFO | train_inner | {"epoch": 63, "update": 62.851, "loss": "1.938", "ntokens": "175417", "nsentences": "16.4", "wps": "736549", "ups": "4.2", "wpb": "175417", "bsz": "16.4", "num_updates": "98400", "lr": "0.00429515", "gnorm": "0.445", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25303"}
2022-01-28 20:01:27 | INFO | train_inner | {"epoch": 63, "update": 62.915, "loss": "1.937", "ntokens": "175550", "nsentences": "17.04", "wps": "736096", "ups": "4.19", "wpb": "175550", "bsz": "17", "num_updates": "98500", "lr": "0.00429378", "gnorm": "0.437", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25327"}
2022-01-28 20:01:51 | INFO | train_inner | {"epoch": 63, "update": 62.979, "loss": "1.94", "ntokens": "175393", "nsentences": "17.52", "wps": "734099", "ups": "4.19", "wpb": "175393", "bsz": "17.5", "num_updates": "98600", "lr": "0.00429241", "gnorm": "0.44", "loss_scale": "0.001", "train_wall": "24", "gb_free": "21", "wall": "25351"}
2022-01-28 20:01:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:02:04 | INFO | valid | {"epoch": 63, "valid_loss": "2.014", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.51538e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "98633", "valid_best_loss": "2.011"}
2022-01-28 20:02:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 98633 updates
2022-01-28 20:02:04 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 20:02:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt
2022-01-28 20:02:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_last.pt (epoch 63 @ 98633 updates, score 2.014) (writing took 4.280419444665313 seconds)
2022-01-28 20:02:08 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-28 20:02:08 | INFO | train | {"epoch": 63, "train_loss": "1.928", "train_ntokens": "175175", "train_nsentences": "16.8862", "train_wps": "695469", "train_ups": "3.97", "train_wpb": "175175", "train_bsz": "16.9", "train_num_updates": "98633", "train_lr": "0.00429196", "train_gnorm": "0.443", "train_loss_scale": "0.001", "train_train_wall": "372", "train_gb_free": "20.4", "train_wall": "25368"}
2022-01-28 20:02:08 | INFO | fairseq.trainer | begin training epoch 64
2022-01-28 20:02:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:02:35 | INFO | train_inner | {"epoch": 64, "update": 63.043, "loss": "1.925", "ntokens": "174915", "nsentences": "17.28", "wps": "400202", "ups": "2.29", "wpb": "174915", "bsz": "17.3", "num_updates": "98700", "lr": "0.00429104", "gnorm": "0.441", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25394"}
2022-01-28 20:02:58 | INFO | train_inner | {"epoch": 64, "update": 63.107, "loss": "1.92", "ntokens": "175061", "nsentences": "16.96", "wps": "738055", "ups": "4.22", "wpb": "175061", "bsz": "17", "num_updates": "98800", "lr": "0.00428967", "gnorm": "0.443", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25418"}
2022-01-28 20:03:22 | INFO | train_inner | {"epoch": 64, "update": 63.17, "loss": "1.913", "ntokens": "176237", "nsentences": "16.56", "wps": "739370", "ups": "4.2", "wpb": "176237", "bsz": "16.6", "num_updates": "98900", "lr": "0.00428829", "gnorm": "0.44", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25442"}
2022-01-28 20:03:46 | INFO | train_inner | {"epoch": 64, "update": 63.234, "loss": "1.923", "ntokens": "176221", "nsentences": "17.52", "wps": "739810", "ups": "4.2", "wpb": "176221", "bsz": "17.5", "num_updates": "99000", "lr": "0.00428692", "gnorm": "0.442", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25466"}
2022-01-28 20:04:10 | INFO | train_inner | {"epoch": 64, "update": 63.298, "loss": "1.923", "ntokens": "173938", "nsentences": "16.48", "wps": "728899", "ups": "4.19", "wpb": "173938", "bsz": "16.5", "num_updates": "99100", "lr": "0.00428554", "gnorm": "0.435", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.5", "wall": "25490"}
2022-01-28 20:04:34 | INFO | train_inner | {"epoch": 64, "update": 63.362, "loss": "1.92", "ntokens": "175777", "nsentences": "16.32", "wps": "735408", "ups": "4.18", "wpb": "175777", "bsz": "16.3", "num_updates": "99200", "lr": "0.00428417", "gnorm": "0.448", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25514"}
2022-01-28 20:04:58 | INFO | train_inner | {"epoch": 64, "update": 63.426, "loss": "1.926", "ntokens": "176970", "nsentences": "16.8", "wps": "739255", "ups": "4.18", "wpb": "176970", "bsz": "16.8", "num_updates": "99300", "lr": "0.00428279", "gnorm": "0.399", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25538"}
2022-01-28 20:05:22 | INFO | train_inner | {"epoch": 64, "update": 63.49, "loss": "1.929", "ntokens": "175908", "nsentences": "16.48", "wps": "735580", "ups": "4.18", "wpb": "175908", "bsz": "16.5", "num_updates": "99400", "lr": "0.00428141", "gnorm": "0.44", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25562"}
2022-01-28 20:05:46 | INFO | train_inner | {"epoch": 64, "update": 63.554, "loss": "1.928", "ntokens": "174482", "nsentences": "17.28", "wps": "731608", "ups": "4.19", "wpb": "174482", "bsz": "17.3", "num_updates": "99500", "lr": "0.00428003", "gnorm": "0.439", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25585"}
2022-01-28 20:06:09 | INFO | train_inner | {"epoch": 64, "update": 63.617, "loss": "1.924", "ntokens": "173497", "nsentences": "16.72", "wps": "734073", "ups": "4.23", "wpb": "173497", "bsz": "16.7", "num_updates": "99600", "lr": "0.00427865", "gnorm": "0.438", "loss_scale": "0.001", "train_wall": "23", "gb_free": "20.4", "wall": "25609"}
2022-01-28 20:06:33 | INFO | train_inner | {"epoch": 64, "update": 63.681, "loss": "1.92", "ntokens": "175432", "nsentences": "16.48", "wps": "733778", "ups": "4.18", "wpb": "175432", "bsz": "16.5", "num_updates": "99700", "lr": "0.00427727", "gnorm": "0.425", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25633"}
2022-01-28 20:06:57 | INFO | train_inner | {"epoch": 64, "update": 63.745, "loss": "1.909", "ntokens": "175870", "nsentences": "16.88", "wps": "738590", "ups": "4.2", "wpb": "175870", "bsz": "16.9", "num_updates": "99800", "lr": "0.00427589", "gnorm": "0.441", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25657"}
2022-01-28 20:07:21 | INFO | train_inner | {"epoch": 64, "update": 63.809, "loss": "1.935", "ntokens": "175132", "nsentences": "17.76", "wps": "733241", "ups": "4.19", "wpb": "175132", "bsz": "17.8", "num_updates": "99900", "lr": "0.0042745", "gnorm": "0.459", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25681"}
2022-01-28 20:07:45 | INFO | train_inner | {"epoch": 64, "update": 63.873, "loss": "1.933", "ntokens": "171931", "nsentences": "17.22", "wps": "722073", "ups": "4.2", "wpb": "171931", "bsz": "17.2", "num_updates": "100000", "lr": "0.00427312", "gnorm": "0.462", "loss_scale": "0.001", "train_wall": "24", "gb_free": "21.2", "wall": "25705"}
2022-01-28 20:08:09 | INFO | train_inner | {"epoch": 64, "update": 63.937, "loss": "1.933", "ntokens": "175759", "nsentences": "16.64", "wps": "737176", "ups": "4.19", "wpb": "175759", "bsz": "16.6", "num_updates": "100100", "lr": "0.00427173", "gnorm": "0.448", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25728"}
2022-01-28 20:08:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:08:37 | INFO | valid | {"epoch": 64, "valid_loss": "2.005", "valid_ntokens": "162195", "valid_nsentences": "16", "valid_wps": "1.54838e+06", "valid_wpb": "162195", "valid_bsz": "16", "valid_num_updates": "100199", "valid_best_loss": "2.005"}
2022-01-28 20:08:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 100199 updates
2022-01-28 20:08:37 | INFO | fairseq.trainer | Saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 20:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt
2022-01-28 20:08:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bestalex/Documents/fairseq/outputs/2022-01-28/checkpoints/checkpoint_best.pt (epoch 64 @ 100199 updates, score 2.005) (writing took 11.428807927295566 seconds)
2022-01-28 20:08:49 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-28 20:08:49 | INFO | train | {"epoch": 64, "train_loss": "1.924", "train_ntokens": "175182", "train_nsentences": "16.8902", "train_wps": "684754", "train_ups": "3.91", "train_wpb": "175182", "train_bsz": "16.9", "train_num_updates": "100199", "train_lr": "0.00427036", "train_gnorm": "0.439", "train_loss_scale": "0.001", "train_train_wall": "371", "train_gb_free": "20.4", "train_wall": "25768"}
2022-01-28 20:08:49 | INFO | fairseq.trainer | begin training epoch 65
2022-01-28 20:08:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:09:00 | INFO | train_inner | {"epoch": 65, "update": 64.001, "loss": "1.92", "ntokens": "175654", "nsentences": "17.12", "wps": "343533", "ups": "1.96", "wpb": "175654", "bsz": "17.1", "num_updates": "100200", "lr": "0.00427034", "gnorm": "0.431", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25780"}
2022-01-28 20:09:23 | INFO | train_inner | {"epoch": 65, "update": 64.064, "loss": "1.915", "ntokens": "174702", "nsentences": "17.28", "wps": "738310", "ups": "4.23", "wpb": "174702", "bsz": "17.3", "num_updates": "100300", "lr": "0.00426896", "gnorm": "0.441", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25803"}
2022-01-28 20:09:47 | INFO | train_inner | {"epoch": 65, "update": 64.128, "loss": "1.911", "ntokens": "175224", "nsentences": "16.64", "wps": "736253", "ups": "4.2", "wpb": "175224", "bsz": "16.6", "num_updates": "100400", "lr": "0.00426757", "gnorm": "0.412", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25827"}
2022-01-28 20:10:11 | INFO | train_inner | {"epoch": 65, "update": 64.192, "loss": "1.91", "ntokens": "173826", "nsentences": "16.8", "wps": "733062", "ups": "4.22", "wpb": "173826", "bsz": "16.8", "num_updates": "100500", "lr": "0.00426618", "gnorm": "0.468", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25851"}
2022-01-28 20:10:35 | INFO | train_inner | {"epoch": 65, "update": 64.256, "loss": "1.94", "ntokens": "174822", "nsentences": "17.2", "wps": "733062", "ups": "4.19", "wpb": "174822", "bsz": "17.2", "num_updates": "100600", "lr": "0.00426478", "gnorm": "0.466", "loss_scale": "0.001", "train_wall": "24", "gb_free": "20.4", "wall": "25875"}
2022-01-28 20:10:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00048828125
2022-01-28 20:10:59 | INFO | train_inner | {"epoch": 65, "update": 64.321, "loss": "1.906", "ntokens": "175996", "nsentences": "16.4", "wps": "729155", "ups": "4.14", "wpb": "175996", "bsz": "16.4", "num_updates": "100700", "lr": "0.00426339", "gnorm": "0.443", "loss_scale": "0.0005", "train_wall": "24", "gb_free": "20.4", "wall": "25899"}
2022-01-28 20:11:23 | INFO | train_inner | {"epoch": 65, "update": 64.384, "loss": "1.937", "ntokens": "176500", "nsentences": "17.6", "wps": "739883", "ups": "4.19", "wpb": "176500", "bsz": "17.6", "num_updates": "100800", "lr": "0.004262", "gnorm": "0.441", "loss_scale": "0.0005", "train_wall": "24", "gb_free": "20.4", "wall": "25923"}
2022-01-28 20:11:47 | INFO | train_inner | {"epoch": 65, "update": 64.448, "loss": "1.935", "ntokens": "175293", "nsentences": "16.64", "wps": "734203", "ups": "4.19", "wpb": "175293", "bsz": "16.6", "num_updates": "100900", "lr": "0.0042606", "gnorm": "0.413", "loss_scale": "0.0005", "train_wall": "24", "gb_free": "20.4", "wall": "25946"}
2022-01-28 20:12:10 | INFO | train_inner | {"epoch": 65, "update": 64.512, "loss": "1.946", "ntokens": "176006", "nsentences": "17.28", "wps": "739370", "ups": "4.2", "wpb": "176006", "bsz": "17.3", "num_updates": "101000", "lr": "0.00425921", "gnorm": "0.424", "loss_scale": "0.0005", "train_wall": "24", "gb_free": "20.4", "wall": "25970"}
2022-01-28 20:12:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.000244140625
2022-01-28 20:12:35 | INFO | train_inner | {"epoch": 65, "update": 64.577, "loss": "1.962", "ntokens": "175308", "nsentences": "16.56", "wps": "730055", "ups": "4.16", "wpb": "175308", "bsz": "16.6", "num_updates": "101100", "lr": "0.00425781", "gnorm": "0.462", "loss_scale": "0.0002", "train_wall": "24", "gb_free": "20.4", "wall": "25994"}
2022-01-28 20:12:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0001220703125
2022-01-28 20:12:58 | INFO | train_inner | {"epoch": 65, "update": 64.641, "loss": "1.968", "ntokens": "174823", "nsentences": "17.28", "wps": "729166", "ups": "4.17", "wpb": "174823", "bsz": "17.3", "num_updates": "101200", "lr": "0.00425641", "gnorm": "0.489", "loss_scale": "0.0001", "train_wall": "24", "gb_free": "20.4", "wall": "26018"}
2022-01-28 20:13:22 | INFO | train_inner | {"epoch": 65, "update": 64.705, "loss": "1.958", "ntokens": "175411", "nsentences": "16.96", "wps": "738304", "ups": "4.21", "wpb": "175411", "bsz": "17", "num_updates": "101300", "lr": "0.00425501", "gnorm": "0.45", "loss_scale": "0.0001", "train_wall": "24", "gb_free": "20.4", "wall": "26042"}
2022-01-28 20:13:38 | WARNING | fairseq.nan_detector | Inf detected in output of feature_extractor.conv_layers.1.0, shape: torch.Size([8, 512, 7498]), forward input max: 769.0, input min: 0.0
2022-01-28 20:13:38 | WARNING | fairseq.nan_detector | NaN detected in output of wav2vec_predictions, shape: torch.Size([978384]), backward
2022-01-28 20:13:38 | INFO | fairseq.nan_detector | Detected nan/inf grad norm, dumping norms...
2022-01-28 20:13:38 | INFO | fairseq.nan_detector | norms: {'feature_extractor.conv_layers.0.0.weight': nan, 'feature_extractor.conv_layers.0.2.weight': nan, 'feature_extractor.conv_layers.0.2.bias': inf, 'feature_extractor.conv_layers.1.0.weight': nan, 'feature_extractor.conv_layers.1.2.weight': nan, 'feature_extractor.conv_layers.1.2.bias': inf, 'feature_extractor.conv_layers.2.0.weight': nan, 'feature_extractor.conv_layers.2.2.weight': nan, 'feature_extractor.conv_layers.2.2.bias': inf, 'feature_extractor.conv_layers.3.0.weight': nan, 'feature_extractor.conv_layers.3.2.weight': nan, 'feature_extractor.conv_layers.3.2.bias': inf, 'feature_extractor.conv_layers.4.0.weight': nan, 'feature_extractor.conv_layers.4.2.weight': nan, 'feature_extractor.conv_layers.4.2.bias': nan, 'feature_extractor.conv_layers.5.0.weight': nan, 'feature_extractor.conv_layers.5.2.weight': nan, 'feature_extractor.conv_layers.5.2.bias': nan, 'feature_extractor.conv_layers.6.0.weight': nan, 'feature_extractor.conv_layers.6.2.weight': nan, 'feature_extractor.conv_layers.6.2.bias': nan, 'feature_aggregator.conv_layers.0.1.weight': nan, 'feature_aggregator.conv_layers.0.1.bias': nan, 'feature_aggregator.conv_layers.0.3.weight': nan, 'feature_aggregator.conv_layers.0.3.bias': nan, 'feature_aggregator.conv_layers.1.1.weight': nan, 'feature_aggregator.conv_layers.1.1.bias': nan, 'feature_aggregator.conv_layers.1.3.weight': nan, 'feature_aggregator.conv_layers.1.3.bias': nan, 'feature_aggregator.conv_layers.2.1.weight': nan, 'feature_aggregator.conv_layers.2.1.bias': nan, 'feature_aggregator.conv_layers.2.3.weight': nan, 'feature_aggregator.conv_layers.2.3.bias': nan, 'feature_aggregator.conv_layers.3.1.weight': nan, 'feature_aggregator.conv_layers.3.1.bias': nan, 'feature_aggregator.conv_layers.3.3.weight': nan, 'feature_aggregator.conv_layers.3.3.bias': nan, 'feature_aggregator.conv_layers.4.1.weight': nan, 'feature_aggregator.conv_layers.4.1.bias': nan, 'feature_aggregator.conv_layers.4.3.weight': nan, 'feature_aggregator.conv_layers.4.3.bias': nan, 'feature_aggregator.conv_layers.5.1.weight': nan, 'feature_aggregator.conv_layers.5.1.bias': nan, 'feature_aggregator.conv_layers.5.3.weight': nan, 'feature_aggregator.conv_layers.5.3.bias': nan, 'feature_aggregator.conv_layers.6.1.weight': nan, 'feature_aggregator.conv_layers.6.1.bias': nan, 'feature_aggregator.conv_layers.6.3.weight': nan, 'feature_aggregator.conv_layers.6.3.bias': nan, 'feature_aggregator.conv_layers.7.1.weight': nan, 'feature_aggregator.conv_layers.7.1.bias': nan, 'feature_aggregator.conv_layers.7.3.weight': nan, 'feature_aggregator.conv_layers.7.3.bias': nan, 'feature_aggregator.conv_layers.8.1.weight': nan, 'feature_aggregator.conv_layers.8.1.bias': nan, 'feature_aggregator.conv_layers.8.3.weight': nan, 'feature_aggregator.conv_layers.8.3.bias': nan, 'feature_aggregator.conv_layers.9.1.weight': nan, 'feature_aggregator.conv_layers.9.1.bias': nan, 'feature_aggregator.conv_layers.9.3.weight': nan, 'feature_aggregator.conv_layers.9.3.bias': nan, 'feature_aggregator.conv_layers.10.1.weight': nan, 'feature_aggregator.conv_layers.10.1.bias': nan, 'feature_aggregator.conv_layers.10.3.weight': nan, 'feature_aggregator.conv_layers.10.3.bias': nan, 'feature_aggregator.conv_layers.11.1.weight': nan, 'feature_aggregator.conv_layers.11.1.bias': nan, 'feature_aggregator.conv_layers.11.3.weight': nan, 'feature_aggregator.conv_layers.11.3.bias': nan, 'wav2vec_predictions.project_to_steps.weight': nan, 'wav2vec_predictions.project_to_steps.bias': nan}
2022-01-28 20:13:38 | INFO | fairseq.nan_detector | gradients: {'feature_extractor.conv_layers.0.0.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.0.2.weight': tensor([nan, 0., 0., nan, 0., -inf, 0., 0., -inf, 0., -inf, -inf, 0., 0., 0., -inf, 0., 0., -inf, -inf, -inf, 0., 0., -inf,
        -inf, -inf, 0., nan, 0., 0., 0., 0., -inf, nan, 0., 0., -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf, 0., 0., 0.,
        -inf, 0., 0., 0., -inf, -inf, 0., -inf, 0., 0., -inf, 0., 0., nan, 0., -inf, 0., 0., 0., -inf, 0., 0., nan, -inf,
        -inf, 0., 0., 0., -inf, -inf, 0., nan, 0., 0., nan, -inf, 0., nan, nan, nan, 0., -inf, 0., 0., -inf, 0., 0., -inf,
        nan, nan, 0., nan, 0., 0., 0., -inf, 0., nan, 0., nan, 0., -inf, 0., 0., 0., -inf, 0., -inf, -inf, -inf, 0., 0.,
        0., -inf, 0., 0., 0., 0., -inf, -inf, nan, 0., -inf, -inf, 0., -inf, 0., -inf, 0., 0., -inf, -inf, -inf, 0., 0., 0.,
        0., 0., -inf, 0., 0., nan, nan, 0., 0., nan, -inf, 0., -inf, 0., 0., -inf, -inf, nan, 0., 0., nan, nan, -inf, 0.,
        0., 0., 0., nan, 0., 0., 0., 0., 0., nan, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, 0., nan, 0., -inf, -inf, -inf,
        0., nan, -inf, nan, 0., -inf, 0., -inf, -inf, -inf, 0., -inf, 0., 0., -inf, -inf, -inf, 0., -inf, -inf, nan, -inf, 0., 0.,
        -inf, 0., -inf, 0., -inf, 0., -inf, 0., 0., 0., nan, 0., -inf, 0., nan, 0., -inf, 0., 0., 0., -inf, 0., -inf, -inf,
        nan, 0., nan, 0., -inf, -inf, nan, 0., nan, 0., 0., 0., -inf, -inf, 0., -inf, 0., -inf, nan, -inf, -inf, -inf, 0., 0.,
        -inf, 0., 0., 0., -inf, -inf, 0., -inf, 0., 0., 0., -inf, 0., -inf, 0., nan, 0., -inf, -inf, -inf, -inf, 0., 0., -inf,
        nan, nan, nan, -inf, -inf, nan, 0., 0., 0., -inf, 0., -inf, nan, 0., 0., 0., -inf, -inf, 0., -inf, 0., 0., -inf, nan,
        0., 0., 0., 0., 0., 0., 0., 0., -inf, 0., 0., -inf, 0., -inf, -inf, 0., nan, -inf, -inf, -inf, 0., -inf, 0., -inf,
        0., 0., 0., 0., -inf, -inf, 0., nan, 0., 0., 0., 0., 0., 0., -inf, 0., -inf, -inf, nan, 0., -inf, 0., 0., -inf,
        0., -inf, 0., -inf, 0., 0., -inf, -inf, -inf, 0., 0., 0., 0., 0., -inf, -inf, -inf, 0., -inf, 0., -inf, -inf, -inf, -inf,
        0., 0., -inf, 0., 0., 0., 0., 0., 0., 0., 0., -inf, 0., 0., 0., 0., -inf, -inf, 0., -inf, -inf, -inf, 0., 0.,
        0., -inf, 0., -inf, -inf, -inf, 0., 0., -inf, 0., 0., -inf, 0., 0., -inf, 0., -inf, 0., 0., 0., nan, 0., 0., 0.,
        -inf, nan, 0., 0., -inf, 0., nan, nan, -inf, -inf, -inf, 0., nan, 0., 0., -inf, -inf, 0., 0., 0., 0., 0., -inf, 0.,
        -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, 0., nan, nan, -inf, 0., 0., -inf, 0., -inf, 0., 0., -inf, 0.,
        0., nan, -inf, 0., 0., -inf, 0., 0., 0., 0., 0., 0., 0., -inf, -inf, 0., -inf, -inf, 0., 0., 0., 0., nan, 0.,
        0., nan, -inf, 0., nan, 0., 0., -inf], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.0.2.bias': tensor([-inf, 0., 0., -inf, 0., -inf, 0., 0., -inf, 0., -inf, -inf, 0., 0., 0., -inf, 0., 0., -inf, -inf, -inf, 0., 0., -inf,
        -inf, -inf, 0., -inf, 0., 0., 0., 0., -inf, -inf, 0., 0., -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf, 0., 0., 0.,
        -inf, 0., 0., 0., -inf, -inf, 0., -inf, 0., 0., -inf, 0., 0., -inf, 0., -inf, 0., 0., 0., -inf, 0., 0., -inf, -inf,
        -inf, 0., 0., 0., -inf, -inf, 0., -inf, 0., 0., -inf, -inf, 0., -inf, -inf, -inf, 0., -inf, 0., 0., -inf, 0., 0., -inf,
        -inf, -inf, 0., -inf, 0., 0., 0., -inf, 0., -inf, 0., -inf, 0., -inf, 0., 0., 0., -inf, 0., -inf, -inf, -inf, 0., 0.,
        0., -inf, 0., 0., 0., 0., -inf, -inf, -inf, 0., -inf, -inf, 0., -inf, 0., -inf, 0., 0., -inf, -inf, -inf, 0., 0., 0.,
        0., 0., -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., -inf, 0., 0., -inf, -inf, -inf, 0., 0., -inf, -inf, -inf, 0.,
        0., 0., 0., -inf, 0., 0., 0., 0., 0., -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, 0., -inf, -inf, -inf,
        0., -inf, -inf, -inf, 0., -inf, 0., -inf, -inf, -inf, 0., -inf, 0., 0., -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, 0., 0.,
        -inf, 0., -inf, 0., -inf, 0., -inf, 0., 0., 0., -inf, 0., -inf, 0., -inf, 0., -inf, 0., 0., 0., -inf, 0., -inf, -inf,
        -inf, 0., -inf, 0., -inf, -inf, -inf, 0., -inf, 0., 0., 0., -inf, -inf, 0., -inf, 0., -inf, -inf, -inf, -inf, -inf, 0., 0.,
        -inf, 0., 0., 0., -inf, -inf, 0., -inf, 0., 0., 0., -inf, 0., -inf, 0., -inf, 0., -inf, -inf, -inf, -inf, 0., 0., -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf, 0., -inf, -inf, 0., 0., 0., -inf, -inf, 0., -inf, 0., 0., -inf, -inf,
        0., 0., 0., 0., 0., 0., 0., 0., -inf, 0., 0., -inf, 0., -inf, -inf, 0., -inf, -inf, -inf, -inf, 0., -inf, 0., -inf,
        0., 0., 0., 0., -inf, -inf, 0., -inf, 0., 0., 0., 0., 0., 0., -inf, 0., -inf, -inf, -inf, 0., -inf, 0., 0., -inf,
        0., -inf, 0., -inf, 0., 0., -inf, -inf, -inf, 0., 0., 0., 0., 0., -inf, -inf, -inf, 0., -inf, 0., -inf, -inf, -inf, -inf,
        0., 0., -inf, 0., 0., 0., 0., 0., 0., 0., 0., -inf, 0., 0., 0., 0., -inf, -inf, 0., -inf, -inf, -inf, 0., 0.,
        0., -inf, 0., -inf, -inf, -inf, 0., 0., -inf, 0., 0., -inf, 0., 0., -inf, 0., -inf, 0., 0., 0., -inf, 0., 0., 0.,
        -inf, -inf, 0., 0., -inf, 0., -inf, -inf, -inf, -inf, -inf, 0., -inf, 0., 0., -inf, -inf, 0., 0., 0., 0., 0., -inf, 0.,
        -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, 0., -inf, -inf, -inf, 0., 0., -inf, 0., -inf, 0., 0., -inf, 0.,
        0., -inf, -inf, 0., 0., -inf, 0., 0., 0., 0., 0., 0., 0., -inf, -inf, 0., -inf, -inf, 0., 0., 0., 0., -inf, 0.,
        0., -inf, -inf, 0., -inf, 0., 0., -inf], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.1.0.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.1.2.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.1.2.bias': tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.2.0.weight': tensor([[[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        ...,

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]]], device='cuda:0', dtype=torch.float16), 'feature_extractor.conv_layers.2.2.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.2.2.bias': tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.3.0.weight': tensor([[[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        ...,

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]]], device='cuda:0', dtype=torch.float16), 'feature_extractor.conv_layers.3.2.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.3.2.bias': tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.4.0.weight': tensor([[[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        ...,

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]]], device='cuda:0', dtype=torch.float16), 'feature_extractor.conv_layers.4.2.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.4.2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.5.0.weight': tensor([[[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        ...,

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]]], device='cuda:0', dtype=torch.float16), 'feature_extractor.conv_layers.5.2.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.5.2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.6.0.weight': tensor([[[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        ...,

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]],

        [[nan],
         [nan],
         [nan],
         ...,
         [nan],
         [nan],
         [nan]]], device='cuda:0', dtype=torch.float16), 'feature_extractor.conv_layers.6.2.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_extractor.conv_layers.6.2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.0.1.weight': tensor([[[nan, nan],
         [nan, nan],
         [nan, nan],
         ...,
         [nan, nan],
         [nan, nan],
         [nan, nan]],

        [[nan, nan],
         [nan, nan],
         [nan, nan],
         ...,
         [nan, nan],
         [nan, nan],
         [nan, nan]],

        [[nan, nan],
         [nan, nan],
         [nan, nan],
         ...,
         [nan, nan],
         [nan, nan],
         [nan, nan]],

        ...,

        [[nan, nan],
         [nan, nan],
         [nan, nan],
         ...,
         [nan, nan],
         [nan, nan],
         [nan, nan]],

        [[nan, nan],
         [nan, nan],
         [nan, nan],
         ...,
         [nan, nan],
         [nan, nan],
         [nan, nan]],

        [[nan, nan],
         [nan, nan],
         [nan, nan],
         ...,
         [nan, nan],
         [nan, nan],
         [nan, nan]]], device='cuda:0', dtype=torch.float16), 'feature_aggregator.conv_layers.0.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.0.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.0.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.1.1.weight': tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]],

        [[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]],

        [[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]],

        ...,

        [[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]],

        [[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]],

        [[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.float16), 'feature_aggregator.conv_layers.1.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.1.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.1.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.2.1.weight': tensor([[[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        ...,

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]],

        [[nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan],
         [nan, nan, nan, nan],
         [nan, nan, nan, nan]]], device='cuda:0', dtype=torch.float16), 'feature_aggregator.conv_layers.2.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.2.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.2.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.3.1.weight': tensor([[[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]],

        [[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]],

        [[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]],

        ...,

        [[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]],

        [[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]],

        [[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]]], device='cuda:0', dtype=torch.float16), 'feature_aggregator.conv_layers.3.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.3.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.3.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.4.1.weight': tensor([[[nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan]],

        [[nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan]],

        [[nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan]],

        ...,

        [[nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan]],

        [[nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan]],

        [[nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         ...,
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.4.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.4.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.4.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.5.1.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.5.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.5.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.5.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.6.1.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.6.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.6.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.6.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.7.1.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.7.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.7.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.7.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.8.1.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.8.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.8.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.8.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.9.1.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.9.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.9.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.9.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.10.1.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.10.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.10.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.10.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.11.1.weight': tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.11.1.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.11.3.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'feature_aggregator.conv_layers.11.3.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16), 'wav2vec_predictions.project_to_steps.weight': tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]],


        [[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]],


        [[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]],


        ...,


        [[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]],


        [[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]],


        [[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.float16), 'wav2vec_predictions.project_to_steps.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16)}
/home/bestalex/Documents/fairseq/fairseqenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/home/bestalex/Documents/fairseq/fairseqenv/lib/python3.8/site-packages/torch/nn/modules/module.py:990: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/home/bestalex/Documents/fairseq/fairseqenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/home/bestalex/Documents/fairseq/fairseqenv/lib/python3.8/site-packages/torch/nn/modules/module.py:990: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/bestalex/Documents/fairseq/fairseq_cli/train.py", line 507, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/bestalex/Documents/fairseq/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/bestalex/Documents/fairseq/fairseqenv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/bestalex/Documents/fairseq/fairseqenv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/home/bestalex/Documents/fairseq/fairseqenv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/bestalex/Documents/fairseq/fairseqenv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/home/bestalex/Documents/fairseq/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/home/bestalex/Documents/fairseq/fairseq_cli/train.py", line 180, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/pkg/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/bestalex/Documents/fairseq/fairseq_cli/train.py", line 291, in train
    log_output = trainer.train_step(samples)
  File "/usr/pkg/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/bestalex/Documents/fairseq/fairseq/trainer.py", line 855, in train_step
    grad_norm = self.clip_grad_norm(self.cfg.optimization.clip_norm)
  File "/home/bestalex/Documents/fairseq/fairseq/trainer.py", line 1185, in clip_grad_norm
    return self.optimizer.clip_grad_norm(
  File "/home/bestalex/Documents/fairseq/fairseq/optim/fp16_optimizer.py", line 200, in clip_grad_norm
    self.scaler.check_overflow(grad_norm)
  File "/home/bestalex/Documents/fairseq/fairseq/optim/dynamic_loss_scaler.py", line 61, in check_overflow
    raise FloatingPointError(
FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.

/usr/pkg/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1548 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
