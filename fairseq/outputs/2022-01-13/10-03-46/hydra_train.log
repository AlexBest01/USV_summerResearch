[2022-01-13 10:03:46,904][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': 'tb_logs', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1400000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1400000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 288, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 96, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.1, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'checkpoint_activations': False}, 'task': {'_name': 'audio_pretraining', 'data': '/local/scratch/bestalex/cut_10s_mixed_usv/usv_pairs_train', 'labels': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 250000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': 'none'}, 'criterion': {'_name': 'wav2vec', 'infonce': True, 'loss_weights': [0.1, 10.0], 'log_keys': ['prob_perplexity', 'code_perplexity', 'temp']}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-01-13 10:03:47,353][fairseq_cli.train][INFO] - Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
        (3): GELU()
      )
      (1): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (2): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (3): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (4): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (5): Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (6): Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=288, bias=True)
  (dropout_input): Dropout(p=0.1, inplace=False)
  (dropout_features): Dropout(p=0.1, inplace=False)
  (quantizer): GumbelVectorQuantizer(
    (weight_proj): Linear(in_features=512, out_features=640, bias=True)
  )
  (project_q): Linear(in_features=96, out_features=96, bias=True)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(288, 288, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU()
    )
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): Linear(in_features=288, out_features=96, bias=True)
)
[2022-01-13 10:03:47,356][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2022-01-13 10:03:47,356][fairseq_cli.train][INFO] - model: Wav2Vec2Model
[2022-01-13 10:03:47,356][fairseq_cli.train][INFO] - criterion: Wav2vecCriterion
[2022-01-13 10:03:47,357][fairseq_cli.train][INFO] - num. shared model params: 30,693,088 (num. trained: 30,693,088)
[2022-01-13 10:03:47,358][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-01-13 10:03:47,361][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 9, skipped 0 samples
[2022-01-13 10:03:52,382][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias
[2022-01-13 10:03:52,382][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias
[2022-01-13 10:03:52,382][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias
[2022-01-13 10:03:52,382][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias
[2022-01-13 10:03:52,382][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias
[2022-01-13 10:03:52,382][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias
[2022-01-13 10:03:52,383][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-01-13 10:03:52,383][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA RTX A5000                        
[2022-01-13 10:03:52,383][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-01-13 10:03:52,383][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2022-01-13 10:03:52,383][fairseq_cli.train][INFO] - max tokens per device = 1400000 and max sentences per device = None
[2022-01-13 10:03:52,385][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2022-01-13 10:03:52,385][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2022-01-13 10:03:52,385][fairseq.trainer][INFO] - loading train data for epoch 1
[2022-01-13 10:03:52,387][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 607, skipped 0 samples
[2022-01-13 10:03:52,571][fairseq.trainer][INFO] - NOTE: your device may support faster training with --fp16 or --amp
[2022-01-13 10:03:52,585][fairseq.trainer][INFO] - begin training epoch 1
[2022-01-13 10:03:52,586][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:04:20,471][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:04:20,917][valid][INFO] - {"epoch": 1, "valid_loss": "7.603", "valid_ntokens": "1676.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "225.537", "valid_code_perplexity": "221.504", "valid_temp": "1.999", "valid_loss_0": "6.679", "valid_loss_1": "0.094", "valid_loss_2": "0.831", "valid_accuracy": "0.01312", "valid_wps": "28433.6", "valid_wpb": "1676.5", "valid_bsz": "4.5", "valid_num_updates": "122"}
[2022-01-13 10:04:20,919][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 122 updates
[2022-01-13 10:04:20,920][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:04:24,831][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:04:31,431][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 122 updates, score 7.603) (writing took 10.51146837696433 seconds)
[2022-01-13 10:04:31,432][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2022-01-13 10:04:31,449][train][INFO] - {"epoch": 1, "train_loss": "8.509", "train_ntokens": "1793.66", "train_nsentences": "4.97541", "train_prob_perplexity": "189.532", "train_code_perplexity": "186.079", "train_temp": "1.999", "train_loss_0": "6.743", "train_loss_1": "0.102", "train_loss_2": "1.665", "train_accuracy": "0.01182", "train_wps": "5718.5", "train_ups": "3.19", "train_wpb": "1793.7", "train_bsz": "5", "train_num_updates": "122", "train_lr": "1.90625e-06", "train_gnorm": "1.597", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "39"}
[2022-01-13 10:04:31,523][fairseq.trainer][INFO] - begin training epoch 2
[2022-01-13 10:04:31,524][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:04:49,343][train_inner][INFO] - {"epoch": 2, "update": 1.639, "loss": "8.043", "ntokens": "1797.38", "nsentences": "4.985", "prob_perplexity": "234.416", "code_perplexity": "230.278", "temp": "1.999", "loss_0": "6.721", "loss_1": "0.091", "loss_2": "1.231", "accuracy": "0.01167", "wps": "6404.7", "ups": "3.56", "wpb": "1797.4", "bsz": "5", "num_updates": "200", "lr": "3.125e-06", "gnorm": "1.213", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "57"}
[2022-01-13 10:04:59,193][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:04:59,631][valid][INFO] - {"epoch": 2, "valid_loss": "6.926", "valid_ntokens": "1645", "valid_nsentences": "4.5", "valid_prob_perplexity": "391.137", "valid_code_perplexity": "382.471", "valid_temp": "1.998", "valid_loss_0": "6.665", "valid_loss_1": "0.056", "valid_loss_2": "0.204", "valid_accuracy": "0.01064", "valid_wps": "29244.1", "valid_wpb": "1645", "valid_bsz": "4.5", "valid_num_updates": "244", "valid_best_loss": "6.926"}
[2022-01-13 10:04:59,633][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 244 updates
[2022-01-13 10:04:59,633][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:05:03,516][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:05:10,163][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 244 updates, score 6.926) (writing took 10.530130113475025 seconds)
[2022-01-13 10:05:10,164][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-01-13 10:05:10,176][train][INFO] - {"epoch": 2, "train_loss": "7.205", "train_ntokens": "1797.7", "train_nsentences": "4.97541", "train_prob_perplexity": "336.126", "train_code_perplexity": "330.489", "train_temp": "1.998", "train_loss_0": "6.681", "train_loss_1": "0.068", "train_loss_2": "0.455", "train_accuracy": "0.01164", "train_wps": "5664.9", "train_ups": "3.15", "train_wpb": "1797.7", "train_bsz": "5", "train_num_updates": "244", "train_lr": "3.8125e-06", "train_gnorm": "0.527", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "78"}
[2022-01-13 10:05:10,267][fairseq.trainer][INFO] - begin training epoch 3
[2022-01-13 10:05:10,268][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:05:37,799][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:05:38,238][valid][INFO] - {"epoch": 3, "valid_loss": "6.767", "valid_ntokens": "1629.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "522.944", "valid_code_perplexity": "514.963", "valid_temp": "1.996", "valid_loss_0": "6.653", "valid_loss_1": "0.026", "valid_loss_2": "0.087", "valid_accuracy": "0.01258", "valid_wps": "28997.8", "valid_wpb": "1629.5", "valid_bsz": "4.5", "valid_num_updates": "366", "valid_best_loss": "6.767"}
[2022-01-13 10:05:38,240][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 366 updates
[2022-01-13 10:05:38,241][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:05:42,128][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:05:49,006][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 366 updates, score 6.767) (writing took 10.765480436384678 seconds)
[2022-01-13 10:05:49,006][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2022-01-13 10:05:49,022][train][INFO] - {"epoch": 3, "train_loss": "6.842", "train_ntokens": "1806.61", "train_nsentences": "4.97541", "train_prob_perplexity": "480.102", "train_code_perplexity": "472.805", "train_temp": "1.997", "train_loss_0": "6.663", "train_loss_1": "0.036", "train_loss_2": "0.143", "train_accuracy": "0.01217", "train_wps": "5676.2", "train_ups": "3.14", "train_wpb": "1806.6", "train_bsz": "5", "train_num_updates": "366", "train_lr": "5.71875e-06", "train_gnorm": "0.275", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "117"}
[2022-01-13 10:05:49,097][fairseq.trainer][INFO] - begin training epoch 4
[2022-01-13 10:05:49,098][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:05:57,055][train_inner][INFO] - {"epoch": 4, "update": 3.279, "loss": "6.863", "ntokens": "1803.88", "nsentences": "4.97", "prob_perplexity": "469.39", "code_perplexity": "462.264", "temp": "1.997", "loss_0": "6.664", "loss_1": "0.038", "loss_2": "0.161", "accuracy": "0.0122", "wps": "5329.3", "ups": "2.95", "wpb": "1803.9", "bsz": "5", "num_updates": "400", "lr": "6.25e-06", "gnorm": "0.289", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "125"}
[2022-01-13 10:06:16,630][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:06:17,104][valid][INFO] - {"epoch": 4, "valid_loss": "6.466", "valid_ntokens": "1622.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "432.665", "valid_code_perplexity": "423.105", "valid_temp": "1.995", "valid_loss_0": "6.354", "valid_loss_1": "0.047", "valid_loss_2": "0.065", "valid_accuracy": "0.02835", "valid_wps": "29538.5", "valid_wpb": "1622.5", "valid_bsz": "4.5", "valid_num_updates": "488", "valid_best_loss": "6.466"}
[2022-01-13 10:06:17,106][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 488 updates
[2022-01-13 10:06:17,106][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:06:20,993][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:06:27,572][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 488 updates, score 6.466) (writing took 10.46646388899535 seconds)
[2022-01-13 10:06:27,573][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2022-01-13 10:06:27,586][train][INFO] - {"epoch": 4, "train_loss": "6.724", "train_ntokens": "1802.33", "train_nsentences": "4.97541", "train_prob_perplexity": "530.182", "train_code_perplexity": "522.497", "train_temp": "1.996", "train_loss_0": "6.627", "train_loss_1": "0.025", "train_loss_2": "0.071", "train_accuracy": "0.01602", "train_wps": "5703.8", "train_ups": "3.16", "train_wpb": "1802.3", "train_bsz": "5", "train_num_updates": "488", "train_lr": "7.625e-06", "train_gnorm": "0.317", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "155"}
[2022-01-13 10:06:27,669][fairseq.trainer][INFO] - begin training epoch 5
[2022-01-13 10:06:27,669][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:06:53,478][train_inner][INFO] - {"epoch": 5, "update": 4.918, "loss": "6.108", "ntokens": "1796.27", "nsentences": "4.97", "prob_perplexity": "300.273", "code_perplexity": "295.631", "temp": "1.995", "loss_0": "5.95", "loss_1": "0.076", "loss_2": "0.082", "accuracy": "0.09165", "wps": "6368.5", "ups": "3.55", "wpb": "1796.3", "bsz": "5", "num_updates": "600", "lr": "9.375e-06", "gnorm": "0.563", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "181"}
[2022-01-13 10:06:55,765][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:06:56,250][valid][INFO] - {"epoch": 5, "valid_loss": "4.756", "valid_ntokens": "1634", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.163", "valid_code_perplexity": "18.062", "valid_temp": "1.994", "valid_loss_0": "4.526", "valid_loss_1": "0.14", "valid_loss_2": "0.09", "valid_accuracy": "0.42105", "valid_wps": "27393.2", "valid_wpb": "1634", "valid_bsz": "4.5", "valid_num_updates": "610", "valid_best_loss": "4.756"}
[2022-01-13 10:06:56,252][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 610 updates
[2022-01-13 10:06:56,252][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:07:00,583][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:07:07,483][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 610 updates, score 4.756) (writing took 11.23085743188858 seconds)
[2022-01-13 10:07:07,483][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2022-01-13 10:07:07,497][train][INFO] - {"epoch": 5, "train_loss": "5.584", "train_ntokens": "1795.48", "train_nsentences": "4.97541", "train_prob_perplexity": "111.924", "train_code_perplexity": "109.898", "train_temp": "1.995", "train_loss_0": "5.373", "train_loss_1": "0.119", "train_loss_2": "0.093", "train_accuracy": "0.16416", "train_wps": "5490.3", "train_ups": "3.06", "train_wpb": "1795.5", "train_bsz": "5", "train_num_updates": "610", "train_lr": "9.53125e-06", "train_gnorm": "0.723", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "195"}
[2022-01-13 10:07:07,567][fairseq.trainer][INFO] - begin training epoch 6
[2022-01-13 10:07:07,568][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:07:35,276][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:07:35,744][valid][INFO] - {"epoch": 6, "valid_loss": "4.576", "valid_ntokens": "1613.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "14.557", "valid_code_perplexity": "14.428", "valid_temp": "1.993", "valid_loss_0": "4.367", "valid_loss_1": "0.141", "valid_loss_2": "0.068", "valid_accuracy": "0.42826", "valid_wps": "28294.6", "valid_wpb": "1613.5", "valid_bsz": "4.5", "valid_num_updates": "732", "valid_best_loss": "4.576"}
[2022-01-13 10:07:35,746][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 732 updates
[2022-01-13 10:07:35,747][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:07:39,639][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:07:46,286][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 732 updates, score 4.576) (writing took 10.53924217261374 seconds)
[2022-01-13 10:07:46,287][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2022-01-13 10:07:46,301][train][INFO] - {"epoch": 6, "train_loss": "4.762", "train_ntokens": "1805", "train_nsentences": "4.97541", "train_prob_perplexity": "18.195", "train_code_perplexity": "18.02", "train_temp": "1.993", "train_loss_0": "4.545", "train_loss_1": "0.14", "train_loss_2": "0.077", "train_accuracy": "0.39243", "train_wps": "5677.1", "train_ups": "3.15", "train_wpb": "1805", "train_bsz": "5", "train_num_updates": "732", "train_lr": "1.14375e-05", "train_gnorm": "0.723", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "234"}
[2022-01-13 10:07:46,353][fairseq.trainer][INFO] - begin training epoch 7
[2022-01-13 10:07:46,354][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:08:01,919][train_inner][INFO] - {"epoch": 7, "update": 6.557, "loss": "4.708", "ntokens": "1800.34", "nsentences": "4.97", "prob_perplexity": "17.843", "code_perplexity": "17.683", "temp": "1.993", "loss_0": "4.493", "loss_1": "0.14", "loss_2": "0.075", "accuracy": "0.40241", "wps": "5262.1", "ups": "2.92", "wpb": "1800.3", "bsz": "5", "num_updates": "800", "lr": "1.25e-05", "gnorm": "0.853", "clip": "0", "train_wall": "44", "gb_free": "13.5", "wall": "250"}
[2022-01-13 10:08:14,093][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:08:14,529][valid][INFO] - {"epoch": 7, "valid_loss": "4.313", "valid_ntokens": "1633.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "14.738", "valid_code_perplexity": "14.633", "valid_temp": "1.991", "valid_loss_0": "4.102", "valid_loss_1": "0.141", "valid_loss_2": "0.07", "valid_accuracy": "0.49005", "valid_wps": "29134.7", "valid_wpb": "1633.5", "valid_bsz": "4.5", "valid_num_updates": "854", "valid_best_loss": "4.313"}
[2022-01-13 10:08:14,531][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 854 updates
[2022-01-13 10:08:14,531][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:08:18,397][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:08:25,438][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 854 updates, score 4.313) (writing took 10.906638950109482 seconds)
[2022-01-13 10:08:25,439][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2022-01-13 10:08:25,452][train][INFO] - {"epoch": 7, "train_loss": "4.509", "train_ntokens": "1800.59", "train_nsentences": "4.97541", "train_prob_perplexity": "16.321", "train_code_perplexity": "16.218", "train_temp": "1.992", "train_loss_0": "4.301", "train_loss_1": "0.141", "train_loss_2": "0.068", "train_accuracy": "0.44245", "train_wps": "5612.8", "train_ups": "3.12", "train_wpb": "1800.6", "train_bsz": "5", "train_num_updates": "854", "train_lr": "1.33438e-05", "train_gnorm": "1.411", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "273"}
[2022-01-13 10:08:25,530][fairseq.trainer][INFO] - begin training epoch 8
[2022-01-13 10:08:25,531][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:08:52,931][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:08:53,409][valid][INFO] - {"epoch": 8, "valid_loss": "4.125", "valid_ntokens": "1635.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "14.759", "valid_code_perplexity": "14.707", "valid_temp": "1.99", "valid_loss_0": "3.926", "valid_loss_1": "0.141", "valid_loss_2": "0.058", "valid_accuracy": "0.50718", "valid_wps": "28294.8", "valid_wpb": "1635.5", "valid_bsz": "4.5", "valid_num_updates": "976", "valid_best_loss": "4.125"}
[2022-01-13 10:08:53,411][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 976 updates
[2022-01-13 10:08:53,412][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:08:57,297][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:09:03,885][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 976 updates, score 4.125) (writing took 10.473242867738008 seconds)
[2022-01-13 10:09:03,885][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2022-01-13 10:09:03,899][train][INFO] - {"epoch": 8, "train_loss": "4.343", "train_ntokens": "1802.02", "train_nsentences": "4.97541", "train_prob_perplexity": "15.138", "train_code_perplexity": "15.066", "train_temp": "1.991", "train_loss_0": "4.139", "train_loss_1": "0.141", "train_loss_2": "0.062", "train_accuracy": "0.47129", "train_wps": "5720.3", "train_ups": "3.17", "train_wpb": "1802", "train_bsz": "5", "train_num_updates": "976", "train_lr": "1.525e-05", "train_gnorm": "1.799", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "312"}
[2022-01-13 10:09:03,994][fairseq.trainer][INFO] - begin training epoch 9
[2022-01-13 10:09:03,995][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:09:09,716][train_inner][INFO] - {"epoch": 9, "update": 8.197, "loss": "4.362", "ntokens": "1808.65", "nsentences": "4.985", "prob_perplexity": "15.313", "code_perplexity": "15.238", "temp": "1.991", "loss_0": "4.157", "loss_1": "0.141", "loss_2": "0.063", "accuracy": "0.46739", "wps": "5336.5", "ups": "2.95", "wpb": "1808.7", "bsz": "5", "num_updates": "1000", "lr": "1.5625e-05", "gnorm": "1.801", "clip": "0", "train_wall": "44", "gb_free": "13.4", "wall": "317"}
[2022-01-13 10:09:31,726][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:09:32,182][valid][INFO] - {"epoch": 9, "valid_loss": "4.319", "valid_ntokens": "1568", "valid_nsentences": "4.5", "valid_prob_perplexity": "15.154", "valid_code_perplexity": "15.107", "valid_temp": "1.989", "valid_loss_0": "4.126", "valid_loss_1": "0.141", "valid_loss_2": "0.052", "valid_accuracy": "0.46205", "valid_wps": "27324", "valid_wpb": "1568", "valid_bsz": "4.5", "valid_num_updates": "1098", "valid_best_loss": "4.125"}
[2022-01-13 10:09:32,183][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 1098 updates
[2022-01-13 10:09:32,184][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:09:36,023][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:09:36,037][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 9 @ 1098 updates, score 4.319) (writing took 3.853298118337989 seconds)
[2022-01-13 10:09:36,037][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2022-01-13 10:09:36,049][train][INFO] - {"epoch": 9, "train_loss": "4.278", "train_ntokens": "1807.86", "train_nsentences": "4.97541", "train_prob_perplexity": "14.884", "train_code_perplexity": "14.836", "train_temp": "1.99", "train_loss_0": "4.082", "train_loss_1": "0.141", "train_loss_2": "0.055", "train_accuracy": "0.4739", "train_wps": "6863", "train_ups": "3.8", "train_wpb": "1807.9", "train_bsz": "5", "train_num_updates": "1098", "train_lr": "1.71563e-05", "train_gnorm": "1.741", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "344"}
[2022-01-13 10:09:36,093][fairseq.trainer][INFO] - begin training epoch 10
[2022-01-13 10:09:36,094][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:09:59,260][train_inner][INFO] - {"epoch": 10, "update": 9.836, "loss": "4.283", "ntokens": "1800.47", "nsentences": "4.97", "prob_perplexity": "15.149", "code_perplexity": "15.105", "temp": "1.989", "loss_0": "4.09", "loss_1": "0.141", "loss_2": "0.052", "accuracy": "0.47045", "wps": "7269.9", "ups": "4.04", "wpb": "1800.5", "bsz": "5", "num_updates": "1200", "lr": "1.875e-05", "gnorm": "1.707", "clip": "0", "train_wall": "44", "gb_free": "13.5", "wall": "367"}
[2022-01-13 10:10:03,780][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:10:04,322][valid][INFO] - {"epoch": 10, "valid_loss": "4.167", "valid_ntokens": "1657", "valid_nsentences": "4.5", "valid_prob_perplexity": "14.753", "valid_code_perplexity": "14.714", "valid_temp": "1.988", "valid_loss_0": "3.978", "valid_loss_1": "0.141", "valid_loss_2": "0.049", "valid_accuracy": "0.48401", "valid_wps": "28704.6", "valid_wpb": "1657", "valid_bsz": "4.5", "valid_num_updates": "1220", "valid_best_loss": "4.125"}
[2022-01-13 10:10:04,325][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 1220 updates
[2022-01-13 10:10:04,326][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:10:08,346][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:10:08,368][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 10 @ 1220 updates, score 4.167) (writing took 4.042660156264901 seconds)
[2022-01-13 10:10:08,368][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2022-01-13 10:10:08,383][train][INFO] - {"epoch": 10, "train_loss": "4.287", "train_ntokens": "1805.5", "train_nsentences": "4.97541", "train_prob_perplexity": "15.33", "train_code_perplexity": "15.288", "train_temp": "1.988", "train_loss_0": "4.096", "train_loss_1": "0.141", "train_loss_2": "0.05", "train_accuracy": "0.46802", "train_wps": "6815.6", "train_ups": "3.77", "train_wpb": "1805.5", "train_bsz": "5", "train_num_updates": "1220", "train_lr": "1.90625e-05", "train_gnorm": "1.714", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "376"}
[2022-01-13 10:10:08,451][fairseq.trainer][INFO] - begin training epoch 11
[2022-01-13 10:10:08,451][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:10:36,207][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:10:36,795][valid][INFO] - {"epoch": 11, "valid_loss": "4.109", "valid_ntokens": "1655.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "16.43", "valid_code_perplexity": "16.354", "valid_temp": "1.987", "valid_loss_0": "3.923", "valid_loss_1": "0.141", "valid_loss_2": "0.045", "valid_accuracy": "0.46934", "valid_wps": "29954.1", "valid_wpb": "1655.5", "valid_bsz": "4.5", "valid_num_updates": "1342", "valid_best_loss": "4.109"}
[2022-01-13 10:10:36,797][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 1342 updates
[2022-01-13 10:10:36,797][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:10:40,717][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:10:48,204][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 1342 updates, score 4.109) (writing took 11.406912859529257 seconds)
[2022-01-13 10:10:48,204][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2022-01-13 10:10:48,218][train][INFO] - {"epoch": 11, "train_loss": "4.314", "train_ntokens": "1805.39", "train_nsentences": "4.97541", "train_prob_perplexity": "16.115", "train_code_perplexity": "16.076", "train_temp": "1.987", "train_loss_0": "4.127", "train_loss_1": "0.141", "train_loss_2": "0.046", "train_accuracy": "0.44946", "train_wps": "5531.1", "train_ups": "3.06", "train_wpb": "1805.4", "train_bsz": "5", "train_num_updates": "1342", "train_lr": "2.09688e-05", "train_gnorm": "1.683", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "416"}
[2022-01-13 10:10:48,296][fairseq.trainer][INFO] - begin training epoch 12
[2022-01-13 10:10:48,297][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:11:01,691][train_inner][INFO] - {"epoch": 12, "update": 11.475, "loss": "4.303", "ntokens": "1808.09", "nsentences": "4.985", "prob_perplexity": "16.158", "code_perplexity": "16.117", "temp": "1.987", "loss_0": "4.115", "loss_1": "0.141", "loss_2": "0.047", "accuracy": "0.44982", "wps": "5793.4", "ups": "3.2", "wpb": "1808.1", "bsz": "5", "num_updates": "1400", "lr": "2.1875e-05", "gnorm": "1.689", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "429"}
[2022-01-13 10:11:15,979][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:11:16,508][valid][INFO] - {"epoch": 12, "valid_loss": "4.076", "valid_ntokens": "1570.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "16.854", "valid_code_perplexity": "16.839", "valid_temp": "1.985", "valid_loss_0": "3.89", "valid_loss_1": "0.14", "valid_loss_2": "0.046", "valid_accuracy": "0.4766", "valid_wps": "27400.8", "valid_wpb": "1570.5", "valid_bsz": "4.5", "valid_num_updates": "1464", "valid_best_loss": "4.076"}
[2022-01-13 10:11:16,511][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 1464 updates
[2022-01-13 10:11:16,512][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:11:20,429][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:11:28,019][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 1464 updates, score 4.076) (writing took 11.508202661760151 seconds)
[2022-01-13 10:11:28,020][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2022-01-13 10:11:28,033][train][INFO] - {"epoch": 12, "train_loss": "4.299", "train_ntokens": "1798", "train_nsentences": "4.97541", "train_prob_perplexity": "16.784", "train_code_perplexity": "16.741", "train_temp": "1.986", "train_loss_0": "4.113", "train_loss_1": "0.14", "train_loss_2": "0.045", "train_accuracy": "0.43742", "train_wps": "5511.2", "train_ups": "3.07", "train_wpb": "1798", "train_bsz": "5", "train_num_updates": "1464", "train_lr": "2.2875e-05", "train_gnorm": "1.681", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "456"}
[2022-01-13 10:11:28,113][fairseq.trainer][INFO] - begin training epoch 13
[2022-01-13 10:11:28,114][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:11:55,711][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:11:56,233][valid][INFO] - {"epoch": 13, "valid_loss": "4.252", "valid_ntokens": "1655.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "16.803", "valid_code_perplexity": "16.79", "valid_temp": "1.984", "valid_loss_0": "4.068", "valid_loss_1": "0.14", "valid_loss_2": "0.044", "valid_accuracy": "0.42132", "valid_wps": "27880.4", "valid_wpb": "1655.5", "valid_bsz": "4.5", "valid_num_updates": "1586", "valid_best_loss": "4.076"}
[2022-01-13 10:11:56,235][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 1586 updates
[2022-01-13 10:11:56,235][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:12:00,149][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:12:00,165][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 13 @ 1586 updates, score 4.252) (writing took 3.930660981684923 seconds)
[2022-01-13 10:12:00,166][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2022-01-13 10:12:00,180][train][INFO] - {"epoch": 13, "train_loss": "4.332", "train_ntokens": "1801.28", "train_nsentences": "4.97541", "train_prob_perplexity": "17.62", "train_code_perplexity": "17.581", "train_temp": "1.985", "train_loss_0": "4.148", "train_loss_1": "0.14", "train_loss_2": "0.043", "train_accuracy": "0.41065", "train_wps": "6839", "train_ups": "3.8", "train_wpb": "1801.3", "train_bsz": "5", "train_num_updates": "1586", "train_lr": "2.47813e-05", "train_gnorm": "1.674", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "488"}
[2022-01-13 10:12:00,251][fairseq.trainer][INFO] - begin training epoch 14
[2022-01-13 10:12:00,252][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:12:03,783][train_inner][INFO] - {"epoch": 14, "update": 13.115, "loss": "4.322", "ntokens": "1799.34", "nsentences": "4.97", "prob_perplexity": "17.408", "code_perplexity": "17.368", "temp": "1.985", "loss_0": "4.138", "loss_1": "0.14", "loss_2": "0.044", "accuracy": "0.41744", "wps": "5797", "ups": "3.22", "wpb": "1799.3", "bsz": "5", "num_updates": "1600", "lr": "2.5e-05", "gnorm": "1.672", "clip": "0", "train_wall": "44", "gb_free": "13.4", "wall": "491"}
[2022-01-13 10:12:27,901][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:12:28,439][valid][INFO] - {"epoch": 14, "valid_loss": "4.195", "valid_ntokens": "1659", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.192", "valid_code_perplexity": "19.154", "valid_temp": "1.983", "valid_loss_0": "4.014", "valid_loss_1": "0.14", "valid_loss_2": "0.041", "valid_accuracy": "0.41019", "valid_wps": "27980.9", "valid_wpb": "1659", "valid_bsz": "4.5", "valid_num_updates": "1708", "valid_best_loss": "4.076"}
[2022-01-13 10:12:28,441][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 1708 updates
[2022-01-13 10:12:28,442][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:12:32,342][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:12:32,366][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 14 @ 1708 updates, score 4.195) (writing took 3.9243506295606494 seconds)
[2022-01-13 10:12:32,366][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2022-01-13 10:12:32,380][train][INFO] - {"epoch": 14, "train_loss": "4.325", "train_ntokens": "1803.11", "train_nsentences": "4.97541", "train_prob_perplexity": "17.893", "train_code_perplexity": "17.862", "train_temp": "1.984", "train_loss_0": "4.143", "train_loss_1": "0.14", "train_loss_2": "0.042", "train_accuracy": "0.39936", "train_wps": "6834.5", "train_ups": "3.79", "train_wpb": "1803.1", "train_bsz": "5", "train_num_updates": "1708", "train_lr": "2.66875e-05", "train_gnorm": "1.614", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "520"}
[2022-01-13 10:12:32,459][fairseq.trainer][INFO] - begin training epoch 15
[2022-01-13 10:12:32,460][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:12:53,577][train_inner][INFO] - {"epoch": 15, "update": 14.754, "loss": "4.333", "ntokens": "1804.05", "nsentences": "4.985", "prob_perplexity": "18.483", "code_perplexity": "18.455", "temp": "1.983", "loss_0": "4.153", "loss_1": "0.14", "loss_2": "0.041", "accuracy": "0.39027", "wps": "7248.1", "ups": "4.02", "wpb": "1804", "bsz": "5", "num_updates": "1800", "lr": "2.8125e-05", "gnorm": "1.603", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "541"}
[2022-01-13 10:13:00,236][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:13:00,773][valid][INFO] - {"epoch": 15, "valid_loss": "4.302", "valid_ntokens": "1643", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.737", "valid_code_perplexity": "19.729", "valid_temp": "1.982", "valid_loss_0": "4.122", "valid_loss_1": "0.14", "valid_loss_2": "0.04", "valid_accuracy": "0.37918", "valid_wps": "29230.7", "valid_wpb": "1643", "valid_bsz": "4.5", "valid_num_updates": "1830", "valid_best_loss": "4.076"}
[2022-01-13 10:13:00,774][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 1830 updates
[2022-01-13 10:13:00,775][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:13:04,648][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:13:04,666][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 15 @ 1830 updates, score 4.302) (writing took 3.8911637626588345 seconds)
[2022-01-13 10:13:04,666][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2022-01-13 10:13:04,678][train][INFO] - {"epoch": 15, "train_loss": "4.327", "train_ntokens": "1799.11", "train_nsentences": "4.97541", "train_prob_perplexity": "19.011", "train_code_perplexity": "18.99", "train_temp": "1.982", "train_loss_0": "4.147", "train_loss_1": "0.14", "train_loss_2": "0.04", "train_accuracy": "0.38269", "train_wps": "6798.4", "train_ups": "3.78", "train_wpb": "1799.1", "train_bsz": "5", "train_num_updates": "1830", "train_lr": "2.85938e-05", "train_gnorm": "1.599", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "552"}
[2022-01-13 10:13:04,723][fairseq.trainer][INFO] - begin training epoch 16
[2022-01-13 10:13:04,723][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:13:32,316][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:13:32,842][valid][INFO] - {"epoch": 16, "valid_loss": "4.372", "valid_ntokens": "1635", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.343", "valid_code_perplexity": "19.327", "valid_temp": "1.981", "valid_loss_0": "4.194", "valid_loss_1": "0.14", "valid_loss_2": "0.038", "valid_accuracy": "0.36667", "valid_wps": "27906.3", "valid_wpb": "1635", "valid_bsz": "4.5", "valid_num_updates": "1952", "valid_best_loss": "4.076"}
[2022-01-13 10:13:32,844][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 1952 updates
[2022-01-13 10:13:32,845][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:13:36,738][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:13:36,763][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 16 @ 1952 updates, score 4.372) (writing took 3.9187301294878125 seconds)
[2022-01-13 10:13:36,763][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2022-01-13 10:13:36,776][train][INFO] - {"epoch": 16, "train_loss": "4.333", "train_ntokens": "1800.78", "train_nsentences": "4.97541", "train_prob_perplexity": "19.688", "train_code_perplexity": "19.663", "train_temp": "1.981", "train_loss_0": "4.155", "train_loss_1": "0.14", "train_loss_2": "0.038", "train_accuracy": "0.37105", "train_wps": "6847.3", "train_ups": "3.8", "train_wpb": "1800.8", "train_bsz": "5", "train_num_updates": "1952", "train_lr": "3.05e-05", "train_gnorm": "1.653", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "584"}
[2022-01-13 10:13:36,848][fairseq.trainer][INFO] - begin training epoch 17
[2022-01-13 10:13:36,849][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:13:48,011][train_inner][INFO] - {"epoch": 17, "update": 16.393, "loss": "4.326", "ntokens": "1794.75", "nsentences": "4.955", "prob_perplexity": "19.673", "code_perplexity": "19.649", "temp": "1.981", "loss_0": "4.148", "loss_1": "0.14", "loss_2": "0.038", "accuracy": "0.37219", "wps": "6595.8", "ups": "3.68", "wpb": "1794.8", "bsz": "5", "num_updates": "2000", "lr": "3.125e-05", "gnorm": "1.635", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "596"}
[2022-01-13 10:14:04,680][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:14:05,216][valid][INFO] - {"epoch": 17, "valid_loss": "4.284", "valid_ntokens": "1676", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.164", "valid_code_perplexity": "21.16", "valid_temp": "1.979", "valid_loss_0": "4.109", "valid_loss_1": "0.139", "valid_loss_2": "0.035", "valid_accuracy": "0.37261", "valid_wps": "29484.6", "valid_wpb": "1676", "valid_bsz": "4.5", "valid_num_updates": "2074", "valid_best_loss": "4.076"}
[2022-01-13 10:14:05,219][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 2074 updates
[2022-01-13 10:14:05,220][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:14:09,142][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:14:09,167][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 17 @ 2074 updates, score 4.284) (writing took 3.9474621983245015 seconds)
[2022-01-13 10:14:09,167][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2022-01-13 10:14:09,181][train][INFO] - {"epoch": 17, "train_loss": "4.312", "train_ntokens": "1805.85", "train_nsentences": "4.97541", "train_prob_perplexity": "20.175", "train_code_perplexity": "20.151", "train_temp": "1.98", "train_loss_0": "4.135", "train_loss_1": "0.14", "train_loss_2": "0.037", "train_accuracy": "0.36911", "train_wps": "6801.6", "train_ups": "3.77", "train_wpb": "1805.9", "train_bsz": "5", "train_num_updates": "2074", "train_lr": "3.24062e-05", "train_gnorm": "1.591", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "617"}
[2022-01-13 10:14:09,233][fairseq.trainer][INFO] - begin training epoch 18
[2022-01-13 10:14:09,234][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:14:36,995][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:14:37,540][valid][INFO] - {"epoch": 18, "valid_loss": "4.291", "valid_ntokens": "1703", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.411", "valid_code_perplexity": "19.405", "valid_temp": "1.978", "valid_loss_0": "4.114", "valid_loss_1": "0.14", "valid_loss_2": "0.037", "valid_accuracy": "0.38579", "valid_wps": "29065", "valid_wpb": "1703", "valid_bsz": "4.5", "valid_num_updates": "2196", "valid_best_loss": "4.076"}
[2022-01-13 10:14:37,542][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 2196 updates
[2022-01-13 10:14:37,542][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:14:41,409][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:14:41,416][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 18 @ 2196 updates, score 4.291) (writing took 3.8738313168287277 seconds)
[2022-01-13 10:14:41,416][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2022-01-13 10:14:41,429][train][INFO] - {"epoch": 18, "train_loss": "4.265", "train_ntokens": "1805.91", "train_nsentences": "4.97541", "train_prob_perplexity": "20.439", "train_code_perplexity": "20.422", "train_temp": "1.979", "train_loss_0": "4.089", "train_loss_1": "0.14", "train_loss_2": "0.036", "train_accuracy": "0.37774", "train_wps": "6834.8", "train_ups": "3.78", "train_wpb": "1805.9", "train_bsz": "5", "train_num_updates": "2196", "train_lr": "3.43125e-05", "train_gnorm": "1.632", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "649"}
[2022-01-13 10:14:41,472][fairseq.trainer][INFO] - begin training epoch 19
[2022-01-13 10:14:41,472][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:14:42,794][train_inner][INFO] - {"epoch": 19, "update": 18.033, "loss": "4.277", "ntokens": "1809.31", "nsentences": "4.985", "prob_perplexity": "20.363", "code_perplexity": "20.345", "temp": "1.979", "loss_0": "4.101", "loss_1": "0.14", "loss_2": "0.036", "accuracy": "0.3755", "wps": "6606.8", "ups": "3.65", "wpb": "1809.3", "bsz": "5", "num_updates": "2200", "lr": "3.4375e-05", "gnorm": "1.619", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "650"}
[2022-01-13 10:15:09,286][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:15:09,769][valid][INFO] - {"epoch": 19, "valid_loss": "4.223", "valid_ntokens": "1606.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.118", "valid_code_perplexity": "20.1", "valid_temp": "1.977", "valid_loss_0": "4.047", "valid_loss_1": "0.14", "valid_loss_2": "0.036", "valid_accuracy": "0.39091", "valid_wps": "28970.4", "valid_wpb": "1606.5", "valid_bsz": "4.5", "valid_num_updates": "2318", "valid_best_loss": "4.076"}
[2022-01-13 10:15:09,771][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 2318 updates
[2022-01-13 10:15:09,772][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:15:13,711][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:15:13,735][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 19 @ 2318 updates, score 4.223) (writing took 3.9637051401659846 seconds)
[2022-01-13 10:15:13,736][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2022-01-13 10:15:13,748][train][INFO] - {"epoch": 19, "train_loss": "4.255", "train_ntokens": "1798.25", "train_nsentences": "4.97541", "train_prob_perplexity": "20.835", "train_code_perplexity": "20.813", "train_temp": "1.978", "train_loss_0": "4.081", "train_loss_1": "0.14", "train_loss_2": "0.035", "train_accuracy": "0.37786", "train_wps": "6790.7", "train_ups": "3.78", "train_wpb": "1798.3", "train_bsz": "5", "train_num_updates": "2318", "train_lr": "3.62188e-05", "train_gnorm": "1.585", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "681"}
[2022-01-13 10:15:13,805][fairseq.trainer][INFO] - begin training epoch 20
[2022-01-13 10:15:13,806][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:15:32,558][train_inner][INFO] - {"epoch": 20, "update": 19.672, "loss": "4.246", "ntokens": "1797.99", "nsentences": "4.97", "prob_perplexity": "20.637", "code_perplexity": "20.616", "temp": "1.977", "loss_0": "4.071", "loss_1": "0.14", "loss_2": "0.035", "accuracy": "0.37908", "wps": "7228", "ups": "4.02", "wpb": "1798", "bsz": "5", "num_updates": "2400", "lr": "3.75e-05", "gnorm": "1.588", "clip": "0", "train_wall": "45", "gb_free": "14.5", "wall": "700"}
[2022-01-13 10:15:41,588][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:15:42,123][valid][INFO] - {"epoch": 20, "valid_loss": "4.023", "valid_ntokens": "1560", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.938", "valid_code_perplexity": "18.94", "valid_temp": "1.976", "valid_loss_0": "3.85", "valid_loss_1": "0.14", "valid_loss_2": "0.034", "valid_accuracy": "0.43814", "valid_wps": "29705.3", "valid_wpb": "1560", "valid_bsz": "4.5", "valid_num_updates": "2440", "valid_best_loss": "4.023"}
[2022-01-13 10:15:42,126][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 2440 updates
[2022-01-13 10:15:42,127][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:15:46,047][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:15:52,602][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 2440 updates, score 4.023) (writing took 10.475685353390872 seconds)
[2022-01-13 10:15:52,602][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2022-01-13 10:15:52,615][train][INFO] - {"epoch": 20, "train_loss": "4.223", "train_ntokens": "1803.34", "train_nsentences": "4.97541", "train_prob_perplexity": "20.37", "train_code_perplexity": "20.353", "train_temp": "1.976", "train_loss_0": "4.049", "train_loss_1": "0.14", "train_loss_2": "0.035", "train_accuracy": "0.38352", "train_wps": "5662.4", "train_ups": "3.14", "train_wpb": "1803.3", "train_bsz": "5", "train_num_updates": "2440", "train_lr": "3.8125e-05", "train_gnorm": "1.594", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "720"}
[2022-01-13 10:15:52,693][fairseq.trainer][INFO] - begin training epoch 21
[2022-01-13 10:15:52,694][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:16:20,527][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:16:20,985][valid][INFO] - {"epoch": 21, "valid_loss": "4.227", "valid_ntokens": "1643", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.269", "valid_code_perplexity": "19.273", "valid_temp": "1.975", "valid_loss_0": "4.055", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.37097", "valid_wps": "28307.3", "valid_wpb": "1643", "valid_bsz": "4.5", "valid_num_updates": "2562", "valid_best_loss": "4.023"}
[2022-01-13 10:16:20,988][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 2562 updates
[2022-01-13 10:16:20,989][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:16:25,125][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:16:25,148][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 21 @ 2562 updates, score 4.227) (writing took 4.159838896244764 seconds)
[2022-01-13 10:16:25,149][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2022-01-13 10:16:25,162][train][INFO] - {"epoch": 21, "train_loss": "4.227", "train_ntokens": "1810.59", "train_nsentences": "4.97541", "train_prob_perplexity": "20.779", "train_code_perplexity": "20.769", "train_temp": "1.975", "train_loss_0": "4.054", "train_loss_1": "0.14", "train_loss_2": "0.033", "train_accuracy": "0.37718", "train_wps": "6789.7", "train_ups": "3.75", "train_wpb": "1810.6", "train_bsz": "5", "train_num_updates": "2562", "train_lr": "4.00313e-05", "train_gnorm": "1.608", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "753"}
[2022-01-13 10:16:25,221][fairseq.trainer][INFO] - begin training epoch 22
[2022-01-13 10:16:25,222][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:16:34,086][train_inner][INFO] - {"epoch": 22, "update": 21.311, "loss": "4.215", "ntokens": "1806.11", "nsentences": "4.97", "prob_perplexity": "20.591", "code_perplexity": "20.58", "temp": "1.975", "loss_0": "4.042", "loss_1": "0.14", "loss_2": "0.033", "accuracy": "0.38094", "wps": "5872.1", "ups": "3.25", "wpb": "1806.1", "bsz": "5", "num_updates": "2600", "lr": "4.0625e-05", "gnorm": "1.621", "clip": "0", "train_wall": "45", "gb_free": "19.4", "wall": "762"}
[2022-01-13 10:16:53,022][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:16:53,654][valid][INFO] - {"epoch": 22, "valid_loss": "4.134", "valid_ntokens": "1592", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.781", "valid_code_perplexity": "20.762", "valid_temp": "1.973", "valid_loss_0": "3.963", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.39165", "valid_wps": "29876.5", "valid_wpb": "1592", "valid_bsz": "4.5", "valid_num_updates": "2684", "valid_best_loss": "4.023"}
[2022-01-13 10:16:53,656][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 2684 updates
[2022-01-13 10:16:53,656][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:16:57,526][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:16:57,544][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 22 @ 2684 updates, score 4.134) (writing took 3.8884269911795855 seconds)
[2022-01-13 10:16:57,545][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2022-01-13 10:16:57,558][train][INFO] - {"epoch": 22, "train_loss": "4.185", "train_ntokens": "1804.66", "train_nsentences": "4.97541", "train_prob_perplexity": "20.077", "train_code_perplexity": "20.06", "train_temp": "1.974", "train_loss_0": "4.013", "train_loss_1": "0.14", "train_loss_2": "0.032", "train_accuracy": "0.3863", "train_wps": "6799.1", "train_ups": "3.77", "train_wpb": "1804.7", "train_bsz": "5", "train_num_updates": "2684", "train_lr": "4.19375e-05", "train_gnorm": "1.63", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "785"}
[2022-01-13 10:16:57,613][fairseq.trainer][INFO] - begin training epoch 23
[2022-01-13 10:16:57,614][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:17:24,102][train_inner][INFO] - {"epoch": 23, "update": 22.951, "loss": "4.192", "ntokens": "1804.61", "nsentences": "4.985", "prob_perplexity": "20.103", "code_perplexity": "20.087", "temp": "1.973", "loss_0": "4.021", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.38546", "wps": "7217.9", "ups": "4", "wpb": "1804.6", "bsz": "5", "num_updates": "2800", "lr": "4.375e-05", "gnorm": "1.618", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "812"}
[2022-01-13 10:17:25,423][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:17:25,973][valid][INFO] - {"epoch": 23, "valid_loss": "4.092", "valid_ntokens": "1565", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.799", "valid_code_perplexity": "20.797", "valid_temp": "1.972", "valid_loss_0": "3.918", "valid_loss_1": "0.14", "valid_loss_2": "0.034", "valid_accuracy": "0.39425", "valid_wps": "25559.9", "valid_wpb": "1565", "valid_bsz": "4.5", "valid_num_updates": "2806", "valid_best_loss": "4.023"}
[2022-01-13 10:17:25,977][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 2806 updates
[2022-01-13 10:17:25,979][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:17:29,874][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:17:29,892][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 23 @ 2806 updates, score 4.092) (writing took 3.914068227633834 seconds)
[2022-01-13 10:17:29,892][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2022-01-13 10:17:29,905][train][INFO] - {"epoch": 23, "train_loss": "4.193", "train_ntokens": "1798.75", "train_nsentences": "4.97541", "train_prob_perplexity": "20.205", "train_code_perplexity": "20.192", "train_temp": "1.973", "train_loss_0": "4.023", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.38486", "train_wps": "6786.9", "train_ups": "3.77", "train_wpb": "1798.7", "train_bsz": "5", "train_num_updates": "2806", "train_lr": "4.38437e-05", "train_gnorm": "1.621", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "818"}
[2022-01-13 10:17:29,944][fairseq.trainer][INFO] - begin training epoch 24
[2022-01-13 10:17:29,945][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:17:57,619][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:17:58,148][valid][INFO] - {"epoch": 24, "valid_loss": "4.084", "valid_ntokens": "1612.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.931", "valid_code_perplexity": "20.922", "valid_temp": "1.971", "valid_loss_0": "3.912", "valid_loss_1": "0.14", "valid_loss_2": "0.033", "valid_accuracy": "0.41705", "valid_wps": "29326.5", "valid_wpb": "1612.5", "valid_bsz": "4.5", "valid_num_updates": "2928", "valid_best_loss": "4.023"}
[2022-01-13 10:17:58,151][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 2928 updates
[2022-01-13 10:17:58,151][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:18:02,031][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:18:02,053][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 24 @ 2928 updates, score 4.084) (writing took 3.9025707552209496 seconds)
[2022-01-13 10:18:02,054][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2022-01-13 10:18:02,067][train][INFO] - {"epoch": 24, "train_loss": "4.176", "train_ntokens": "1801.61", "train_nsentences": "4.97541", "train_prob_perplexity": "20.478", "train_code_perplexity": "20.467", "train_temp": "1.972", "train_loss_0": "4.005", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.38417", "train_wps": "6836.8", "train_ups": "3.79", "train_wpb": "1801.6", "train_bsz": "5", "train_num_updates": "2928", "train_lr": "4.575e-05", "train_gnorm": "1.58", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "850"}
[2022-01-13 10:18:02,132][fairseq.trainer][INFO] - begin training epoch 25
[2022-01-13 10:18:02,132][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:18:18,762][train_inner][INFO] - {"epoch": 25, "update": 24.59, "loss": "4.158", "ntokens": "1803.38", "nsentences": "4.985", "prob_perplexity": "20.51", "code_perplexity": "20.5", "temp": "1.971", "loss_0": "3.987", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.38831", "wps": "6600.1", "ups": "3.66", "wpb": "1803.4", "bsz": "5", "num_updates": "3000", "lr": "4.6875e-05", "gnorm": "1.6", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "866"}
[2022-01-13 10:18:29,898][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:18:30,445][valid][INFO] - {"epoch": 25, "valid_loss": "4.2", "valid_ntokens": "1628", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.627", "valid_code_perplexity": "21.65", "valid_temp": "1.97", "valid_loss_0": "4.029", "valid_loss_1": "0.139", "valid_loss_2": "0.032", "valid_accuracy": "0.38575", "valid_wps": "29293.5", "valid_wpb": "1628", "valid_bsz": "4.5", "valid_num_updates": "3050", "valid_best_loss": "4.023"}
[2022-01-13 10:18:30,447][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 3050 updates
[2022-01-13 10:18:30,447][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:18:34,342][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:18:34,367][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 25 @ 3050 updates, score 4.2) (writing took 3.91985060274601 seconds)
[2022-01-13 10:18:34,367][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2022-01-13 10:18:34,380][train][INFO] - {"epoch": 25, "train_loss": "4.145", "train_ntokens": "1804.75", "train_nsentences": "4.97541", "train_prob_perplexity": "20.413", "train_code_perplexity": "20.403", "train_temp": "1.97", "train_loss_0": "3.974", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.39138", "train_wps": "6816.6", "train_ups": "3.78", "train_wpb": "1804.7", "train_bsz": "5", "train_num_updates": "3050", "train_lr": "4.76562e-05", "train_gnorm": "1.625", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "882"}
[2022-01-13 10:18:34,437][fairseq.trainer][INFO] - begin training epoch 26
[2022-01-13 10:18:34,438][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:19:02,116][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:19:02,649][valid][INFO] - {"epoch": 26, "valid_loss": "4.061", "valid_ntokens": "1642.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.476", "valid_code_perplexity": "20.483", "valid_temp": "1.969", "valid_loss_0": "3.891", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.40944", "valid_wps": "28071.1", "valid_wpb": "1642.5", "valid_bsz": "4.5", "valid_num_updates": "3172", "valid_best_loss": "4.023"}
[2022-01-13 10:19:02,651][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 3172 updates
[2022-01-13 10:19:02,651][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:19:06,556][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:19:06,581][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 26 @ 3172 updates, score 4.061) (writing took 3.929944838397205 seconds)
[2022-01-13 10:19:06,581][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2022-01-13 10:19:06,594][train][INFO] - {"epoch": 26, "train_loss": "4.114", "train_ntokens": "1798.84", "train_nsentences": "4.97541", "train_prob_perplexity": "20.809", "train_code_perplexity": "20.798", "train_temp": "1.969", "train_loss_0": "3.944", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.39227", "train_wps": "6815.1", "train_ups": "3.79", "train_wpb": "1798.8", "train_bsz": "5", "train_num_updates": "3172", "train_lr": "4.95625e-05", "train_gnorm": "1.614", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "914"}
[2022-01-13 10:19:06,667][fairseq.trainer][INFO] - begin training epoch 27
[2022-01-13 10:19:06,668][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:19:13,384][train_inner][INFO] - {"epoch": 27, "update": 26.23, "loss": "4.127", "ntokens": "1800.81", "nsentences": "4.97", "prob_perplexity": "20.578", "code_perplexity": "20.566", "temp": "1.969", "loss_0": "3.956", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.39136", "wps": "6595.2", "ups": "3.66", "wpb": "1800.8", "bsz": "5", "num_updates": "3200", "lr": "5e-05", "gnorm": "1.615", "clip": "0", "train_wall": "44", "gb_free": "13.4", "wall": "921"}
[2022-01-13 10:19:34,430][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:19:34,955][valid][INFO] - {"epoch": 27, "valid_loss": "4.053", "valid_ntokens": "1701.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.536", "valid_code_perplexity": "19.519", "valid_temp": "1.967", "valid_loss_0": "3.882", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.41581", "valid_wps": "28608.6", "valid_wpb": "1701.5", "valid_bsz": "4.5", "valid_num_updates": "3294", "valid_best_loss": "4.023"}
[2022-01-13 10:19:34,958][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 3294 updates
[2022-01-13 10:19:34,958][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:19:38,840][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:19:38,863][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 27 @ 3294 updates, score 4.053) (writing took 3.905190506018698 seconds)
[2022-01-13 10:19:38,863][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2022-01-13 10:19:38,876][train][INFO] - {"epoch": 27, "train_loss": "4.115", "train_ntokens": "1806.32", "train_nsentences": "4.97541", "train_prob_perplexity": "20.31", "train_code_perplexity": "20.298", "train_temp": "1.968", "train_loss_0": "3.945", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.39622", "train_wps": "6829.2", "train_ups": "3.78", "train_wpb": "1806.3", "train_bsz": "5", "train_num_updates": "3294", "train_lr": "5.14688e-05", "train_gnorm": "1.623", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "946"}
[2022-01-13 10:19:38,942][fairseq.trainer][INFO] - begin training epoch 28
[2022-01-13 10:19:38,943][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:20:03,252][train_inner][INFO] - {"epoch": 28, "update": 27.869, "loss": "4.106", "ntokens": "1812.11", "nsentences": "4.985", "prob_perplexity": "20.307", "code_perplexity": "20.297", "temp": "1.967", "loss_0": "3.936", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.39818", "wps": "7269.7", "ups": "4.01", "wpb": "1812.1", "bsz": "5", "num_updates": "3400", "lr": "5.3125e-05", "gnorm": "1.655", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "971"}
[2022-01-13 10:20:06,671][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:20:07,200][valid][INFO] - {"epoch": 28, "valid_loss": "4.007", "valid_ntokens": "1616.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.772", "valid_code_perplexity": "21.748", "valid_temp": "1.966", "valid_loss_0": "3.84", "valid_loss_1": "0.139", "valid_loss_2": "0.028", "valid_accuracy": "0.4052", "valid_wps": "27933.5", "valid_wpb": "1616.5", "valid_bsz": "4.5", "valid_num_updates": "3416", "valid_best_loss": "4.007"}
[2022-01-13 10:20:07,202][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 3416 updates
[2022-01-13 10:20:07,203][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:20:10,953][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:20:17,597][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 28 @ 3416 updates, score 4.007) (writing took 10.394338893704116 seconds)
[2022-01-13 10:20:17,598][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2022-01-13 10:20:17,610][train][INFO] - {"epoch": 28, "train_loss": "4.088", "train_ntokens": "1806.05", "train_nsentences": "4.97541", "train_prob_perplexity": "20.42", "train_code_perplexity": "20.41", "train_temp": "1.967", "train_loss_0": "3.919", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.40095", "train_wps": "5690.3", "train_ups": "3.15", "train_wpb": "1806", "train_bsz": "5", "train_num_updates": "3416", "train_lr": "5.3375e-05", "train_gnorm": "1.696", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "985"}
[2022-01-13 10:20:17,688][fairseq.trainer][INFO] - begin training epoch 29
[2022-01-13 10:20:17,689][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:20:45,374][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:20:45,901][valid][INFO] - {"epoch": 29, "valid_loss": "4.077", "valid_ntokens": "1654.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.491", "valid_code_perplexity": "20.504", "valid_temp": "1.965", "valid_loss_0": "3.907", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.39649", "valid_wps": "29820.4", "valid_wpb": "1654.5", "valid_bsz": "4.5", "valid_num_updates": "3538", "valid_best_loss": "4.007"}
[2022-01-13 10:20:45,903][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 3538 updates
[2022-01-13 10:20:45,903][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:20:49,930][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:20:49,948][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 29 @ 3538 updates, score 4.077) (writing took 4.045328963547945 seconds)
[2022-01-13 10:20:49,949][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2022-01-13 10:20:49,961][train][INFO] - {"epoch": 29, "train_loss": "4.088", "train_ntokens": "1800.55", "train_nsentences": "4.97541", "train_prob_perplexity": "20.141", "train_code_perplexity": "20.134", "train_temp": "1.966", "train_loss_0": "3.919", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.39907", "train_wps": "6792.7", "train_ups": "3.77", "train_wpb": "1800.5", "train_bsz": "5", "train_num_updates": "3538", "train_lr": "5.52812e-05", "train_gnorm": "1.699", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1018"}
[2022-01-13 10:20:50,017][fairseq.trainer][INFO] - begin training epoch 30
[2022-01-13 10:20:50,018][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:21:04,357][train_inner][INFO] - {"epoch": 30, "update": 29.508, "loss": "4.078", "ntokens": "1794.65", "nsentences": "4.97", "prob_perplexity": "20.19", "code_perplexity": "20.182", "temp": "1.965", "loss_0": "3.909", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.39988", "wps": "5875.4", "ups": "3.27", "wpb": "1794.7", "bsz": "5", "num_updates": "3600", "lr": "5.625e-05", "gnorm": "1.704", "clip": "0", "train_wall": "44", "gb_free": "14.6", "wall": "1032"}
[2022-01-13 10:21:17,705][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:21:18,247][valid][INFO] - {"epoch": 30, "valid_loss": "4.004", "valid_ntokens": "1630", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.198", "valid_code_perplexity": "18.201", "valid_temp": "1.964", "valid_loss_0": "3.832", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.41963", "valid_wps": "20066.8", "valid_wpb": "1630", "valid_bsz": "4.5", "valid_num_updates": "3660", "valid_best_loss": "4.004"}
[2022-01-13 10:21:18,249][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 3660 updates
[2022-01-13 10:21:18,249][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:21:22,120][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:21:29,159][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 30 @ 3660 updates, score 4.004) (writing took 10.909788058139384 seconds)
[2022-01-13 10:21:29,159][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2022-01-13 10:21:29,174][train][INFO] - {"epoch": 30, "train_loss": "4.084", "train_ntokens": "1794.41", "train_nsentences": "4.97541", "train_prob_perplexity": "20.097", "train_code_perplexity": "20.087", "train_temp": "1.964", "train_loss_0": "3.915", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.39805", "train_wps": "5584.8", "train_ups": "3.11", "train_wpb": "1794.4", "train_bsz": "5", "train_num_updates": "3660", "train_lr": "5.71875e-05", "train_gnorm": "1.704", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1057"}
[2022-01-13 10:21:29,272][fairseq.trainer][INFO] - begin training epoch 31
[2022-01-13 10:21:29,273][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:21:56,850][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:21:57,306][valid][INFO] - {"epoch": 31, "valid_loss": "4.063", "valid_ntokens": "1609.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.276", "valid_code_perplexity": "21.281", "valid_temp": "1.963", "valid_loss_0": "3.894", "valid_loss_1": "0.139", "valid_loss_2": "0.029", "valid_accuracy": "0.4082", "valid_wps": "27511.4", "valid_wpb": "1609.5", "valid_bsz": "4.5", "valid_num_updates": "3782", "valid_best_loss": "4.004"}
[2022-01-13 10:21:57,309][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 3782 updates
[2022-01-13 10:21:57,309][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:22:01,279][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:22:01,303][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 31 @ 3782 updates, score 4.063) (writing took 3.9940451942384243 seconds)
[2022-01-13 10:22:01,303][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2022-01-13 10:22:01,317][train][INFO] - {"epoch": 31, "train_loss": "4.07", "train_ntokens": "1804.09", "train_nsentences": "4.97541", "train_prob_perplexity": "20.879", "train_code_perplexity": "20.872", "train_temp": "1.963", "train_loss_0": "3.901", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.39705", "train_wps": "6850.4", "train_ups": "3.8", "train_wpb": "1804.1", "train_bsz": "5", "train_num_updates": "3782", "train_lr": "5.90938e-05", "train_gnorm": "1.687", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1089"}
[2022-01-13 10:22:01,374][fairseq.trainer][INFO] - begin training epoch 32
[2022-01-13 10:22:01,375][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:22:05,629][train_inner][INFO] - {"epoch": 32, "update": 31.148, "loss": "4.079", "ntokens": "1794.61", "nsentences": "4.955", "prob_perplexity": "20.631", "code_perplexity": "20.623", "temp": "1.963", "loss_0": "3.91", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.39712", "wps": "5859.2", "ups": "3.26", "wpb": "1794.6", "bsz": "5", "num_updates": "3800", "lr": "5.9375e-05", "gnorm": "1.698", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "1093"}
[2022-01-13 10:22:29,264][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:22:29,733][valid][INFO] - {"epoch": 32, "valid_loss": "3.88", "valid_ntokens": "1604", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.229", "valid_code_perplexity": "20.229", "valid_temp": "1.961", "valid_loss_0": "3.712", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.42862", "valid_wps": "28696.4", "valid_wpb": "1604", "valid_bsz": "4.5", "valid_num_updates": "3904", "valid_best_loss": "3.88"}
[2022-01-13 10:22:29,735][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 3904 updates
[2022-01-13 10:22:29,736][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:22:33,625][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:22:40,239][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 32 @ 3904 updates, score 3.88) (writing took 10.503693781793118 seconds)
[2022-01-13 10:22:40,240][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2022-01-13 10:22:40,255][train][INFO] - {"epoch": 32, "train_loss": "4.067", "train_ntokens": "1800.78", "train_nsentences": "4.97541", "train_prob_perplexity": "20.457", "train_code_perplexity": "20.448", "train_temp": "1.962", "train_loss_0": "3.897", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.39975", "train_wps": "5644.4", "train_ups": "3.13", "train_wpb": "1800.8", "train_bsz": "5", "train_num_updates": "3904", "train_lr": "6.1e-05", "train_gnorm": "1.689", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "15.1", "train_wall": "1128"}
[2022-01-13 10:22:40,340][fairseq.trainer][INFO] - begin training epoch 33
[2022-01-13 10:22:40,341][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:23:02,372][train_inner][INFO] - {"epoch": 33, "update": 32.787, "loss": "4.07", "ntokens": "1810.56", "nsentences": "4.985", "prob_perplexity": "20.624", "code_perplexity": "20.616", "temp": "1.961", "loss_0": "3.901", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.39732", "wps": "6383.1", "ups": "3.53", "wpb": "1810.6", "bsz": "5", "num_updates": "4000", "lr": "6.25e-05", "gnorm": "1.664", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "1150"}
[2022-01-13 10:23:08,253][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:23:08,694][valid][INFO] - {"epoch": 33, "valid_loss": "4.088", "valid_ntokens": "1689.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.154", "valid_code_perplexity": "20.126", "valid_temp": "1.96", "valid_loss_0": "3.919", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.39598", "valid_wps": "27962.8", "valid_wpb": "1689.5", "valid_bsz": "4.5", "valid_num_updates": "4026", "valid_best_loss": "3.88"}
[2022-01-13 10:23:08,696][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 4026 updates
[2022-01-13 10:23:08,697][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:23:12,653][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:23:12,669][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 33 @ 4026 updates, score 4.088) (writing took 3.973045692779124 seconds)
[2022-01-13 10:23:12,670][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2022-01-13 10:23:12,683][train][INFO] - {"epoch": 33, "train_loss": "4.065", "train_ntokens": "1810.75", "train_nsentences": "4.97541", "train_prob_perplexity": "20.808", "train_code_perplexity": "20.801", "train_temp": "1.961", "train_loss_0": "3.896", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.39668", "train_wps": "6815.2", "train_ups": "3.76", "train_wpb": "1810.7", "train_bsz": "5", "train_num_updates": "4026", "train_lr": "6.29063e-05", "train_gnorm": "1.667", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1160"}
[2022-01-13 10:23:12,749][fairseq.trainer][INFO] - begin training epoch 34
[2022-01-13 10:23:12,749][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:23:40,566][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:23:41,042][valid][INFO] - {"epoch": 34, "valid_loss": "4.099", "valid_ntokens": "1682", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.503", "valid_code_perplexity": "18.494", "valid_temp": "1.959", "valid_loss_0": "3.929", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.40488", "valid_wps": "31009.9", "valid_wpb": "1682", "valid_bsz": "4.5", "valid_num_updates": "4148", "valid_best_loss": "3.88"}
[2022-01-13 10:23:41,044][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 4148 updates
[2022-01-13 10:23:41,044][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:23:45,032][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:23:45,046][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 34 @ 4148 updates, score 4.099) (writing took 4.002183626405895 seconds)
[2022-01-13 10:23:45,048][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2022-01-13 10:23:45,063][train][INFO] - {"epoch": 34, "train_loss": "4.058", "train_ntokens": "1802.25", "train_nsentences": "4.97541", "train_prob_perplexity": "20.467", "train_code_perplexity": "20.46", "train_temp": "1.96", "train_loss_0": "3.889", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.39883", "train_wps": "6793.4", "train_ups": "3.77", "train_wpb": "1802.3", "train_bsz": "5", "train_num_updates": "4148", "train_lr": "6.48125e-05", "train_gnorm": "1.636", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1193"}
[2022-01-13 10:23:45,145][fairseq.trainer][INFO] - begin training epoch 35
[2022-01-13 10:23:45,146][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:23:57,187][train_inner][INFO] - {"epoch": 35, "update": 34.426, "loss": "4.043", "ntokens": "1805.67", "nsentences": "4.985", "prob_perplexity": "20.315", "code_perplexity": "20.309", "temp": "1.959", "loss_0": "3.875", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.40315", "wps": "6589.9", "ups": "3.65", "wpb": "1805.7", "bsz": "5", "num_updates": "4200", "lr": "6.5625e-05", "gnorm": "1.64", "clip": "0", "train_wall": "45", "gb_free": "15.1", "wall": "1205"}
[2022-01-13 10:24:12,818][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:24:13,305][valid][INFO] - {"epoch": 35, "valid_loss": "4.096", "valid_ntokens": "1668.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.957", "valid_code_perplexity": "21.966", "valid_temp": "1.958", "valid_loss_0": "3.929", "valid_loss_1": "0.139", "valid_loss_2": "0.027", "valid_accuracy": "0.39556", "valid_wps": "29671.2", "valid_wpb": "1668.5", "valid_bsz": "4.5", "valid_num_updates": "4270", "valid_best_loss": "3.88"}
[2022-01-13 10:24:13,308][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 4270 updates
[2022-01-13 10:24:13,309][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:24:17,221][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:24:17,240][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 35 @ 4270 updates, score 4.096) (writing took 3.932067269459367 seconds)
[2022-01-13 10:24:17,241][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2022-01-13 10:24:17,255][train][INFO] - {"epoch": 35, "train_loss": "4.018", "train_ntokens": "1796.12", "train_nsentences": "4.97541", "train_prob_perplexity": "20.045", "train_code_perplexity": "20.037", "train_temp": "1.958", "train_loss_0": "3.85", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.4112", "train_wps": "6809.9", "train_ups": "3.79", "train_wpb": "1796.1", "train_bsz": "5", "train_num_updates": "4270", "train_lr": "6.67187e-05", "train_gnorm": "1.656", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "1225"}
[2022-01-13 10:24:17,328][fairseq.trainer][INFO] - begin training epoch 36
[2022-01-13 10:24:17,329][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:24:45,150][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:24:45,603][valid][INFO] - {"epoch": 36, "valid_loss": "4.008", "valid_ntokens": "1689", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.672", "valid_code_perplexity": "19.673", "valid_temp": "1.957", "valid_loss_0": "3.84", "valid_loss_1": "0.14", "valid_loss_2": "0.028", "valid_accuracy": "0.40586", "valid_wps": "29051.5", "valid_wpb": "1689", "valid_bsz": "4.5", "valid_num_updates": "4392", "valid_best_loss": "3.88"}
[2022-01-13 10:24:45,605][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 4392 updates
[2022-01-13 10:24:45,606][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:24:49,644][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:24:49,659][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 36 @ 4392 updates, score 4.008) (writing took 4.053669863380492 seconds)
[2022-01-13 10:24:49,659][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2022-01-13 10:24:49,673][train][INFO] - {"epoch": 36, "train_loss": "4.032", "train_ntokens": "1808.33", "train_nsentences": "4.97541", "train_prob_perplexity": "20.149", "train_code_perplexity": "20.141", "train_temp": "1.957", "train_loss_0": "3.863", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.40536", "train_wps": "6808.1", "train_ups": "3.76", "train_wpb": "1808.3", "train_bsz": "5", "train_num_updates": "4392", "train_lr": "6.8625e-05", "train_gnorm": "1.593", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1257"}
[2022-01-13 10:24:49,751][fairseq.trainer][INFO] - begin training epoch 37
[2022-01-13 10:24:49,752][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:24:51,879][train_inner][INFO] - {"epoch": 37, "update": 36.066, "loss": "4.028", "ntokens": "1799.34", "nsentences": "4.97", "prob_perplexity": "20.176", "code_perplexity": "20.167", "temp": "1.957", "loss_0": "3.86", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.40706", "wps": "6581.5", "ups": "3.66", "wpb": "1799.3", "bsz": "5", "num_updates": "4400", "lr": "6.875e-05", "gnorm": "1.626", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1259"}
[2022-01-13 10:25:17,496][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:25:17,981][valid][INFO] - {"epoch": 37, "valid_loss": "4.046", "valid_ntokens": "1713.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "17.65", "valid_code_perplexity": "17.64", "valid_temp": "1.955", "valid_loss_0": "3.875", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.41436", "valid_wps": "28123.6", "valid_wpb": "1713.5", "valid_bsz": "4.5", "valid_num_updates": "4514", "valid_best_loss": "3.88"}
[2022-01-13 10:25:17,982][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 4514 updates
[2022-01-13 10:25:17,983][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:25:21,878][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:25:21,895][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 37 @ 4514 updates, score 4.046) (writing took 3.91292442381382 seconds)
[2022-01-13 10:25:21,896][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2022-01-13 10:25:21,909][train][INFO] - {"epoch": 37, "train_loss": "4.033", "train_ntokens": "1809.52", "train_nsentences": "4.97541", "train_prob_perplexity": "20.128", "train_code_perplexity": "20.122", "train_temp": "1.956", "train_loss_0": "3.865", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.40715", "train_wps": "6851", "train_ups": "3.79", "train_wpb": "1809.5", "train_bsz": "5", "train_num_updates": "4514", "train_lr": "7.05313e-05", "train_gnorm": "1.59", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.5", "train_wall": "1290"}
[2022-01-13 10:25:21,964][fairseq.trainer][INFO] - begin training epoch 38
[2022-01-13 10:25:21,964][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:25:41,703][train_inner][INFO] - {"epoch": 38, "update": 37.705, "loss": "4.03", "ntokens": "1809.44", "nsentences": "4.97", "prob_perplexity": "20.252", "code_perplexity": "20.246", "temp": "1.956", "loss_0": "3.862", "loss_1": "0.14", "loss_2": "0.028", "accuracy": "0.40549", "wps": "7265.2", "ups": "4.02", "wpb": "1809.4", "bsz": "5", "num_updates": "4600", "lr": "7.1875e-05", "gnorm": "1.58", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1309"}
[2022-01-13 10:25:49,763][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:25:50,251][valid][INFO] - {"epoch": 38, "valid_loss": "3.83", "valid_ntokens": "1580", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.876", "valid_code_perplexity": "19.858", "valid_temp": "1.954", "valid_loss_0": "3.663", "valid_loss_1": "0.14", "valid_loss_2": "0.027", "valid_accuracy": "0.4443", "valid_wps": "26274.3", "valid_wpb": "1580", "valid_bsz": "4.5", "valid_num_updates": "4636", "valid_best_loss": "3.83"}
[2022-01-13 10:25:50,253][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 4636 updates
[2022-01-13 10:25:50,253][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:25:54,203][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:26:01,126][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 38 @ 4636 updates, score 3.83) (writing took 10.873459144495428 seconds)
[2022-01-13 10:26:01,127][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2022-01-13 10:26:01,145][train][INFO] - {"epoch": 38, "train_loss": "4.02", "train_ntokens": "1808.51", "train_nsentences": "4.97541", "train_prob_perplexity": "20.172", "train_code_perplexity": "20.166", "train_temp": "1.955", "train_loss_0": "3.852", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.40528", "train_wps": "5625.9", "train_ups": "3.11", "train_wpb": "1808.5", "train_bsz": "5", "train_num_updates": "4636", "train_lr": "7.24375e-05", "train_gnorm": "1.552", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1329"}
[2022-01-13 10:26:01,197][fairseq.trainer][INFO] - begin training epoch 39
[2022-01-13 10:26:01,198][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:26:28,971][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:26:29,463][valid][INFO] - {"epoch": 39, "valid_loss": "3.969", "valid_ntokens": "1700.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.051", "valid_code_perplexity": "21.04", "valid_temp": "1.953", "valid_loss_0": "3.8", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.40576", "valid_wps": "29833.9", "valid_wpb": "1700.5", "valid_bsz": "4.5", "valid_num_updates": "4758", "valid_best_loss": "3.83"}
[2022-01-13 10:26:29,466][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 4758 updates
[2022-01-13 10:26:29,466][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:26:33,387][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:26:33,407][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 39 @ 4758 updates, score 3.969) (writing took 3.9416907113045454 seconds)
[2022-01-13 10:26:33,408][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2022-01-13 10:26:33,420][train][INFO] - {"epoch": 39, "train_loss": "4.007", "train_ntokens": "1803.24", "train_nsentences": "4.97541", "train_prob_perplexity": "20.006", "train_code_perplexity": "20.001", "train_temp": "1.954", "train_loss_0": "3.839", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.41013", "train_wps": "6818.9", "train_ups": "3.78", "train_wpb": "1803.2", "train_bsz": "5", "train_num_updates": "4758", "train_lr": "7.43438e-05", "train_gnorm": "1.542", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1361"}
[2022-01-13 10:26:33,503][fairseq.trainer][INFO] - begin training epoch 40
[2022-01-13 10:26:33,504][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:26:43,140][train_inner][INFO] - {"epoch": 40, "update": 39.344, "loss": "4.009", "ntokens": "1800.95", "nsentences": "4.97", "prob_perplexity": "20.057", "code_perplexity": "20.049", "temp": "1.954", "loss_0": "3.84", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.40969", "wps": "5864", "ups": "3.26", "wpb": "1801", "bsz": "5", "num_updates": "4800", "lr": "7.5e-05", "gnorm": "1.54", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "1371"}
[2022-01-13 10:27:01,038][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:27:01,489][valid][INFO] - {"epoch": 40, "valid_loss": "4.075", "valid_ntokens": "1636", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.406", "valid_code_perplexity": "20.396", "valid_temp": "1.952", "valid_loss_0": "3.905", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.40801", "valid_wps": "29113.4", "valid_wpb": "1636", "valid_bsz": "4.5", "valid_num_updates": "4880", "valid_best_loss": "3.83"}
[2022-01-13 10:27:01,491][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 4880 updates
[2022-01-13 10:27:01,492][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:27:05,399][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:27:05,422][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 40 @ 4880 updates, score 4.075) (writing took 3.931295587681234 seconds)
[2022-01-13 10:27:05,423][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2022-01-13 10:27:05,436][train][INFO] - {"epoch": 40, "train_loss": "3.996", "train_ntokens": "1798.77", "train_nsentences": "4.97541", "train_prob_perplexity": "20.381", "train_code_perplexity": "20.373", "train_temp": "1.952", "train_loss_0": "3.826", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.40941", "train_wps": "6857.3", "train_ups": "3.81", "train_wpb": "1798.8", "train_bsz": "5", "train_num_updates": "4880", "train_lr": "7.625e-05", "train_gnorm": "1.527", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1393"}
[2022-01-13 10:27:05,497][fairseq.trainer][INFO] - begin training epoch 41
[2022-01-13 10:27:05,497][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:27:33,138][train_inner][INFO] - {"epoch": 41, "update": 40.984, "loss": "3.985", "ntokens": "1801.71", "nsentences": "4.985", "prob_perplexity": "20.27", "code_perplexity": "20.264", "temp": "1.952", "loss_0": "3.816", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.41205", "wps": "7209", "ups": "4", "wpb": "1801.7", "bsz": "5", "num_updates": "5000", "lr": "7.8125e-05", "gnorm": "1.511", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1421"}
[2022-01-13 10:27:33,605][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:27:34,071][valid][INFO] - {"epoch": 41, "valid_loss": "3.842", "valid_ntokens": "1554.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.848", "valid_code_perplexity": "19.832", "valid_temp": "1.951", "valid_loss_0": "3.673", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.43487", "valid_wps": "27566", "valid_wpb": "1554.5", "valid_bsz": "4.5", "valid_num_updates": "5002", "valid_best_loss": "3.83"}
[2022-01-13 10:27:34,073][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 5002 updates
[2022-01-13 10:27:34,074][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:27:37,969][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:27:37,988][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 41 @ 5002 updates, score 3.842) (writing took 3.914730138145387 seconds)
[2022-01-13 10:27:37,988][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2022-01-13 10:27:38,000][train][INFO] - {"epoch": 41, "train_loss": "3.985", "train_ntokens": "1799.29", "train_nsentences": "4.97541", "train_prob_perplexity": "20.214", "train_code_perplexity": "20.207", "train_temp": "1.951", "train_loss_0": "3.816", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41367", "train_wps": "6743.4", "train_ups": "3.75", "train_wpb": "1799.3", "train_bsz": "5", "train_num_updates": "5002", "train_lr": "7.81562e-05", "train_gnorm": "1.509", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1426"}
[2022-01-13 10:27:38,039][fairseq.trainer][INFO] - begin training epoch 42
[2022-01-13 10:27:38,039][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:28:06,156][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:28:06,620][valid][INFO] - {"epoch": 42, "valid_loss": "3.99", "valid_ntokens": "1651.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.672", "valid_code_perplexity": "20.668", "valid_temp": "1.949", "valid_loss_0": "3.821", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.40357", "valid_wps": "27706.8", "valid_wpb": "1651.5", "valid_bsz": "4.5", "valid_num_updates": "5124", "valid_best_loss": "3.83"}
[2022-01-13 10:28:06,622][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 5124 updates
[2022-01-13 10:28:06,623][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:28:10,542][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:28:10,562][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 42 @ 5124 updates, score 3.99) (writing took 3.9400534220039845 seconds)
[2022-01-13 10:28:10,563][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2022-01-13 10:28:10,576][train][INFO] - {"epoch": 42, "train_loss": "3.983", "train_ntokens": "1804.75", "train_nsentences": "4.97541", "train_prob_perplexity": "20.416", "train_code_perplexity": "20.411", "train_temp": "1.95", "train_loss_0": "3.815", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41296", "train_wps": "6761.8", "train_ups": "3.75", "train_wpb": "1804.7", "train_bsz": "5", "train_num_updates": "5124", "train_lr": "8.00625e-05", "train_gnorm": "1.486", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1458"}
[2022-01-13 10:28:10,649][fairseq.trainer][INFO] - begin training epoch 43
[2022-01-13 10:28:10,651][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:28:28,285][train_inner][INFO] - {"epoch": 43, "update": 42.623, "loss": "3.985", "ntokens": "1801.63", "nsentences": "4.97", "prob_perplexity": "20.431", "code_perplexity": "20.426", "temp": "1.95", "loss_0": "3.816", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.41229", "wps": "6535.4", "ups": "3.63", "wpb": "1801.6", "bsz": "5", "num_updates": "5200", "lr": "8.125e-05", "gnorm": "1.474", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "1476"}
[2022-01-13 10:28:38,687][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:28:39,157][valid][INFO] - {"epoch": 43, "valid_loss": "4.01", "valid_ntokens": "1668.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.753", "valid_code_perplexity": "19.755", "valid_temp": "1.948", "valid_loss_0": "3.84", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.42373", "valid_wps": "28662.6", "valid_wpb": "1668.5", "valid_bsz": "4.5", "valid_num_updates": "5246", "valid_best_loss": "3.83"}
[2022-01-13 10:28:39,159][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 5246 updates
[2022-01-13 10:28:39,160][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:28:43,075][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:28:43,101][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 43 @ 5246 updates, score 4.01) (writing took 3.9416321869939566 seconds)
[2022-01-13 10:28:43,102][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2022-01-13 10:28:43,115][train][INFO] - {"epoch": 43, "train_loss": "3.979", "train_ntokens": "1796.16", "train_nsentences": "4.97541", "train_prob_perplexity": "20.44", "train_code_perplexity": "20.434", "train_temp": "1.949", "train_loss_0": "3.809", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41412", "train_wps": "6737.1", "train_ups": "3.75", "train_wpb": "1796.2", "train_bsz": "5", "train_num_updates": "5246", "train_lr": "8.19688e-05", "train_gnorm": "1.457", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1491"}
[2022-01-13 10:28:43,167][fairseq.trainer][INFO] - begin training epoch 44
[2022-01-13 10:28:43,168][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:29:10,781][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:29:11,247][valid][INFO] - {"epoch": 44, "valid_loss": "4.001", "valid_ntokens": "1683", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.047", "valid_code_perplexity": "20.058", "valid_temp": "1.947", "valid_loss_0": "3.833", "valid_loss_1": "0.14", "valid_loss_2": "0.028", "valid_accuracy": "0.40879", "valid_wps": "29164.3", "valid_wpb": "1683", "valid_bsz": "4.5", "valid_num_updates": "5368", "valid_best_loss": "3.83"}
[2022-01-13 10:29:11,249][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 5368 updates
[2022-01-13 10:29:11,250][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:29:15,158][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:29:15,182][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 44 @ 5368 updates, score 4.001) (writing took 3.9322580359876156 seconds)
[2022-01-13 10:29:15,182][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2022-01-13 10:29:15,196][train][INFO] - {"epoch": 44, "train_loss": "3.972", "train_ntokens": "1804.01", "train_nsentences": "4.97541", "train_prob_perplexity": "20.491", "train_code_perplexity": "20.487", "train_temp": "1.948", "train_loss_0": "3.802", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41417", "train_wps": "6863.4", "train_ups": "3.8", "train_wpb": "1804", "train_bsz": "5", "train_num_updates": "5368", "train_lr": "8.3875e-05", "train_gnorm": "1.461", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "15.1", "train_wall": "1523"}
[2022-01-13 10:29:15,263][fairseq.trainer][INFO] - begin training epoch 45
[2022-01-13 10:29:15,264][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:29:22,840][train_inner][INFO] - {"epoch": 45, "update": 44.262, "loss": "3.966", "ntokens": "1806.07", "nsentences": "4.985", "prob_perplexity": "20.256", "code_perplexity": "20.251", "temp": "1.948", "loss_0": "3.796", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.41746", "wps": "6622.7", "ups": "3.67", "wpb": "1806.1", "bsz": "5", "num_updates": "5400", "lr": "8.4375e-05", "gnorm": "1.464", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "1530"}
[2022-01-13 10:29:43,185][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:29:43,630][valid][INFO] - {"epoch": 45, "valid_loss": "3.835", "valid_ntokens": "1584.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.245", "valid_code_perplexity": "19.254", "valid_temp": "1.946", "valid_loss_0": "3.664", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.45062", "valid_wps": "26734.6", "valid_wpb": "1584.5", "valid_bsz": "4.5", "valid_num_updates": "5490", "valid_best_loss": "3.83"}
[2022-01-13 10:29:43,632][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 5490 updates
[2022-01-13 10:29:43,633][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:29:47,496][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:29:47,513][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 45 @ 5490 updates, score 3.835) (writing took 3.8803105782717466 seconds)
[2022-01-13 10:29:47,513][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2022-01-13 10:29:47,528][train][INFO] - {"epoch": 45, "train_loss": "3.958", "train_ntokens": "1807.76", "train_nsentences": "4.97541", "train_prob_perplexity": "19.821", "train_code_perplexity": "19.816", "train_temp": "1.946", "train_loss_0": "3.788", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.42006", "train_wps": "6824.6", "train_ups": "3.78", "train_wpb": "1807.8", "train_bsz": "5", "train_num_updates": "5490", "train_lr": "8.57813e-05", "train_gnorm": "1.457", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1555"}
[2022-01-13 10:29:47,582][fairseq.trainer][INFO] - begin training epoch 46
[2022-01-13 10:29:47,582][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:30:12,948][train_inner][INFO] - {"epoch": 46, "update": 45.902, "loss": "3.964", "ntokens": "1809.21", "nsentences": "4.985", "prob_perplexity": "20.042", "code_perplexity": "20.037", "temp": "1.946", "loss_0": "3.795", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.41738", "wps": "7223", "ups": "3.99", "wpb": "1809.2", "bsz": "5", "num_updates": "5600", "lr": "8.75e-05", "gnorm": "1.425", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1581"}
[2022-01-13 10:30:15,516][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:30:15,978][valid][INFO] - {"epoch": 46, "valid_loss": "3.87", "valid_ntokens": "1640", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.446", "valid_code_perplexity": "19.422", "valid_temp": "1.945", "valid_loss_0": "3.7", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.43323", "valid_wps": "26896.9", "valid_wpb": "1640", "valid_bsz": "4.5", "valid_num_updates": "5612", "valid_best_loss": "3.83"}
[2022-01-13 10:30:15,980][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 5612 updates
[2022-01-13 10:30:15,981][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:30:19,965][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:30:19,991][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 46 @ 5612 updates, score 3.87) (writing took 4.011130691505969 seconds)
[2022-01-13 10:30:19,992][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2022-01-13 10:30:20,006][train][INFO] - {"epoch": 46, "train_loss": "3.959", "train_ntokens": "1806.31", "train_nsentences": "4.97541", "train_prob_perplexity": "20.073", "train_code_perplexity": "20.067", "train_temp": "1.945", "train_loss_0": "3.789", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41802", "train_wps": "6788", "train_ups": "3.76", "train_wpb": "1806.3", "train_bsz": "5", "train_num_updates": "5612", "train_lr": "8.76875e-05", "train_gnorm": "1.414", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1588"}
[2022-01-13 10:30:20,089][fairseq.trainer][INFO] - begin training epoch 47
[2022-01-13 10:30:20,090][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:30:48,029][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:30:48,508][valid][INFO] - {"epoch": 47, "valid_loss": "3.9", "valid_ntokens": "1645", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.523", "valid_code_perplexity": "19.527", "valid_temp": "1.943", "valid_loss_0": "3.73", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.43526", "valid_wps": "27306.8", "valid_wpb": "1645", "valid_bsz": "4.5", "valid_num_updates": "5734", "valid_best_loss": "3.83"}
[2022-01-13 10:30:48,510][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 5734 updates
[2022-01-13 10:30:48,511][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:30:52,398][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:30:52,422][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 47 @ 5734 updates, score 3.9) (writing took 3.9118852270767093 seconds)
[2022-01-13 10:30:52,423][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2022-01-13 10:30:52,436][train][INFO] - {"epoch": 47, "train_loss": "3.956", "train_ntokens": "1803.79", "train_nsentences": "4.97541", "train_prob_perplexity": "20.737", "train_code_perplexity": "20.733", "train_temp": "1.944", "train_loss_0": "3.788", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.41411", "train_wps": "6788.6", "train_ups": "3.76", "train_wpb": "1803.8", "train_bsz": "5", "train_num_updates": "5734", "train_lr": "8.95938e-05", "train_gnorm": "1.388", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1620"}
[2022-01-13 10:30:52,489][fairseq.trainer][INFO] - begin training epoch 48
[2022-01-13 10:30:52,490][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:31:07,648][train_inner][INFO] - {"epoch": 48, "update": 47.541, "loss": "3.953", "ntokens": "1797.14", "nsentences": "4.955", "prob_perplexity": "20.546", "code_perplexity": "20.543", "temp": "1.944", "loss_0": "3.785", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.41548", "wps": "6572.4", "ups": "3.66", "wpb": "1797.1", "bsz": "5", "num_updates": "5800", "lr": "9.0625e-05", "gnorm": "1.401", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1635"}
[2022-01-13 10:31:20,224][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:31:20,671][valid][INFO] - {"epoch": 48, "valid_loss": "4", "valid_ntokens": "1678.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.504", "valid_code_perplexity": "18.501", "valid_temp": "1.942", "valid_loss_0": "3.831", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.41406", "valid_wps": "29470.9", "valid_wpb": "1678.5", "valid_bsz": "4.5", "valid_num_updates": "5856", "valid_best_loss": "3.83"}
[2022-01-13 10:31:20,672][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 5856 updates
[2022-01-13 10:31:20,673][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:31:24,548][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:31:24,569][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 48 @ 5856 updates, score 4.0) (writing took 3.896481093019247 seconds)
[2022-01-13 10:31:24,569][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2022-01-13 10:31:24,582][train][INFO] - {"epoch": 48, "train_loss": "3.945", "train_ntokens": "1800.34", "train_nsentences": "4.97541", "train_prob_perplexity": "19.998", "train_code_perplexity": "19.996", "train_temp": "1.943", "train_loss_0": "3.775", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41851", "train_wps": "6835.4", "train_ups": "3.8", "train_wpb": "1800.3", "train_bsz": "5", "train_num_updates": "5856", "train_lr": "9.15e-05", "train_gnorm": "1.407", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "1652"}
[2022-01-13 10:31:24,650][fairseq.trainer][INFO] - begin training epoch 49
[2022-01-13 10:31:24,650][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:31:52,344][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:31:52,782][valid][INFO] - {"epoch": 49, "valid_loss": "3.964", "valid_ntokens": "1644.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.523", "valid_code_perplexity": "20.508", "valid_temp": "1.941", "valid_loss_0": "3.794", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.41441", "valid_wps": "29500.1", "valid_wpb": "1644.5", "valid_bsz": "4.5", "valid_num_updates": "5978", "valid_best_loss": "3.83"}
[2022-01-13 10:31:52,784][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 5978 updates
[2022-01-13 10:31:52,785][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:31:56,666][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:31:56,689][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 49 @ 5978 updates, score 3.964) (writing took 3.904388398863375 seconds)
[2022-01-13 10:31:56,689][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2022-01-13 10:31:56,702][train][INFO] - {"epoch": 49, "train_loss": "3.943", "train_ntokens": "1788.85", "train_nsentences": "4.97541", "train_prob_perplexity": "20.111", "train_code_perplexity": "20.107", "train_temp": "1.942", "train_loss_0": "3.774", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41843", "train_wps": "6797.3", "train_ups": "3.8", "train_wpb": "1788.9", "train_bsz": "5", "train_num_updates": "5978", "train_lr": "9.34062e-05", "train_gnorm": "1.399", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1684"}
[2022-01-13 10:31:56,770][fairseq.trainer][INFO] - begin training epoch 50
[2022-01-13 10:31:56,771][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:32:02,079][train_inner][INFO] - {"epoch": 50, "update": 49.18, "loss": "3.932", "ntokens": "1794.38", "nsentences": "4.985", "prob_perplexity": "20.006", "code_perplexity": "20.003", "temp": "1.942", "loss_0": "3.764", "loss_1": "0.14", "loss_2": "0.028", "accuracy": "0.41988", "wps": "6594.8", "ups": "3.68", "wpb": "1794.4", "bsz": "5", "num_updates": "6000", "lr": "9.375e-05", "gnorm": "1.397", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1690"}
[2022-01-13 10:32:24,661][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:32:25,137][valid][INFO] - {"epoch": 50, "valid_loss": "3.952", "valid_ntokens": "1651.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.986", "valid_code_perplexity": "20.987", "valid_temp": "1.94", "valid_loss_0": "3.781", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.41144", "valid_wps": "28303.5", "valid_wpb": "1651.5", "valid_bsz": "4.5", "valid_num_updates": "6100", "valid_best_loss": "3.83"}
[2022-01-13 10:32:25,139][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 6100 updates
[2022-01-13 10:32:25,139][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:32:29,033][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:32:29,054][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 50 @ 6100 updates, score 3.952) (writing took 3.91567740496248 seconds)
[2022-01-13 10:32:29,055][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2022-01-13 10:32:29,068][train][INFO] - {"epoch": 50, "train_loss": "3.929", "train_ntokens": "1796.07", "train_nsentences": "4.97541", "train_prob_perplexity": "20.016", "train_code_perplexity": "20.007", "train_temp": "1.941", "train_loss_0": "3.76", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.42131", "train_wps": "6772.9", "train_ups": "3.77", "train_wpb": "1796.1", "train_bsz": "5", "train_num_updates": "6100", "train_lr": "9.53125e-05", "train_gnorm": "1.365", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1717"}
[2022-01-13 10:32:29,122][fairseq.trainer][INFO] - begin training epoch 51
[2022-01-13 10:32:29,123][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:32:51,826][train_inner][INFO] - {"epoch": 51, "update": 50.82, "loss": "3.933", "ntokens": "1793.2", "nsentences": "4.97", "prob_perplexity": "20.123", "code_perplexity": "20.115", "temp": "1.94", "loss_0": "3.763", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.4205", "wps": "7211.3", "ups": "4.02", "wpb": "1793.2", "bsz": "5", "num_updates": "6200", "lr": "9.6875e-05", "gnorm": "1.367", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1739"}
[2022-01-13 10:32:56,752][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:32:57,238][valid][INFO] - {"epoch": 51, "valid_loss": "3.817", "valid_ntokens": "1667", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.275", "valid_code_perplexity": "21.272", "valid_temp": "1.939", "valid_loss_0": "3.644", "valid_loss_1": "0.139", "valid_loss_2": "0.034", "valid_accuracy": "0.43101", "valid_wps": "30912.4", "valid_wpb": "1667", "valid_bsz": "4.5", "valid_num_updates": "6222", "valid_best_loss": "3.817"}
[2022-01-13 10:32:57,240][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 6222 updates
[2022-01-13 10:32:57,240][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:33:01,157][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:33:07,979][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 51 @ 6222 updates, score 3.817) (writing took 10.73971288651228 seconds)
[2022-01-13 10:33:07,981][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2022-01-13 10:33:07,994][train][INFO] - {"epoch": 51, "train_loss": "3.931", "train_ntokens": "1792.16", "train_nsentences": "4.97541", "train_prob_perplexity": "20.314", "train_code_perplexity": "20.309", "train_temp": "1.939", "train_loss_0": "3.76", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.41981", "train_wps": "5618.8", "train_ups": "3.14", "train_wpb": "1792.2", "train_bsz": "5", "train_num_updates": "6222", "train_lr": "9.72188e-05", "train_gnorm": "1.364", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1756"}
[2022-01-13 10:33:08,093][fairseq.trainer][INFO] - begin training epoch 52
[2022-01-13 10:33:08,094][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:33:35,817][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:33:36,276][valid][INFO] - {"epoch": 52, "valid_loss": "3.919", "valid_ntokens": "1608.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.836", "valid_code_perplexity": "19.831", "valid_temp": "1.938", "valid_loss_0": "3.749", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.4212", "valid_wps": "28626.5", "valid_wpb": "1608.5", "valid_bsz": "4.5", "valid_num_updates": "6344", "valid_best_loss": "3.817"}
[2022-01-13 10:33:36,278][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 6344 updates
[2022-01-13 10:33:36,279][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:33:40,226][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:33:40,244][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 52 @ 6344 updates, score 3.919) (writing took 3.9659382440149784 seconds)
[2022-01-13 10:33:40,245][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2022-01-13 10:33:40,258][train][INFO] - {"epoch": 52, "train_loss": "3.94", "train_ntokens": "1801.72", "train_nsentences": "4.97541", "train_prob_perplexity": "20.135", "train_code_perplexity": "20.132", "train_temp": "1.938", "train_loss_0": "3.769", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.41812", "train_wps": "6815.7", "train_ups": "3.78", "train_wpb": "1801.7", "train_bsz": "5", "train_num_updates": "6344", "train_lr": "9.9125e-05", "train_gnorm": "1.349", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1788"}
[2022-01-13 10:33:40,326][fairseq.trainer][INFO] - begin training epoch 53
[2022-01-13 10:33:40,327][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:33:53,120][train_inner][INFO] - {"epoch": 53, "update": 52.459, "loss": "3.934", "ntokens": "1795.47", "nsentences": "4.97", "prob_perplexity": "20.258", "code_perplexity": "20.254", "temp": "1.938", "loss_0": "3.763", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.41875", "wps": "5859.9", "ups": "3.26", "wpb": "1795.5", "bsz": "5", "num_updates": "6400", "lr": "0.0001", "gnorm": "1.338", "clip": "0", "train_wall": "44", "gb_free": "13.4", "wall": "1801"}
[2022-01-13 10:34:07,958][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:34:08,410][valid][INFO] - {"epoch": 53, "valid_loss": "4.016", "valid_ntokens": "1680.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.892", "valid_code_perplexity": "19.895", "valid_temp": "1.936", "valid_loss_0": "3.845", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.41833", "valid_wps": "28714.4", "valid_wpb": "1680.5", "valid_bsz": "4.5", "valid_num_updates": "6466", "valid_best_loss": "3.817"}
[2022-01-13 10:34:08,412][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 6466 updates
[2022-01-13 10:34:08,413][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:34:12,301][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:34:12,326][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 53 @ 6466 updates, score 4.016) (writing took 3.9136689323931932 seconds)
[2022-01-13 10:34:12,327][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2022-01-13 10:34:12,341][train][INFO] - {"epoch": 53, "train_loss": "3.928", "train_ntokens": "1806.84", "train_nsentences": "4.97541", "train_prob_perplexity": "20.395", "train_code_perplexity": "20.389", "train_temp": "1.937", "train_loss_0": "3.759", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41767", "train_wps": "6873.9", "train_ups": "3.8", "train_wpb": "1806.8", "train_bsz": "5", "train_num_updates": "6466", "train_lr": "0.000101031", "train_gnorm": "1.305", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1820"}
[2022-01-13 10:34:12,397][fairseq.trainer][INFO] - begin training epoch 54
[2022-01-13 10:34:12,398][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:34:40,129][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:34:40,647][valid][INFO] - {"epoch": 54, "valid_loss": "3.778", "valid_ntokens": "1579.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.025", "valid_code_perplexity": "19.006", "valid_temp": "1.935", "valid_loss_0": "3.607", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.4416", "valid_wps": "25722.3", "valid_wpb": "1579.5", "valid_bsz": "4.5", "valid_num_updates": "6588", "valid_best_loss": "3.778"}
[2022-01-13 10:34:40,649][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 6588 updates
[2022-01-13 10:34:40,649][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:34:44,556][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:34:51,105][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 54 @ 6588 updates, score 3.778) (writing took 10.455805683508515 seconds)
[2022-01-13 10:34:51,105][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2022-01-13 10:34:51,119][train][INFO] - {"epoch": 54, "train_loss": "3.906", "train_ntokens": "1802.9", "train_nsentences": "4.97541", "train_prob_perplexity": "20.399", "train_code_perplexity": "20.395", "train_temp": "1.936", "train_loss_0": "3.737", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.42355", "train_wps": "5674.1", "train_ups": "3.15", "train_wpb": "1802.9", "train_bsz": "5", "train_num_updates": "6588", "train_lr": "0.000102938", "train_gnorm": "1.346", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1859"}
[2022-01-13 10:34:51,190][fairseq.trainer][INFO] - begin training epoch 55
[2022-01-13 10:34:51,191][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:34:54,264][train_inner][INFO] - {"epoch": 55, "update": 54.098, "loss": "3.914", "ntokens": "1811.69", "nsentences": "4.985", "prob_perplexity": "20.384", "code_perplexity": "20.379", "temp": "1.936", "loss_0": "3.745", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.42098", "wps": "5927.3", "ups": "3.27", "wpb": "1811.7", "bsz": "5", "num_updates": "6600", "lr": "0.000103125", "gnorm": "1.333", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1862"}
[2022-01-13 10:35:18,900][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:35:19,461][valid][INFO] - {"epoch": 55, "valid_loss": "4.054", "valid_ntokens": "1648", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.218", "valid_code_perplexity": "21.221", "valid_temp": "1.934", "valid_loss_0": "3.883", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.38865", "valid_wps": "28961.3", "valid_wpb": "1648", "valid_bsz": "4.5", "valid_num_updates": "6710", "valid_best_loss": "3.778"}
[2022-01-13 10:35:19,463][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 6710 updates
[2022-01-13 10:35:19,463][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:35:23,306][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:35:23,316][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 55 @ 6710 updates, score 4.054) (writing took 3.8535029254853725 seconds)
[2022-01-13 10:35:23,317][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2022-01-13 10:35:23,329][train][INFO] - {"epoch": 55, "train_loss": "3.92", "train_ntokens": "1812.07", "train_nsentences": "4.97541", "train_prob_perplexity": "19.942", "train_code_perplexity": "19.938", "train_temp": "1.935", "train_loss_0": "3.75", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.42264", "train_wps": "6866.1", "train_ups": "3.79", "train_wpb": "1812.1", "train_bsz": "5", "train_num_updates": "6710", "train_lr": "0.000104844", "train_gnorm": "1.329", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.5", "train_wall": "1891"}
[2022-01-13 10:35:23,376][fairseq.trainer][INFO] - begin training epoch 56
[2022-01-13 10:35:23,377][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:35:44,000][train_inner][INFO] - {"epoch": 56, "update": 55.738, "loss": "3.929", "ntokens": "1812.46", "nsentences": "4.985", "prob_perplexity": "20.074", "code_perplexity": "20.069", "temp": "1.934", "loss_0": "3.759", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.41973", "wps": "7290.3", "ups": "4.02", "wpb": "1812.5", "bsz": "5", "num_updates": "6800", "lr": "0.00010625", "gnorm": "1.304", "clip": "0", "train_wall": "44", "gb_free": "13.4", "wall": "1912"}
[2022-01-13 10:35:51,087][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:35:51,603][valid][INFO] - {"epoch": 56, "valid_loss": "3.908", "valid_ntokens": "1662", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.89", "valid_code_perplexity": "18.867", "valid_temp": "1.933", "valid_loss_0": "3.737", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.4278", "valid_wps": "27381.7", "valid_wpb": "1662", "valid_bsz": "4.5", "valid_num_updates": "6832", "valid_best_loss": "3.778"}
[2022-01-13 10:35:51,605][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 6832 updates
[2022-01-13 10:35:51,605][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:35:55,497][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:35:55,522][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 56 @ 6832 updates, score 3.908) (writing took 3.9176850616931915 seconds)
[2022-01-13 10:35:55,523][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2022-01-13 10:35:55,536][train][INFO] - {"epoch": 56, "train_loss": "3.933", "train_ntokens": "1803.66", "train_nsentences": "4.97541", "train_prob_perplexity": "20.333", "train_code_perplexity": "20.327", "train_temp": "1.933", "train_loss_0": "3.763", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41627", "train_wps": "6835.1", "train_ups": "3.79", "train_wpb": "1803.7", "train_bsz": "5", "train_num_updates": "6832", "train_lr": "0.00010675", "train_gnorm": "1.282", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "1923"}
[2022-01-13 10:35:55,614][fairseq.trainer][INFO] - begin training epoch 57
[2022-01-13 10:35:55,615][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:36:23,382][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:36:23,914][valid][INFO] - {"epoch": 57, "valid_loss": "3.919", "valid_ntokens": "1643", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.98", "valid_code_perplexity": "20.991", "valid_temp": "1.932", "valid_loss_0": "3.748", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.41692", "valid_wps": "29298.1", "valid_wpb": "1643", "valid_bsz": "4.5", "valid_num_updates": "6954", "valid_best_loss": "3.778"}
[2022-01-13 10:36:23,916][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 6954 updates
[2022-01-13 10:36:23,917][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:36:27,790][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:36:27,813][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 57 @ 6954 updates, score 3.919) (writing took 3.8963870434090495 seconds)
[2022-01-13 10:36:27,813][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2022-01-13 10:36:27,826][train][INFO] - {"epoch": 57, "train_loss": "3.908", "train_ntokens": "1802.51", "train_nsentences": "4.97541", "train_prob_perplexity": "20.026", "train_code_perplexity": "20.021", "train_temp": "1.932", "train_loss_0": "3.738", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.42442", "train_wps": "6813", "train_ups": "3.78", "train_wpb": "1802.5", "train_bsz": "5", "train_num_updates": "6954", "train_lr": "0.000108656", "train_gnorm": "1.29", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1955"}
[2022-01-13 10:36:27,891][fairseq.trainer][INFO] - begin training epoch 58
[2022-01-13 10:36:27,892][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:36:38,506][train_inner][INFO] - {"epoch": 58, "update": 57.377, "loss": "3.912", "ntokens": "1792.3", "nsentences": "4.955", "prob_perplexity": "20.16", "code_perplexity": "20.153", "temp": "1.932", "loss_0": "3.742", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.42224", "wps": "6578", "ups": "3.67", "wpb": "1792.3", "bsz": "5", "num_updates": "7000", "lr": "0.000109375", "gnorm": "1.289", "clip": "0", "train_wall": "44", "gb_free": "14.6", "wall": "1966"}
[2022-01-13 10:36:55,649][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:36:56,189][valid][INFO] - {"epoch": 58, "valid_loss": "3.873", "valid_ntokens": "1645", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.871", "valid_code_perplexity": "20.883", "valid_temp": "1.93", "valid_loss_0": "3.7", "valid_loss_1": "0.14", "valid_loss_2": "0.034", "valid_accuracy": "0.43283", "valid_wps": "27833.8", "valid_wpb": "1645", "valid_bsz": "4.5", "valid_num_updates": "7076", "valid_best_loss": "3.778"}
[2022-01-13 10:36:56,190][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 7076 updates
[2022-01-13 10:36:56,191][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:37:00,154][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:37:00,178][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 58 @ 7076 updates, score 3.873) (writing took 3.988019767217338 seconds)
[2022-01-13 10:37:00,179][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2022-01-13 10:37:00,192][train][INFO] - {"epoch": 58, "train_loss": "3.909", "train_ntokens": "1800.66", "train_nsentences": "4.97541", "train_prob_perplexity": "20.236", "train_code_perplexity": "20.23", "train_temp": "1.931", "train_loss_0": "3.738", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.42344", "train_wps": "6790.1", "train_ups": "3.77", "train_wpb": "1800.7", "train_bsz": "5", "train_num_updates": "7076", "train_lr": "0.000110562", "train_gnorm": "1.268", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1988"}
[2022-01-13 10:37:00,245][fairseq.trainer][INFO] - begin training epoch 59
[2022-01-13 10:37:00,246][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:37:27,873][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:37:28,395][valid][INFO] - {"epoch": 59, "valid_loss": "3.768", "valid_ntokens": "1496", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.585", "valid_code_perplexity": "19.587", "valid_temp": "1.929", "valid_loss_0": "3.597", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.4502", "valid_wps": "24791.5", "valid_wpb": "1496", "valid_bsz": "4.5", "valid_num_updates": "7198", "valid_best_loss": "3.768"}
[2022-01-13 10:37:28,397][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 7198 updates
[2022-01-13 10:37:28,398][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:37:32,274][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:37:38,818][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 59 @ 7198 updates, score 3.768) (writing took 10.421266016550362 seconds)
[2022-01-13 10:37:38,819][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2022-01-13 10:37:38,833][train][INFO] - {"epoch": 59, "train_loss": "3.916", "train_ntokens": "1796.62", "train_nsentences": "4.97541", "train_prob_perplexity": "20.288", "train_code_perplexity": "20.282", "train_temp": "1.93", "train_loss_0": "3.745", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.41971", "train_wps": "5674.6", "train_ups": "3.16", "train_wpb": "1796.6", "train_bsz": "5", "train_num_updates": "7198", "train_lr": "0.000112469", "train_gnorm": "1.262", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2026"}
[2022-01-13 10:37:38,907][fairseq.trainer][INFO] - begin training epoch 60
[2022-01-13 10:37:38,908][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:37:39,772][train_inner][INFO] - {"epoch": 60, "update": 59.016, "loss": "3.913", "ntokens": "1802.79", "nsentences": "4.985", "prob_perplexity": "20.253", "code_perplexity": "20.247", "temp": "1.93", "loss_0": "3.743", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.42129", "wps": "5886.4", "ups": "3.27", "wpb": "1802.8", "bsz": "5", "num_updates": "7200", "lr": "0.0001125", "gnorm": "1.258", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "2027"}
[2022-01-13 10:38:06,714][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:38:07,239][valid][INFO] - {"epoch": 60, "valid_loss": "3.971", "valid_ntokens": "1680.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.595", "valid_code_perplexity": "19.599", "valid_temp": "1.928", "valid_loss_0": "3.8", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.41803", "valid_wps": "28040", "valid_wpb": "1680.5", "valid_bsz": "4.5", "valid_num_updates": "7320", "valid_best_loss": "3.768"}
[2022-01-13 10:38:07,241][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 7320 updates
[2022-01-13 10:38:07,242][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:38:11,218][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:38:11,235][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 60 @ 7320 updates, score 3.971) (writing took 3.993497198447585 seconds)
[2022-01-13 10:38:11,235][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2022-01-13 10:38:11,248][train][INFO] - {"epoch": 60, "train_loss": "3.904", "train_ntokens": "1796.71", "train_nsentences": "4.97541", "train_prob_perplexity": "20.4", "train_code_perplexity": "20.392", "train_temp": "1.929", "train_loss_0": "3.733", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.42161", "train_wps": "6764.9", "train_ups": "3.77", "train_wpb": "1796.7", "train_bsz": "5", "train_num_updates": "7320", "train_lr": "0.000114375", "train_gnorm": "1.218", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2059"}
[2022-01-13 10:38:11,320][fairseq.trainer][INFO] - begin training epoch 61
[2022-01-13 10:38:11,320][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:38:29,561][train_inner][INFO] - {"epoch": 61, "update": 60.656, "loss": "3.899", "ntokens": "1798.95", "nsentences": "4.97", "prob_perplexity": "20.445", "code_perplexity": "20.439", "temp": "1.928", "loss_0": "3.728", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.42238", "wps": "7228.1", "ups": "4.02", "wpb": "1799", "bsz": "5", "num_updates": "7400", "lr": "0.000115625", "gnorm": "1.229", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "2077"}
[2022-01-13 10:38:39,038][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:38:39,560][valid][INFO] - {"epoch": 61, "valid_loss": "3.829", "valid_ntokens": "1631.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.756", "valid_code_perplexity": "21.727", "valid_temp": "1.927", "valid_loss_0": "3.658", "valid_loss_1": "0.139", "valid_loss_2": "0.032", "valid_accuracy": "0.43855", "valid_wps": "28542.7", "valid_wpb": "1631.5", "valid_bsz": "4.5", "valid_num_updates": "7442", "valid_best_loss": "3.768"}
[2022-01-13 10:38:39,562][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 7442 updates
[2022-01-13 10:38:39,562][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:38:43,432][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:38:43,453][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 61 @ 7442 updates, score 3.829) (writing took 3.8917866833508015 seconds)
[2022-01-13 10:38:43,454][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2022-01-13 10:38:43,467][train][INFO] - {"epoch": 61, "train_loss": "3.887", "train_ntokens": "1800.99", "train_nsentences": "4.97541", "train_prob_perplexity": "20.372", "train_code_perplexity": "20.367", "train_temp": "1.928", "train_loss_0": "3.716", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.42491", "train_wps": "6822.3", "train_ups": "3.79", "train_wpb": "1801", "train_bsz": "5", "train_num_updates": "7442", "train_lr": "0.000116281", "train_gnorm": "1.241", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "15.1", "train_wall": "2091"}
[2022-01-13 10:38:43,532][fairseq.trainer][INFO] - begin training epoch 62
[2022-01-13 10:38:43,533][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:39:11,291][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:39:11,873][valid][INFO] - {"epoch": 62, "valid_loss": "3.821", "valid_ntokens": "1685.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.595", "valid_code_perplexity": "18.596", "valid_temp": "1.926", "valid_loss_0": "3.648", "valid_loss_1": "0.14", "valid_loss_2": "0.033", "valid_accuracy": "0.4423", "valid_wps": "30054", "valid_wpb": "1685.5", "valid_bsz": "4.5", "valid_num_updates": "7564", "valid_best_loss": "3.768"}
[2022-01-13 10:39:11,875][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 7564 updates
[2022-01-13 10:39:11,876][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:39:15,740][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:39:15,755][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 62 @ 7564 updates, score 3.821) (writing took 3.8798212464898825 seconds)
[2022-01-13 10:39:15,755][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2022-01-13 10:39:15,769][train][INFO] - {"epoch": 62, "train_loss": "3.899", "train_ntokens": "1808.52", "train_nsentences": "4.97541", "train_prob_perplexity": "20.506", "train_code_perplexity": "20.5", "train_temp": "1.926", "train_loss_0": "3.728", "train_loss_1": "0.14", "train_loss_2": "0.032", "train_accuracy": "0.4245", "train_wps": "6833.4", "train_ups": "3.78", "train_wpb": "1808.5", "train_bsz": "5", "train_num_updates": "7564", "train_lr": "0.000118188", "train_gnorm": "1.196", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2123"}
[2022-01-13 10:39:15,808][fairseq.trainer][INFO] - begin training epoch 63
[2022-01-13 10:39:15,809][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:39:24,240][train_inner][INFO] - {"epoch": 63, "update": 62.295, "loss": "3.891", "ntokens": "1805.64", "nsentences": "4.985", "prob_perplexity": "20.459", "code_perplexity": "20.452", "temp": "1.926", "loss_0": "3.719", "loss_1": "0.14", "loss_2": "0.032", "accuracy": "0.42611", "wps": "6606", "ups": "3.66", "wpb": "1805.6", "bsz": "5", "num_updates": "7600", "lr": "0.00011875", "gnorm": "1.207", "clip": "0", "train_wall": "45", "gb_free": "14.5", "wall": "2132"}
[2022-01-13 10:39:43,366][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:39:43,897][valid][INFO] - {"epoch": 63, "valid_loss": "3.885", "valid_ntokens": "1682.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.972", "valid_code_perplexity": "18.973", "valid_temp": "1.925", "valid_loss_0": "3.713", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.44577", "valid_wps": "27312.2", "valid_wpb": "1682.5", "valid_bsz": "4.5", "valid_num_updates": "7686", "valid_best_loss": "3.768"}
[2022-01-13 10:39:43,899][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 7686 updates
[2022-01-13 10:39:43,900][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:39:47,802][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:39:47,818][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 63 @ 7686 updates, score 3.885) (writing took 3.919054528698325 seconds)
[2022-01-13 10:39:47,819][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2022-01-13 10:39:47,832][train][INFO] - {"epoch": 63, "train_loss": "3.886", "train_ntokens": "1797.3", "train_nsentences": "4.97541", "train_prob_perplexity": "20.417", "train_code_perplexity": "20.412", "train_temp": "1.925", "train_loss_0": "3.715", "train_loss_1": "0.14", "train_loss_2": "0.032", "train_accuracy": "0.42638", "train_wps": "6841.5", "train_ups": "3.81", "train_wpb": "1797.3", "train_bsz": "5", "train_num_updates": "7686", "train_lr": "0.000120094", "train_gnorm": "1.194", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2155"}
[2022-01-13 10:39:47,882][fairseq.trainer][INFO] - begin training epoch 64
[2022-01-13 10:39:47,883][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:40:13,891][train_inner][INFO] - {"epoch": 64, "update": 63.934, "loss": "3.896", "ntokens": "1798.33", "nsentences": "4.97", "prob_perplexity": "20.489", "code_perplexity": "20.484", "temp": "1.924", "loss_0": "3.725", "loss_1": "0.14", "loss_2": "0.032", "accuracy": "0.42267", "wps": "7245.8", "ups": "4.03", "wpb": "1798.3", "bsz": "5", "num_updates": "7800", "lr": "0.000121875", "gnorm": "1.184", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "2181"}
[2022-01-13 10:40:15,677][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:40:16,202][valid][INFO] - {"epoch": 64, "valid_loss": "3.83", "valid_ntokens": "1650.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.853", "valid_code_perplexity": "19.849", "valid_temp": "1.923", "valid_loss_0": "3.657", "valid_loss_1": "0.14", "valid_loss_2": "0.033", "valid_accuracy": "0.45198", "valid_wps": "28363.2", "valid_wpb": "1650.5", "valid_bsz": "4.5", "valid_num_updates": "7808", "valid_best_loss": "3.768"}
[2022-01-13 10:40:16,204][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 7808 updates
[2022-01-13 10:40:16,205][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:40:20,068][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:40:20,089][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 64 @ 7808 updates, score 3.83) (writing took 3.8852576911449432 seconds)
[2022-01-13 10:40:20,090][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2022-01-13 10:40:20,102][train][INFO] - {"epoch": 64, "train_loss": "3.896", "train_ntokens": "1800.2", "train_nsentences": "4.97541", "train_prob_perplexity": "20.63", "train_code_perplexity": "20.625", "train_temp": "1.924", "train_loss_0": "3.724", "train_loss_1": "0.14", "train_loss_2": "0.032", "train_accuracy": "0.42159", "train_wps": "6808.4", "train_ups": "3.78", "train_wpb": "1800.2", "train_bsz": "5", "train_num_updates": "7808", "train_lr": "0.000122", "train_gnorm": "1.181", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2188"}
[2022-01-13 10:40:20,161][fairseq.trainer][INFO] - begin training epoch 65
[2022-01-13 10:40:20,162][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:40:47,943][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:40:48,458][valid][INFO] - {"epoch": 65, "valid_loss": "3.86", "valid_ntokens": "1674.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.328", "valid_code_perplexity": "21.311", "valid_temp": "1.922", "valid_loss_0": "3.687", "valid_loss_1": "0.139", "valid_loss_2": "0.033", "valid_accuracy": "0.41206", "valid_wps": "28179.5", "valid_wpb": "1674.5", "valid_bsz": "4.5", "valid_num_updates": "7930", "valid_best_loss": "3.768"}
[2022-01-13 10:40:48,460][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 7930 updates
[2022-01-13 10:40:48,461][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:40:52,317][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:40:52,342][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 65 @ 7930 updates, score 3.86) (writing took 3.8816636614501476 seconds)
[2022-01-13 10:40:52,342][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2022-01-13 10:40:52,355][train][INFO] - {"epoch": 65, "train_loss": "3.866", "train_ntokens": "1801.9", "train_nsentences": "4.97541", "train_prob_perplexity": "20.384", "train_code_perplexity": "20.381", "train_temp": "1.923", "train_loss_0": "3.693", "train_loss_1": "0.14", "train_loss_2": "0.033", "train_accuracy": "0.42934", "train_wps": "6818.6", "train_ups": "3.78", "train_wpb": "1801.9", "train_bsz": "5", "train_num_updates": "7930", "train_lr": "0.000123906", "train_gnorm": "1.175", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2220"}
[2022-01-13 10:40:52,405][fairseq.trainer][INFO] - begin training epoch 66
[2022-01-13 10:40:52,406][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:41:08,472][train_inner][INFO] - {"epoch": 66, "update": 65.574, "loss": "3.876", "ntokens": "1796.66", "nsentences": "4.97", "prob_perplexity": "20.373", "code_perplexity": "20.37", "temp": "1.923", "loss_0": "3.704", "loss_1": "0.14", "loss_2": "0.032", "accuracy": "0.42665", "wps": "6585.1", "ups": "3.67", "wpb": "1796.7", "bsz": "5", "num_updates": "8000", "lr": "0.000125", "gnorm": "1.165", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2236"}
[2022-01-13 10:41:20,190][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:41:20,716][valid][INFO] - {"epoch": 66, "valid_loss": "3.824", "valid_ntokens": "1679", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.809", "valid_code_perplexity": "21.81", "valid_temp": "1.921", "valid_loss_0": "3.652", "valid_loss_1": "0.139", "valid_loss_2": "0.032", "valid_accuracy": "0.43925", "valid_wps": "27530.9", "valid_wpb": "1679", "valid_bsz": "4.5", "valid_num_updates": "8052", "valid_best_loss": "3.768"}
[2022-01-13 10:41:20,718][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 8052 updates
[2022-01-13 10:41:20,719][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:41:24,605][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:41:24,629][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 66 @ 8052 updates, score 3.824) (writing took 3.9111441373825073 seconds)
[2022-01-13 10:41:24,630][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2022-01-13 10:41:24,643][train][INFO] - {"epoch": 66, "train_loss": "3.897", "train_ntokens": "1805.79", "train_nsentences": "4.97541", "train_prob_perplexity": "20.312", "train_code_perplexity": "20.307", "train_temp": "1.922", "train_loss_0": "3.725", "train_loss_1": "0.14", "train_loss_2": "0.033", "train_accuracy": "0.42372", "train_wps": "6825.8", "train_ups": "3.78", "train_wpb": "1805.8", "train_bsz": "5", "train_num_updates": "8052", "train_lr": "0.000125813", "train_gnorm": "1.141", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2252"}
[2022-01-13 10:41:24,702][fairseq.trainer][INFO] - begin training epoch 67
[2022-01-13 10:41:24,703][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:41:52,494][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:41:53,075][valid][INFO] - {"epoch": 67, "valid_loss": "3.841", "valid_ntokens": "1638", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.82", "valid_code_perplexity": "20.805", "valid_temp": "1.92", "valid_loss_0": "3.67", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.4362", "valid_wps": "29766.7", "valid_wpb": "1638", "valid_bsz": "4.5", "valid_num_updates": "8174", "valid_best_loss": "3.768"}
[2022-01-13 10:41:53,077][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 8174 updates
[2022-01-13 10:41:53,077][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:41:56,951][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:41:56,973][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 67 @ 8174 updates, score 3.841) (writing took 3.89622341748327 seconds)
[2022-01-13 10:41:56,973][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2022-01-13 10:41:56,986][train][INFO] - {"epoch": 67, "train_loss": "3.883", "train_ntokens": "1802.57", "train_nsentences": "4.97541", "train_prob_perplexity": "21.187", "train_code_perplexity": "21.18", "train_temp": "1.92", "train_loss_0": "3.711", "train_loss_1": "0.139", "train_loss_2": "0.032", "train_accuracy": "0.41965", "train_wps": "6802.2", "train_ups": "3.77", "train_wpb": "1802.6", "train_bsz": "5", "train_num_updates": "8174", "train_lr": "0.000127719", "train_gnorm": "1.131", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "2285"}
[2022-01-13 10:41:57,049][fairseq.trainer][INFO] - begin training epoch 68
[2022-01-13 10:41:57,050][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:42:03,111][train_inner][INFO] - {"epoch": 68, "update": 67.213, "loss": "3.889", "ntokens": "1804.76", "nsentences": "4.97", "prob_perplexity": "20.897", "code_perplexity": "20.891", "temp": "1.921", "loss_0": "3.717", "loss_1": "0.14", "loss_2": "0.032", "accuracy": "0.42119", "wps": "6607.6", "ups": "3.66", "wpb": "1804.8", "bsz": "5", "num_updates": "8200", "lr": "0.000128125", "gnorm": "1.134", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "2291"}
[2022-01-13 10:42:24,782][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:42:25,316][valid][INFO] - {"epoch": 68, "valid_loss": "3.867", "valid_ntokens": "1684", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.805", "valid_code_perplexity": "20.787", "valid_temp": "1.919", "valid_loss_0": "3.691", "valid_loss_1": "0.14", "valid_loss_2": "0.036", "valid_accuracy": "0.40707", "valid_wps": "28370.1", "valid_wpb": "1684", "valid_bsz": "4.5", "valid_num_updates": "8296", "valid_best_loss": "3.768"}
[2022-01-13 10:42:25,318][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 8296 updates
[2022-01-13 10:42:25,319][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:42:29,223][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:42:29,250][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 68 @ 8296 updates, score 3.867) (writing took 3.9316644705832005 seconds)
[2022-01-13 10:42:29,250][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2022-01-13 10:42:29,263][train][INFO] - {"epoch": 68, "train_loss": "3.883", "train_ntokens": "1793.43", "train_nsentences": "4.97541", "train_prob_perplexity": "20.927", "train_code_perplexity": "20.925", "train_temp": "1.919", "train_loss_0": "3.711", "train_loss_1": "0.14", "train_loss_2": "0.033", "train_accuracy": "0.41873", "train_wps": "6781.6", "train_ups": "3.78", "train_wpb": "1793.4", "train_bsz": "5", "train_num_updates": "8296", "train_lr": "0.000129625", "train_gnorm": "1.129", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "2317"}
[2022-01-13 10:42:29,345][fairseq.trainer][INFO] - begin training epoch 69
[2022-01-13 10:42:29,346][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:42:53,020][train_inner][INFO] - {"epoch": 69, "update": 68.852, "loss": "3.887", "ntokens": "1800.15", "nsentences": "4.985", "prob_perplexity": "20.818", "code_perplexity": "20.815", "temp": "1.919", "loss_0": "3.715", "loss_1": "0.14", "loss_2": "0.033", "accuracy": "0.42005", "wps": "7215.5", "ups": "4.01", "wpb": "1800.2", "bsz": "5", "num_updates": "8400", "lr": "0.00013125", "gnorm": "1.117", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2341"}
[2022-01-13 10:42:57,070][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:42:57,659][valid][INFO] - {"epoch": 69, "valid_loss": "3.739", "valid_ntokens": "1596", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.369", "valid_code_perplexity": "20.369", "valid_temp": "1.918", "valid_loss_0": "3.568", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.44831", "valid_wps": "30181.8", "valid_wpb": "1596", "valid_bsz": "4.5", "valid_num_updates": "8418", "valid_best_loss": "3.739"}
[2022-01-13 10:42:57,661][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 8418 updates
[2022-01-13 10:42:57,661][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:43:01,529][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-13 10:43:10,131][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 69 @ 8418 updates, score 3.739) (writing took 12.470345153473318 seconds)
[2022-01-13 10:43:10,132][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2022-01-13 10:43:10,144][train][INFO] - {"epoch": 69, "train_loss": "3.896", "train_ntokens": "1799.67", "train_nsentences": "4.97541", "train_prob_perplexity": "20.763", "train_code_perplexity": "20.759", "train_temp": "1.918", "train_loss_0": "3.724", "train_loss_1": "0.14", "train_loss_2": "0.032", "train_accuracy": "0.41969", "train_wps": "5372.3", "train_ups": "2.99", "train_wpb": "1799.7", "train_bsz": "5", "train_num_updates": "8418", "train_lr": "0.000131531", "train_gnorm": "1.109", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2358"}
[2022-01-13 10:43:10,190][fairseq.trainer][INFO] - begin training epoch 70
[2022-01-13 10:43:10,191][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:43:37,902][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:43:38,439][valid][INFO] - {"epoch": 70, "valid_loss": "3.818", "valid_ntokens": "1603.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.097", "valid_code_perplexity": "19.097", "valid_temp": "1.916", "valid_loss_0": "3.644", "valid_loss_1": "0.14", "valid_loss_2": "0.034", "valid_accuracy": "0.44309", "valid_wps": "26711", "valid_wpb": "1603.5", "valid_bsz": "4.5", "valid_num_updates": "8540", "valid_best_loss": "3.739"}
[2022-01-13 10:43:38,441][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 8540 updates
[2022-01-13 10:43:38,441][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:43:42,417][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:43:42,435][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 70 @ 8540 updates, score 3.818) (writing took 3.994492432102561 seconds)
[2022-01-13 10:43:42,436][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2022-01-13 10:43:42,448][train][INFO] - {"epoch": 70, "train_loss": "3.895", "train_ntokens": "1806.01", "train_nsentences": "4.97541", "train_prob_perplexity": "20.869", "train_code_perplexity": "20.864", "train_temp": "1.917", "train_loss_0": "3.722", "train_loss_1": "0.14", "train_loss_2": "0.033", "train_accuracy": "0.41856", "train_wps": "6823.2", "train_ups": "3.78", "train_wpb": "1806", "train_bsz": "5", "train_num_updates": "8540", "train_lr": "0.000133437", "train_gnorm": "1.098", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2390"}
[2022-01-13 10:43:42,524][fairseq.trainer][INFO] - begin training epoch 71
[2022-01-13 10:43:42,525][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:43:56,366][train_inner][INFO] - {"epoch": 71, "update": 70.492, "loss": "3.898", "ntokens": "1803.12", "nsentences": "4.97", "prob_perplexity": "20.751", "code_perplexity": "20.745", "temp": "1.917", "loss_0": "3.725", "loss_1": "0.14", "loss_2": "0.033", "accuracy": "0.41831", "wps": "5694.1", "ups": "3.16", "wpb": "1803.1", "bsz": "5", "num_updates": "8600", "lr": "0.000134375", "gnorm": "1.104", "clip": "0", "train_wall": "44", "gb_free": "13.4", "wall": "2404"}
[2022-01-13 10:44:10,310][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:44:10,893][valid][INFO] - {"epoch": 71, "valid_loss": "3.81", "valid_ntokens": "1592", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.673", "valid_code_perplexity": "20.666", "valid_temp": "1.915", "valid_loss_0": "3.634", "valid_loss_1": "0.14", "valid_loss_2": "0.037", "valid_accuracy": "0.4441", "valid_wps": "28653.9", "valid_wpb": "1592", "valid_bsz": "4.5", "valid_num_updates": "8662", "valid_best_loss": "3.739"}
[2022-01-13 10:44:10,895][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 8662 updates
[2022-01-13 10:44:10,895][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:44:14,970][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:44:14,995][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 71 @ 8662 updates, score 3.81) (writing took 4.100791402161121 seconds)
[2022-01-13 10:44:14,996][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2022-01-13 10:44:15,009][train][INFO] - {"epoch": 71, "train_loss": "3.895", "train_ntokens": "1805", "train_nsentences": "4.97541", "train_prob_perplexity": "20.699", "train_code_perplexity": "20.692", "train_temp": "1.916", "train_loss_0": "3.722", "train_loss_1": "0.14", "train_loss_2": "0.033", "train_accuracy": "0.41867", "train_wps": "6765.8", "train_ups": "3.75", "train_wpb": "1805", "train_bsz": "5", "train_num_updates": "8662", "train_lr": "0.000135344", "train_gnorm": "1.089", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2423"}
[2022-01-13 10:44:15,088][fairseq.trainer][INFO] - begin training epoch 72
[2022-01-13 10:44:15,089][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:44:42,853][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:44:43,370][valid][INFO] - {"epoch": 72, "valid_loss": "3.903", "valid_ntokens": "1672.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.571", "valid_code_perplexity": "18.572", "valid_temp": "1.914", "valid_loss_0": "3.733", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.42272", "valid_wps": "28039.3", "valid_wpb": "1672.5", "valid_bsz": "4.5", "valid_num_updates": "8784", "valid_best_loss": "3.739"}
[2022-01-13 10:44:43,373][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 8784 updates
[2022-01-13 10:44:43,373][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:44:47,342][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:44:47,363][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 72 @ 8784 updates, score 3.903) (writing took 3.990774675272405 seconds)
[2022-01-13 10:44:47,364][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2022-01-13 10:44:47,377][train][INFO] - {"epoch": 72, "train_loss": "3.886", "train_ntokens": "1807.22", "train_nsentences": "4.97541", "train_prob_perplexity": "21.402", "train_code_perplexity": "21.397", "train_temp": "1.915", "train_loss_0": "3.713", "train_loss_1": "0.139", "train_loss_2": "0.033", "train_accuracy": "0.41981", "train_wps": "6814.5", "train_ups": "3.77", "train_wpb": "1807.2", "train_bsz": "5", "train_num_updates": "8784", "train_lr": "0.00013725", "train_gnorm": "1.053", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2455"}
[2022-01-13 10:44:47,427][fairseq.trainer][INFO] - begin training epoch 73
[2022-01-13 10:44:47,428][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:44:51,453][train_inner][INFO] - {"epoch": 73, "update": 72.131, "loss": "3.885", "ntokens": "1810.61", "nsentences": "4.985", "prob_perplexity": "21.302", "code_perplexity": "21.297", "temp": "1.915", "loss_0": "3.713", "loss_1": "0.139", "loss_2": "0.033", "accuracy": "0.41948", "wps": "6575.1", "ups": "3.63", "wpb": "1810.6", "bsz": "5", "num_updates": "8800", "lr": "0.0001375", "gnorm": "1.057", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2459"}
[2022-01-13 10:45:15,275][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:45:15,795][valid][INFO] - {"epoch": 73, "valid_loss": "3.947", "valid_ntokens": "1711", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.789", "valid_code_perplexity": "19.792", "valid_temp": "1.913", "valid_loss_0": "3.774", "valid_loss_1": "0.14", "valid_loss_2": "0.033", "valid_accuracy": "0.39714", "valid_wps": "27685.3", "valid_wpb": "1711", "valid_bsz": "4.5", "valid_num_updates": "8906", "valid_best_loss": "3.739"}
[2022-01-13 10:45:15,798][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 8906 updates
[2022-01-13 10:45:15,798][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:45:19,824][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:45:19,848][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 73 @ 8906 updates, score 3.947) (writing took 4.050379632972181 seconds)
[2022-01-13 10:45:19,849][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2022-01-13 10:45:19,864][train][INFO] - {"epoch": 73, "train_loss": "3.884", "train_ntokens": "1800.2", "train_nsentences": "4.97541", "train_prob_perplexity": "21.164", "train_code_perplexity": "21.16", "train_temp": "1.913", "train_loss_0": "3.711", "train_loss_1": "0.139", "train_loss_2": "0.033", "train_accuracy": "0.41947", "train_wps": "6763.7", "train_ups": "3.76", "train_wpb": "1800.2", "train_bsz": "5", "train_num_updates": "8906", "train_lr": "0.000139156", "train_gnorm": "1.062", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2487"}
[2022-01-13 10:45:19,940][fairseq.trainer][INFO] - begin training epoch 74
[2022-01-13 10:45:19,941][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:45:41,373][train_inner][INFO] - {"epoch": 74, "update": 73.77, "loss": "3.888", "ntokens": "1793.87", "nsentences": "4.97", "prob_perplexity": "20.975", "code_perplexity": "20.971", "temp": "1.913", "loss_0": "3.714", "loss_1": "0.14", "loss_2": "0.034", "accuracy": "0.41853", "wps": "7188.9", "ups": "4.01", "wpb": "1793.9", "bsz": "5", "num_updates": "9000", "lr": "0.000140625", "gnorm": "1.078", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2509"}
[2022-01-13 10:45:47,721][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:45:48,258][valid][INFO] - {"epoch": 74, "valid_loss": "3.77", "valid_ntokens": "1663", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.295", "valid_code_perplexity": "22.293", "valid_temp": "1.912", "valid_loss_0": "3.597", "valid_loss_1": "0.139", "valid_loss_2": "0.034", "valid_accuracy": "0.42995", "valid_wps": "27621.3", "valid_wpb": "1663", "valid_bsz": "4.5", "valid_num_updates": "9028", "valid_best_loss": "3.739"}
[2022-01-13 10:45:48,261][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 9028 updates
[2022-01-13 10:45:48,261][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:45:52,175][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:45:52,200][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 74 @ 9028 updates, score 3.77) (writing took 3.9390811258926988 seconds)
[2022-01-13 10:45:52,200][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2022-01-13 10:45:52,214][train][INFO] - {"epoch": 74, "train_loss": "3.892", "train_ntokens": "1798.27", "train_nsentences": "4.97541", "train_prob_perplexity": "20.785", "train_code_perplexity": "20.781", "train_temp": "1.912", "train_loss_0": "3.717", "train_loss_1": "0.14", "train_loss_2": "0.035", "train_accuracy": "0.41875", "train_wps": "6784.7", "train_ups": "3.77", "train_wpb": "1798.3", "train_bsz": "5", "train_num_updates": "9028", "train_lr": "0.000141063", "train_gnorm": "1.086", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2520"}
[2022-01-13 10:45:52,287][fairseq.trainer][INFO] - begin training epoch 75
[2022-01-13 10:45:52,288][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:46:20,077][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:46:20,527][valid][INFO] - {"epoch": 75, "valid_loss": "3.9", "valid_ntokens": "1597.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.485", "valid_code_perplexity": "20.49", "valid_temp": "1.911", "valid_loss_0": "3.724", "valid_loss_1": "0.14", "valid_loss_2": "0.036", "valid_accuracy": "0.41315", "valid_wps": "28076.6", "valid_wpb": "1597.5", "valid_bsz": "4.5", "valid_num_updates": "9150", "valid_best_loss": "3.739"}
[2022-01-13 10:46:20,529][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 9150 updates
[2022-01-13 10:46:20,530][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:46:24,419][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:46:24,439][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 75 @ 9150 updates, score 3.9) (writing took 3.910006041638553 seconds)
[2022-01-13 10:46:24,440][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2022-01-13 10:46:24,453][train][INFO] - {"epoch": 75, "train_loss": "3.887", "train_ntokens": "1808.46", "train_nsentences": "4.97541", "train_prob_perplexity": "20.958", "train_code_perplexity": "20.953", "train_temp": "1.911", "train_loss_0": "3.713", "train_loss_1": "0.14", "train_loss_2": "0.034", "train_accuracy": "0.42003", "train_wps": "6846.5", "train_ups": "3.79", "train_wpb": "1808.5", "train_bsz": "5", "train_num_updates": "9150", "train_lr": "0.000142969", "train_gnorm": "1.068", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2552"}
[2022-01-13 10:46:24,514][fairseq.trainer][INFO] - begin training epoch 76
[2022-01-13 10:46:24,514][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:46:36,036][train_inner][INFO] - {"epoch": 76, "update": 75.41, "loss": "3.888", "ntokens": "1813.73", "nsentences": "4.985", "prob_perplexity": "20.901", "code_perplexity": "20.897", "temp": "1.911", "loss_0": "3.714", "loss_1": "0.14", "loss_2": "0.034", "accuracy": "0.42108", "wps": "6637.6", "ups": "3.66", "wpb": "1813.7", "bsz": "5", "num_updates": "9200", "lr": "0.00014375", "gnorm": "1.053", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2564"}
[2022-01-13 10:46:52,471][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:46:52,958][valid][INFO] - {"epoch": 76, "valid_loss": "4.038", "valid_ntokens": "1755.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.557", "valid_code_perplexity": "21.553", "valid_temp": "1.909", "valid_loss_0": "3.865", "valid_loss_1": "0.139", "valid_loss_2": "0.034", "valid_accuracy": "0.39419", "valid_wps": "31252.8", "valid_wpb": "1755.5", "valid_bsz": "4.5", "valid_num_updates": "9272", "valid_best_loss": "3.739"}
[2022-01-13 10:46:52,960][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 9272 updates
[2022-01-13 10:46:52,961][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:46:56,880][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:46:56,906][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 76 @ 9272 updates, score 4.038) (writing took 3.9462138386443257 seconds)
[2022-01-13 10:46:56,907][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2022-01-13 10:46:56,920][train][INFO] - {"epoch": 76, "train_loss": "3.884", "train_ntokens": "1811.81", "train_nsentences": "4.97541", "train_prob_perplexity": "20.886", "train_code_perplexity": "20.884", "train_temp": "1.91", "train_loss_0": "3.71", "train_loss_1": "0.14", "train_loss_2": "0.034", "train_accuracy": "0.42133", "train_wps": "6811", "train_ups": "3.76", "train_wpb": "1811.8", "train_bsz": "5", "train_num_updates": "9272", "train_lr": "0.000144875", "train_gnorm": "1.037", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2585"}
[2022-01-13 10:46:56,983][fairseq.trainer][INFO] - begin training epoch 77
[2022-01-13 10:46:56,984][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:47:24,624][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:47:25,106][valid][INFO] - {"epoch": 77, "valid_loss": "3.892", "valid_ntokens": "1679.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.767", "valid_code_perplexity": "19.761", "valid_temp": "1.908", "valid_loss_0": "3.715", "valid_loss_1": "0.14", "valid_loss_2": "0.037", "valid_accuracy": "0.41411", "valid_wps": "29267.9", "valid_wpb": "1679.5", "valid_bsz": "4.5", "valid_num_updates": "9394", "valid_best_loss": "3.739"}
[2022-01-13 10:47:25,108][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 9394 updates
[2022-01-13 10:47:25,109][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:47:29,169][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:47:29,197][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 77 @ 9394 updates, score 3.892) (writing took 4.088410914875567 seconds)
[2022-01-13 10:47:29,198][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2022-01-13 10:47:29,211][train][INFO] - {"epoch": 77, "train_loss": "3.873", "train_ntokens": "1803.23", "train_nsentences": "4.97541", "train_prob_perplexity": "21.294", "train_code_perplexity": "21.29", "train_temp": "1.909", "train_loss_0": "3.699", "train_loss_1": "0.139", "train_loss_2": "0.035", "train_accuracy": "0.41782", "train_wps": "6815.7", "train_ups": "3.78", "train_wpb": "1803.2", "train_bsz": "5", "train_num_updates": "9394", "train_lr": "0.000146781", "train_gnorm": "1.008", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "2617"}
[2022-01-13 10:47:29,272][fairseq.trainer][INFO] - begin training epoch 78
[2022-01-13 10:47:29,273][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:47:30,952][train_inner][INFO] - {"epoch": 78, "update": 77.049, "loss": "3.878", "ntokens": "1805.05", "nsentences": "4.97", "prob_perplexity": "21.118", "code_perplexity": "21.115", "temp": "1.909", "loss_0": "3.704", "loss_1": "0.14", "loss_2": "0.035", "accuracy": "0.41832", "wps": "6575.5", "ups": "3.64", "wpb": "1805", "bsz": "5", "num_updates": "9400", "lr": "0.000146875", "gnorm": "1.022", "clip": "0", "train_wall": "45", "gb_free": "15.1", "wall": "2619"}
[2022-01-13 10:47:57,114][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:47:57,598][valid][INFO] - {"epoch": 78, "valid_loss": "3.884", "valid_ntokens": "1671.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.002", "valid_code_perplexity": "19.992", "valid_temp": "1.907", "valid_loss_0": "3.711", "valid_loss_1": "0.14", "valid_loss_2": "0.034", "valid_accuracy": "0.4143", "valid_wps": "28184.2", "valid_wpb": "1671.5", "valid_bsz": "4.5", "valid_num_updates": "9516", "valid_best_loss": "3.739"}
[2022-01-13 10:47:57,601][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 9516 updates
[2022-01-13 10:47:57,601][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:48:01,493][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-13 10:48:01,512][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 78 @ 9516 updates, score 3.884) (writing took 3.9108147425577044 seconds)
[2022-01-13 10:48:01,513][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2022-01-13 10:48:01,526][train][INFO] - {"epoch": 78, "train_loss": "3.901", "train_ntokens": "1817.22", "train_nsentences": "4.97541", "train_prob_perplexity": "20.997", "train_code_perplexity": "20.992", "train_temp": "1.908", "train_loss_0": "3.727", "train_loss_1": "0.14", "train_loss_2": "0.035", "train_accuracy": "0.41622", "train_wps": "6863.6", "train_ups": "3.78", "train_wpb": "1817.2", "train_bsz": "5", "train_num_updates": "9516", "train_lr": "0.000148688", "train_gnorm": "1.002", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2649"}
[2022-01-13 10:48:01,589][fairseq.trainer][INFO] - begin training epoch 79
[2022-01-13 10:48:01,590][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-13 10:48:21,161][train_inner][INFO] - {"epoch": 79, "update": 78.689, "loss": "3.897", "ntokens": "1809.04", "nsentences": "4.97", "prob_perplexity": "20.894", "code_perplexity": "20.891", "temp": "1.907", "loss_0": "3.723", "loss_1": "0.14", "loss_2": "0.035", "accuracy": "0.41665", "wps": "7207.9", "ups": "3.98", "wpb": "1809", "bsz": "5", "num_updates": "9600", "lr": "0.00015", "gnorm": "1.001", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2669"}
[2022-01-13 10:48:29,644][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-13 10:48:30,190][valid][INFO] - {"epoch": 79, "valid_loss": "3.853", "valid_ntokens": "1614.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.981", "valid_code_perplexity": "18.974", "valid_temp": "1.906", "valid_loss_0": "3.678", "valid_loss_1": "0.14", "valid_loss_2": "0.035", "valid_accuracy": "0.4345", "valid_wps": "27774.2", "valid_wpb": "1614.5", "valid_bsz": "4.5", "valid_num_updates": "9638", "valid_best_loss": "3.739"}
[2022-01-13 10:48:30,193][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 9638 updates
[2022-01-13 10:48:30,193][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
