[2022-01-17 11:59:05,138][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': 'tb_logs', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1400000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1400000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 288, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 96, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.1, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'checkpoint_activations': False}, 'task': {'_name': 'audio_pretraining', 'data': '/local/scratch/bestalex/cut_10s_mixed_usv/usv_pairs_train', 'labels': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 250000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': 'none'}, 'criterion': {'_name': 'wav2vec', 'infonce': True, 'loss_weights': [0.1, 10.0], 'log_keys': ['prob_perplexity', 'code_perplexity', 'temp']}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-01-17 11:59:05,604][fairseq_cli.train][INFO] - Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
        (3): GELU()
      )
      (1): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (2): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (3): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (4): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (5): Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (6): Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=288, bias=True)
  (dropout_input): Dropout(p=0.1, inplace=False)
  (dropout_features): Dropout(p=0.1, inplace=False)
  (quantizer): GumbelVectorQuantizer(
    (weight_proj): Linear(in_features=512, out_features=640, bias=True)
  )
  (project_q): Linear(in_features=96, out_features=96, bias=True)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(288, 288, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU()
    )
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=288, out_features=288, bias=True)
          (v_proj): Linear(in_features=288, out_features=288, bias=True)
          (q_proj): Linear(in_features=288, out_features=288, bias=True)
          (out_proj): Linear(in_features=288, out_features=288, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=288, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=288, bias=True)
        (final_layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): Linear(in_features=288, out_features=96, bias=True)
)
[2022-01-17 11:59:05,606][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2022-01-17 11:59:05,606][fairseq_cli.train][INFO] - model: Wav2Vec2Model
[2022-01-17 11:59:05,606][fairseq_cli.train][INFO] - criterion: Wav2vecCriterion
[2022-01-17 11:59:05,607][fairseq_cli.train][INFO] - num. shared model params: 30,693,088 (num. trained: 30,693,088)
[2022-01-17 11:59:05,608][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-01-17 11:59:05,611][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 9, skipped 0 samples
[2022-01-17 11:59:11,701][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias
[2022-01-17 11:59:11,701][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias
[2022-01-17 11:59:11,701][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias
[2022-01-17 11:59:11,701][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias
[2022-01-17 11:59:11,701][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias
[2022-01-17 11:59:11,701][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias
[2022-01-17 11:59:11,702][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-01-17 11:59:11,702][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA RTX A5000                        
[2022-01-17 11:59:11,702][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-01-17 11:59:11,702][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2022-01-17 11:59:11,702][fairseq_cli.train][INFO] - max tokens per device = 1400000 and max sentences per device = None
[2022-01-17 11:59:11,704][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2022-01-17 11:59:11,704][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2022-01-17 11:59:11,704][fairseq.trainer][INFO] - loading train data for epoch 1
[2022-01-17 11:59:11,706][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 607, skipped 0 samples
[2022-01-17 11:59:11,903][fairseq.trainer][INFO] - NOTE: your device may support faster training with --fp16 or --amp
[2022-01-17 11:59:11,916][fairseq.trainer][INFO] - begin training epoch 1
[2022-01-17 11:59:11,917][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 11:59:39,925][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 11:59:40,385][valid][INFO] - {"epoch": 1, "valid_loss": "7.603", "valid_ntokens": "1676.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "225.507", "valid_code_perplexity": "221.342", "valid_temp": "1.999", "valid_loss_0": "6.679", "valid_loss_1": "0.094", "valid_loss_2": "0.831", "valid_accuracy": "0.01312", "valid_wps": "29865.2", "valid_wpb": "1676.5", "valid_bsz": "4.5", "valid_num_updates": "122"}
[2022-01-17 11:59:40,387][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 122 updates
[2022-01-17 11:59:40,387][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 11:59:44,269][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 11:59:55,080][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 122 updates, score 7.603) (writing took 14.693515986204147 seconds)
[2022-01-17 11:59:55,081][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2022-01-17 11:59:55,100][train][INFO] - {"epoch": 1, "train_loss": "8.509", "train_ntokens": "1793.66", "train_nsentences": "4.97541", "train_prob_perplexity": "189.525", "train_code_perplexity": "186.096", "train_temp": "1.999", "train_loss_0": "6.743", "train_loss_1": "0.102", "train_loss_2": "1.665", "train_accuracy": "0.01184", "train_wps": "5136.3", "train_ups": "2.87", "train_wpb": "1793.7", "train_bsz": "5", "train_num_updates": "122", "train_lr": "1.90625e-06", "train_gnorm": "1.597", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "43"}
[2022-01-17 11:59:55,195][fairseq.trainer][INFO] - begin training epoch 2
[2022-01-17 11:59:55,196][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:00:13,253][train_inner][INFO] - {"epoch": 2, "update": 1.639, "loss": "8.043", "ntokens": "1797.38", "nsentences": "4.985", "prob_perplexity": "234.413", "code_perplexity": "230.301", "temp": "1.999", "loss_0": "6.721", "loss_1": "0.091", "loss_2": "1.231", "accuracy": "0.01168", "wps": "5921.1", "ups": "3.3", "wpb": "1797.4", "bsz": "5", "num_updates": "200", "lr": "3.125e-06", "gnorm": "1.213", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "62"}
[2022-01-17 12:00:23,096][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:00:23,586][valid][INFO] - {"epoch": 2, "valid_loss": "6.926", "valid_ntokens": "1645", "valid_nsentences": "4.5", "valid_prob_perplexity": "391.206", "valid_code_perplexity": "383.057", "valid_temp": "1.998", "valid_loss_0": "6.665", "valid_loss_1": "0.056", "valid_loss_2": "0.204", "valid_accuracy": "0.01033", "valid_wps": "28683.2", "valid_wpb": "1645", "valid_bsz": "4.5", "valid_num_updates": "244", "valid_best_loss": "6.926"}
[2022-01-17 12:00:23,589][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 244 updates
[2022-01-17 12:00:23,591][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:00:27,555][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:00:37,150][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 244 updates, score 6.926) (writing took 13.560302005149424 seconds)
[2022-01-17 12:00:37,152][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-01-17 12:00:37,169][train][INFO] - {"epoch": 2, "train_loss": "7.205", "train_ntokens": "1797.7", "train_nsentences": "4.97541", "train_prob_perplexity": "336.127", "train_code_perplexity": "330.508", "train_temp": "1.998", "train_loss_0": "6.681", "train_loss_1": "0.068", "train_loss_2": "0.455", "train_accuracy": "0.01165", "train_wps": "5215.4", "train_ups": "2.9", "train_wpb": "1797.7", "train_bsz": "5", "train_num_updates": "244", "train_lr": "3.8125e-06", "train_gnorm": "0.527", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "85"}
[2022-01-17 12:00:37,265][fairseq.trainer][INFO] - begin training epoch 3
[2022-01-17 12:00:37,266][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:01:05,005][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:01:05,493][valid][INFO] - {"epoch": 3, "valid_loss": "6.767", "valid_ntokens": "1629.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "522.951", "valid_code_perplexity": "514.767", "valid_temp": "1.996", "valid_loss_0": "6.653", "valid_loss_1": "0.026", "valid_loss_2": "0.087", "valid_accuracy": "0.01258", "valid_wps": "29443.5", "valid_wpb": "1629.5", "valid_bsz": "4.5", "valid_num_updates": "366", "valid_best_loss": "6.767"}
[2022-01-17 12:01:05,496][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 366 updates
[2022-01-17 12:01:05,497][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:01:09,588][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:01:19,602][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 366 updates, score 6.767) (writing took 14.105747231282294 seconds)
[2022-01-17 12:01:19,603][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2022-01-17 12:01:19,617][train][INFO] - {"epoch": 3, "train_loss": "6.842", "train_ntokens": "1806.61", "train_nsentences": "4.97541", "train_prob_perplexity": "480.119", "train_code_perplexity": "472.823", "train_temp": "1.997", "train_loss_0": "6.663", "train_loss_1": "0.036", "train_loss_2": "0.143", "train_accuracy": "0.01216", "train_wps": "5194.1", "train_ups": "2.88", "train_wpb": "1806.6", "train_bsz": "5", "train_num_updates": "366", "train_lr": "5.71875e-06", "train_gnorm": "0.275", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "128"}
[2022-01-17 12:01:19,711][fairseq.trainer][INFO] - begin training epoch 4
[2022-01-17 12:01:19,712][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:01:27,749][train_inner][INFO] - {"epoch": 4, "update": 3.279, "loss": "6.863", "ntokens": "1803.88", "nsentences": "4.97", "prob_perplexity": "469.407", "code_perplexity": "462.295", "temp": "1.997", "loss_0": "6.664", "loss_1": "0.038", "loss_2": "0.161", "accuracy": "0.01221", "wps": "4843.8", "ups": "2.69", "wpb": "1803.9", "bsz": "5", "num_updates": "400", "lr": "6.25e-06", "gnorm": "0.289", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "136"}
[2022-01-17 12:01:47,512][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:01:47,919][valid][INFO] - {"epoch": 4, "valid_loss": "6.474", "valid_ntokens": "1622.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "435.774", "valid_code_perplexity": "426.635", "valid_temp": "1.995", "valid_loss_0": "6.363", "valid_loss_1": "0.046", "valid_loss_2": "0.065", "valid_accuracy": "0.03328", "valid_wps": "29563.2", "valid_wpb": "1622.5", "valid_bsz": "4.5", "valid_num_updates": "488", "valid_best_loss": "6.474"}
[2022-01-17 12:01:47,920][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 488 updates
[2022-01-17 12:01:47,921][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:01:51,823][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:02:01,633][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 488 updates, score 6.474) (writing took 13.712417146191001 seconds)
[2022-01-17 12:02:01,634][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2022-01-17 12:02:01,648][train][INFO] - {"epoch": 4, "train_loss": "6.724", "train_ntokens": "1802.33", "train_nsentences": "4.97541", "train_prob_perplexity": "530.471", "train_code_perplexity": "522.804", "train_temp": "1.996", "train_loss_0": "6.628", "train_loss_1": "0.025", "train_loss_2": "0.071", "train_accuracy": "0.01608", "train_wps": "5233.2", "train_ups": "2.9", "train_wpb": "1802.3", "train_bsz": "5", "train_num_updates": "488", "train_lr": "7.625e-06", "train_gnorm": "0.315", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "170"}
[2022-01-17 12:02:01,740][fairseq.trainer][INFO] - begin training epoch 5
[2022-01-17 12:02:01,740][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:02:27,443][train_inner][INFO] - {"epoch": 5, "update": 4.918, "loss": "6.115", "ntokens": "1796.27", "nsentences": "4.97", "prob_perplexity": "301.648", "code_perplexity": "296.974", "temp": "1.995", "loss_0": "5.957", "loss_1": "0.076", "loss_2": "0.082", "accuracy": "0.08946", "wps": "6019.6", "ups": "3.35", "wpb": "1796.3", "bsz": "5", "num_updates": "600", "lr": "9.375e-06", "gnorm": "0.558", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "196"}
[2022-01-17 12:02:29,703][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:02:30,123][valid][INFO] - {"epoch": 5, "valid_loss": "4.786", "valid_ntokens": "1634", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.953", "valid_code_perplexity": "18.749", "valid_temp": "1.994", "valid_loss_0": "4.557", "valid_loss_1": "0.14", "valid_loss_2": "0.089", "valid_accuracy": "0.39259", "valid_wps": "27693.6", "valid_wpb": "1634", "valid_bsz": "4.5", "valid_num_updates": "610", "valid_best_loss": "4.786"}
[2022-01-17 12:02:30,124][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 610 updates
[2022-01-17 12:02:30,125][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:02:34,120][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:02:42,978][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 610 updates, score 4.786) (writing took 12.853638557717204 seconds)
[2022-01-17 12:02:42,978][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2022-01-17 12:02:42,993][train][INFO] - {"epoch": 5, "train_loss": "5.596", "train_ntokens": "1795.48", "train_nsentences": "4.97541", "train_prob_perplexity": "113.959", "train_code_perplexity": "111.886", "train_temp": "1.995", "train_loss_0": "5.385", "train_loss_1": "0.118", "train_loss_2": "0.092", "train_accuracy": "0.15983", "train_wps": "5299.9", "train_ups": "2.95", "train_wpb": "1795.5", "train_bsz": "5", "train_num_updates": "610", "train_lr": "9.53125e-06", "train_gnorm": "0.716", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "211"}
[2022-01-17 12:02:43,082][fairseq.trainer][INFO] - begin training epoch 6
[2022-01-17 12:02:43,082][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:03:10,958][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:03:11,398][valid][INFO] - {"epoch": 6, "valid_loss": "4.64", "valid_ntokens": "1613.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "14.31", "valid_code_perplexity": "14.206", "valid_temp": "1.993", "valid_loss_0": "4.432", "valid_loss_1": "0.141", "valid_loss_2": "0.067", "valid_accuracy": "0.4295", "valid_wps": "28244.1", "valid_wpb": "1613.5", "valid_bsz": "4.5", "valid_num_updates": "732", "valid_best_loss": "4.64"}
[2022-01-17 12:03:11,400][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 732 updates
[2022-01-17 12:03:11,401][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:03:15,291][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:03:24,736][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 732 updates, score 4.64) (writing took 13.335938619449735 seconds)
[2022-01-17 12:03:24,737][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2022-01-17 12:03:24,749][train][INFO] - {"epoch": 6, "train_loss": "4.798", "train_ntokens": "1805", "train_nsentences": "4.97541", "train_prob_perplexity": "18.058", "train_code_perplexity": "17.884", "train_temp": "1.993", "train_loss_0": "4.582", "train_loss_1": "0.14", "train_loss_2": "0.076", "train_accuracy": "0.37887", "train_wps": "5275.3", "train_ups": "2.92", "train_wpb": "1805", "train_bsz": "5", "train_num_updates": "732", "train_lr": "1.14375e-05", "train_gnorm": "0.689", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "253"}
[2022-01-17 12:03:24,832][fairseq.trainer][INFO] - begin training epoch 7
[2022-01-17 12:03:24,833][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:03:40,486][train_inner][INFO] - {"epoch": 7, "update": 6.557, "loss": "4.723", "ntokens": "1800.34", "nsentences": "4.97", "prob_perplexity": "17.579", "code_perplexity": "17.427", "temp": "1.993", "loss_0": "4.509", "loss_1": "0.14", "loss_2": "0.074", "accuracy": "0.39716", "wps": "4930.5", "ups": "2.74", "wpb": "1800.3", "bsz": "5", "num_updates": "800", "lr": "1.25e-05", "gnorm": "0.874", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "269"}
[2022-01-17 12:03:52,739][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:03:53,167][valid][INFO] - {"epoch": 7, "valid_loss": "4.368", "valid_ntokens": "1633.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "15.011", "valid_code_perplexity": "14.994", "valid_temp": "1.991", "valid_loss_0": "4.159", "valid_loss_1": "0.141", "valid_loss_2": "0.068", "valid_accuracy": "0.46832", "valid_wps": "29658.3", "valid_wpb": "1633.5", "valid_bsz": "4.5", "valid_num_updates": "854", "valid_best_loss": "4.368"}
[2022-01-17 12:03:53,169][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 854 updates
[2022-01-17 12:03:53,169][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:03:57,186][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:04:06,807][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 854 updates, score 4.368) (writing took 13.637837102636695 seconds)
[2022-01-17 12:04:06,808][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2022-01-17 12:04:06,822][train][INFO] - {"epoch": 7, "train_loss": "4.49", "train_ntokens": "1800.59", "train_nsentences": "4.97541", "train_prob_perplexity": "15.757", "train_code_perplexity": "15.67", "train_temp": "1.992", "train_loss_0": "4.282", "train_loss_1": "0.141", "train_loss_2": "0.067", "train_accuracy": "0.45117", "train_wps": "5222.9", "train_ups": "2.9", "train_wpb": "1800.6", "train_bsz": "5", "train_num_updates": "854", "train_lr": "1.33438e-05", "train_gnorm": "1.459", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "295"}
[2022-01-17 12:04:06,915][fairseq.trainer][INFO] - begin training epoch 8
[2022-01-17 12:04:06,915][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:04:34,694][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:04:35,143][valid][INFO] - {"epoch": 8, "valid_loss": "4.273", "valid_ntokens": "1635.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "15.782", "valid_code_perplexity": "15.725", "valid_temp": "1.99", "valid_loss_0": "4.074", "valid_loss_1": "0.141", "valid_loss_2": "0.058", "valid_accuracy": "0.46102", "valid_wps": "29298.8", "valid_wpb": "1635.5", "valid_bsz": "4.5", "valid_num_updates": "976", "valid_best_loss": "4.273"}
[2022-01-17 12:04:35,145][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 976 updates
[2022-01-17 12:04:35,145][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:04:39,045][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:04:48,074][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 976 updates, score 4.273) (writing took 12.92890321277082 seconds)
[2022-01-17 12:04:48,074][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2022-01-17 12:04:48,088][train][INFO] - {"epoch": 8, "train_loss": "4.384", "train_ntokens": "1802.02", "train_nsentences": "4.97541", "train_prob_perplexity": "15.282", "train_code_perplexity": "15.222", "train_temp": "1.991", "train_loss_0": "4.18", "train_loss_1": "0.141", "train_loss_2": "0.062", "train_accuracy": "0.45989", "train_wps": "5329.3", "train_ups": "2.96", "train_wpb": "1802", "train_bsz": "5", "train_num_updates": "976", "train_lr": "1.525e-05", "train_gnorm": "1.807", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "336"}
[2022-01-17 12:04:48,167][fairseq.trainer][INFO] - begin training epoch 9
[2022-01-17 12:04:48,167][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:04:53,863][train_inner][INFO] - {"epoch": 9, "update": 8.197, "loss": "4.385", "ntokens": "1808.65", "nsentences": "4.985", "prob_perplexity": "15.329", "code_perplexity": "15.267", "temp": "1.991", "loss_0": "4.181", "loss_1": "0.141", "loss_2": "0.063", "accuracy": "0.46134", "wps": "4930.6", "ups": "2.73", "wpb": "1808.7", "bsz": "5", "num_updates": "1000", "lr": "1.5625e-05", "gnorm": "1.792", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "342"}
[2022-01-17 12:05:16,086][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:05:16,503][valid][INFO] - {"epoch": 9, "valid_loss": "4.278", "valid_ntokens": "1568", "valid_nsentences": "4.5", "valid_prob_perplexity": "15.35", "valid_code_perplexity": "15.359", "valid_temp": "1.989", "valid_loss_0": "4.083", "valid_loss_1": "0.141", "valid_loss_2": "0.053", "valid_accuracy": "0.4764", "valid_wps": "26494", "valid_wpb": "1568", "valid_bsz": "4.5", "valid_num_updates": "1098", "valid_best_loss": "4.273"}
[2022-01-17 12:05:16,505][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 1098 updates
[2022-01-17 12:05:16,506][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:05:20,382][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:05:20,402][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 9 @ 1098 updates, score 4.278) (writing took 3.8972699716687202 seconds)
[2022-01-17 12:05:20,403][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2022-01-17 12:05:20,419][train][INFO] - {"epoch": 9, "train_loss": "4.295", "train_ntokens": "1807.86", "train_nsentences": "4.97541", "train_prob_perplexity": "15.15", "train_code_perplexity": "15.104", "train_temp": "1.99", "train_loss_0": "4.099", "train_loss_1": "0.141", "train_loss_2": "0.055", "train_accuracy": "0.46996", "train_wps": "6825.4", "train_ups": "3.78", "train_wpb": "1807.9", "train_bsz": "5", "train_num_updates": "1098", "train_lr": "1.71563e-05", "train_gnorm": "1.779", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "369"}
[2022-01-17 12:05:20,468][fairseq.trainer][INFO] - begin training epoch 10
[2022-01-17 12:05:20,468][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:05:44,000][train_inner][INFO] - {"epoch": 10, "update": 9.836, "loss": "4.298", "ntokens": "1800.47", "nsentences": "4.97", "prob_perplexity": "15.236", "code_perplexity": "15.19", "temp": "1.989", "loss_0": "4.105", "loss_1": "0.141", "loss_2": "0.052", "accuracy": "0.4653", "wps": "7184", "ups": "3.99", "wpb": "1800.5", "bsz": "5", "num_updates": "1200", "lr": "1.875e-05", "gnorm": "1.721", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "392"}
[2022-01-17 12:05:48,536][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:05:48,997][valid][INFO] - {"epoch": 10, "valid_loss": "4.191", "valid_ntokens": "1657", "valid_nsentences": "4.5", "valid_prob_perplexity": "14.945", "valid_code_perplexity": "14.935", "valid_temp": "1.988", "valid_loss_0": "4.001", "valid_loss_1": "0.141", "valid_loss_2": "0.048", "valid_accuracy": "0.48552", "valid_wps": "30174.8", "valid_wpb": "1657", "valid_bsz": "4.5", "valid_num_updates": "1220", "valid_best_loss": "4.191"}
[2022-01-17 12:05:48,999][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 1220 updates
[2022-01-17 12:05:49,000][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:05:52,892][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:06:02,697][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 1220 updates, score 4.191) (writing took 13.698163654655218 seconds)
[2022-01-17 12:06:02,698][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2022-01-17 12:06:02,711][train][INFO] - {"epoch": 10, "train_loss": "4.306", "train_ntokens": "1805.5", "train_nsentences": "4.97541", "train_prob_perplexity": "15.315", "train_code_perplexity": "15.27", "train_temp": "1.988", "train_loss_0": "4.115", "train_loss_1": "0.141", "train_loss_2": "0.05", "train_accuracy": "0.45895", "train_wps": "5209.9", "train_ups": "2.89", "train_wpb": "1805.5", "train_bsz": "5", "train_num_updates": "1220", "train_lr": "1.90625e-05", "train_gnorm": "1.678", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "411"}
[2022-01-17 12:06:02,796][fairseq.trainer][INFO] - begin training epoch 11
[2022-01-17 12:06:02,797][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:06:31,172][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:06:31,620][valid][INFO] - {"epoch": 11, "valid_loss": "4.129", "valid_ntokens": "1655.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "15.466", "valid_code_perplexity": "15.424", "valid_temp": "1.987", "valid_loss_0": "3.943", "valid_loss_1": "0.141", "valid_loss_2": "0.046", "valid_accuracy": "0.47992", "valid_wps": "28829.4", "valid_wpb": "1655.5", "valid_bsz": "4.5", "valid_num_updates": "1342", "valid_best_loss": "4.129"}
[2022-01-17 12:06:31,622][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 1342 updates
[2022-01-17 12:06:31,622][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:06:35,541][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:06:45,534][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 1342 updates, score 4.129) (writing took 13.91238153539598 seconds)
[2022-01-17 12:06:45,535][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2022-01-17 12:06:45,549][train][INFO] - {"epoch": 11, "train_loss": "4.316", "train_ntokens": "1805.39", "train_nsentences": "4.97541", "train_prob_perplexity": "15.729", "train_code_perplexity": "15.694", "train_temp": "1.987", "train_loss_0": "4.129", "train_loss_1": "0.141", "train_loss_2": "0.047", "train_accuracy": "0.45047", "train_wps": "5143.2", "train_ups": "2.85", "train_wpb": "1805.4", "train_bsz": "5", "train_num_updates": "1342", "train_lr": "2.09688e-05", "train_gnorm": "1.631", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "454"}
[2022-01-17 12:06:45,637][fairseq.trainer][INFO] - begin training epoch 12
[2022-01-17 12:06:45,638][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:06:59,075][train_inner][INFO] - {"epoch": 12, "update": 11.475, "loss": "4.31", "ntokens": "1808.09", "nsentences": "4.985", "prob_perplexity": "15.809", "code_perplexity": "15.771", "temp": "1.987", "loss_0": "4.123", "loss_1": "0.141", "loss_2": "0.046", "accuracy": "0.44842", "wps": "4817.6", "ups": "2.66", "wpb": "1808.1", "bsz": "5", "num_updates": "1400", "lr": "2.1875e-05", "gnorm": "1.635", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "467"}
[2022-01-17 12:07:13,478][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:07:13,894][valid][INFO] - {"epoch": 12, "valid_loss": "4.155", "valid_ntokens": "1570.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "16.655", "valid_code_perplexity": "16.642", "valid_temp": "1.985", "valid_loss_0": "3.969", "valid_loss_1": "0.141", "valid_loss_2": "0.045", "valid_accuracy": "0.45336", "valid_wps": "27934.8", "valid_wpb": "1570.5", "valid_bsz": "4.5", "valid_num_updates": "1464", "valid_best_loss": "4.129"}
[2022-01-17 12:07:13,896][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 1464 updates
[2022-01-17 12:07:13,896][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:07:17,828][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:07:17,854][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 12 @ 1464 updates, score 4.155) (writing took 3.95788761600852 seconds)
[2022-01-17 12:07:17,854][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2022-01-17 12:07:17,867][train][INFO] - {"epoch": 12, "train_loss": "4.316", "train_ntokens": "1798", "train_nsentences": "4.97541", "train_prob_perplexity": "16.327", "train_code_perplexity": "16.288", "train_temp": "1.986", "train_loss_0": "4.131", "train_loss_1": "0.141", "train_loss_2": "0.045", "train_accuracy": "0.43352", "train_wps": "6790.2", "train_ups": "3.78", "train_wpb": "1798", "train_bsz": "5", "train_num_updates": "1464", "train_lr": "2.2875e-05", "train_gnorm": "1.625", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "486"}
[2022-01-17 12:07:17,938][fairseq.trainer][INFO] - begin training epoch 13
[2022-01-17 12:07:17,939][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:07:45,933][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:07:46,418][valid][INFO] - {"epoch": 13, "valid_loss": "4.263", "valid_ntokens": "1655.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "16.945", "valid_code_perplexity": "16.945", "valid_temp": "1.984", "valid_loss_0": "4.081", "valid_loss_1": "0.14", "valid_loss_2": "0.042", "valid_accuracy": "0.41075", "valid_wps": "27692.2", "valid_wpb": "1655.5", "valid_bsz": "4.5", "valid_num_updates": "1586", "valid_best_loss": "4.129"}
[2022-01-17 12:07:46,421][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 1586 updates
[2022-01-17 12:07:46,422][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:07:50,473][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:07:50,500][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 13 @ 1586 updates, score 4.263) (writing took 4.079655668698251 seconds)
[2022-01-17 12:07:50,501][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2022-01-17 12:07:50,514][train][INFO] - {"epoch": 13, "train_loss": "4.327", "train_ntokens": "1801.28", "train_nsentences": "4.97541", "train_prob_perplexity": "16.839", "train_code_perplexity": "16.807", "train_temp": "1.985", "train_loss_0": "4.143", "train_loss_1": "0.14", "train_loss_2": "0.044", "train_accuracy": "0.41375", "train_wps": "6734", "train_ups": "3.74", "train_wpb": "1801.3", "train_bsz": "5", "train_num_updates": "1586", "train_lr": "2.47813e-05", "train_gnorm": "1.685", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "519"}
[2022-01-17 12:07:50,590][fairseq.trainer][INFO] - begin training epoch 14
[2022-01-17 12:07:50,591][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:07:54,110][train_inner][INFO] - {"epoch": 14, "update": 13.115, "loss": "4.329", "ntokens": "1799.34", "nsentences": "4.97", "prob_perplexity": "16.795", "code_perplexity": "16.761", "temp": "1.985", "loss_0": "4.145", "loss_1": "0.14", "loss_2": "0.044", "accuracy": "0.41698", "wps": "6540.4", "ups": "3.63", "wpb": "1799.3", "bsz": "5", "num_updates": "1600", "lr": "2.5e-05", "gnorm": "1.656", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "522"}
[2022-01-17 12:08:18,490][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:08:18,941][valid][INFO] - {"epoch": 14, "valid_loss": "4.225", "valid_ntokens": "1659", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.172", "valid_code_perplexity": "18.162", "valid_temp": "1.983", "valid_loss_0": "4.045", "valid_loss_1": "0.14", "valid_loss_2": "0.04", "valid_accuracy": "0.40506", "valid_wps": "29286.9", "valid_wpb": "1659", "valid_bsz": "4.5", "valid_num_updates": "1708", "valid_best_loss": "4.129"}
[2022-01-17 12:08:18,943][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 1708 updates
[2022-01-17 12:08:18,944][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:08:23,028][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:08:23,054][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 14 @ 1708 updates, score 4.225) (writing took 4.110904068686068 seconds)
[2022-01-17 12:08:23,055][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2022-01-17 12:08:23,067][train][INFO] - {"epoch": 14, "train_loss": "4.328", "train_ntokens": "1803.11", "train_nsentences": "4.97541", "train_prob_perplexity": "17.652", "train_code_perplexity": "17.625", "train_temp": "1.984", "train_loss_0": "4.146", "train_loss_1": "0.14", "train_loss_2": "0.041", "train_accuracy": "0.40183", "train_wps": "6760.1", "train_ups": "3.75", "train_wpb": "1803.1", "train_bsz": "5", "train_num_updates": "1708", "train_lr": "2.66875e-05", "train_gnorm": "1.615", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "551"}
[2022-01-17 12:08:23,126][fairseq.trainer][INFO] - begin training epoch 15
[2022-01-17 12:08:23,127][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:08:44,535][train_inner][INFO] - {"epoch": 15, "update": 14.754, "loss": "4.323", "ntokens": "1804.05", "nsentences": "4.985", "prob_perplexity": "17.82", "code_perplexity": "17.796", "temp": "1.983", "loss_0": "4.142", "loss_1": "0.14", "loss_2": "0.041", "accuracy": "0.39876", "wps": "7157.2", "ups": "3.97", "wpb": "1804", "bsz": "5", "num_updates": "1800", "lr": "2.8125e-05", "gnorm": "1.619", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "573"}
[2022-01-17 12:08:51,272][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:08:51,740][valid][INFO] - {"epoch": 15, "valid_loss": "4.248", "valid_ntokens": "1643", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.421", "valid_code_perplexity": "18.384", "valid_temp": "1.982", "valid_loss_0": "4.068", "valid_loss_1": "0.14", "valid_loss_2": "0.039", "valid_accuracy": "0.40231", "valid_wps": "27842.2", "valid_wpb": "1643", "valid_bsz": "4.5", "valid_num_updates": "1830", "valid_best_loss": "4.129"}
[2022-01-17 12:08:51,742][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 1830 updates
[2022-01-17 12:08:51,743][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:08:55,615][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:08:55,622][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 15 @ 1830 updates, score 4.248) (writing took 3.8798589911311865 seconds)
[2022-01-17 12:08:55,623][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2022-01-17 12:08:55,635][train][INFO] - {"epoch": 15, "train_loss": "4.299", "train_ntokens": "1799.11", "train_nsentences": "4.97541", "train_prob_perplexity": "17.917", "train_code_perplexity": "17.896", "train_temp": "1.982", "train_loss_0": "4.118", "train_loss_1": "0.14", "train_loss_2": "0.041", "train_accuracy": "0.39908", "train_wps": "6742.1", "train_ups": "3.75", "train_wpb": "1799.1", "train_bsz": "5", "train_num_updates": "1830", "train_lr": "2.85938e-05", "train_gnorm": "1.627", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "584"}
[2022-01-17 12:08:55,689][fairseq.trainer][INFO] - begin training epoch 16
[2022-01-17 12:08:55,690][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:09:23,505][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:09:23,958][valid][INFO] - {"epoch": 16, "valid_loss": "4.353", "valid_ntokens": "1635", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.55", "valid_code_perplexity": "18.531", "valid_temp": "1.981", "valid_loss_0": "4.174", "valid_loss_1": "0.14", "valid_loss_2": "0.039", "valid_accuracy": "0.38226", "valid_wps": "28439.9", "valid_wpb": "1635", "valid_bsz": "4.5", "valid_num_updates": "1952", "valid_best_loss": "4.129"}
[2022-01-17 12:09:23,960][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 1952 updates
[2022-01-17 12:09:23,960][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:09:27,970][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:09:27,988][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 16 @ 1952 updates, score 4.353) (writing took 4.028825365938246 seconds)
[2022-01-17 12:09:27,989][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2022-01-17 12:09:28,002][train][INFO] - {"epoch": 16, "train_loss": "4.293", "train_ntokens": "1800.78", "train_nsentences": "4.97541", "train_prob_perplexity": "18.274", "train_code_perplexity": "18.248", "train_temp": "1.981", "train_loss_0": "4.115", "train_loss_1": "0.14", "train_loss_2": "0.038", "train_accuracy": "0.39236", "train_wps": "6790.3", "train_ups": "3.77", "train_wpb": "1800.8", "train_bsz": "5", "train_num_updates": "1952", "train_lr": "3.05e-05", "train_gnorm": "1.674", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "616"}
[2022-01-17 12:09:28,048][fairseq.trainer][INFO] - begin training epoch 17
[2022-01-17 12:09:28,049][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:09:39,302][train_inner][INFO] - {"epoch": 17, "update": 16.393, "loss": "4.28", "ntokens": "1794.75", "nsentences": "4.955", "prob_perplexity": "18.205", "code_perplexity": "18.181", "temp": "1.981", "loss_0": "4.102", "loss_1": "0.14", "loss_2": "0.039", "accuracy": "0.39523", "wps": "6555.6", "ups": "3.65", "wpb": "1794.8", "bsz": "5", "num_updates": "2000", "lr": "3.125e-05", "gnorm": "1.645", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "628"}
[2022-01-17 12:09:56,041][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:09:56,535][valid][INFO] - {"epoch": 17, "valid_loss": "4.262", "valid_ntokens": "1676", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.447", "valid_code_perplexity": "19.454", "valid_temp": "1.979", "valid_loss_0": "4.088", "valid_loss_1": "0.14", "valid_loss_2": "0.034", "valid_accuracy": "0.37977", "valid_wps": "30362.4", "valid_wpb": "1676", "valid_bsz": "4.5", "valid_num_updates": "2074", "valid_best_loss": "4.129"}
[2022-01-17 12:09:56,537][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 2074 updates
[2022-01-17 12:09:56,538][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:10:00,652][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:10:00,679][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 17 @ 2074 updates, score 4.262) (writing took 4.14231956936419 seconds)
[2022-01-17 12:10:00,680][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2022-01-17 12:10:00,694][train][INFO] - {"epoch": 17, "train_loss": "4.265", "train_ntokens": "1805.85", "train_nsentences": "4.97541", "train_prob_perplexity": "18.918", "train_code_perplexity": "18.897", "train_temp": "1.98", "train_loss_0": "4.088", "train_loss_1": "0.14", "train_loss_2": "0.036", "train_accuracy": "0.39109", "train_wps": "6741.9", "train_ups": "3.73", "train_wpb": "1805.9", "train_bsz": "5", "train_num_updates": "2074", "train_lr": "3.24062e-05", "train_gnorm": "1.585", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "649"}
[2022-01-17 12:10:00,759][fairseq.trainer][INFO] - begin training epoch 18
[2022-01-17 12:10:00,759][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:10:28,630][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:10:29,087][valid][INFO] - {"epoch": 18, "valid_loss": "4.269", "valid_ntokens": "1703", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.137", "valid_code_perplexity": "19.149", "valid_temp": "1.978", "valid_loss_0": "4.092", "valid_loss_1": "0.14", "valid_loss_2": "0.037", "valid_accuracy": "0.39489", "valid_wps": "29101.6", "valid_wpb": "1703", "valid_bsz": "4.5", "valid_num_updates": "2196", "valid_best_loss": "4.129"}
[2022-01-17 12:10:29,089][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 2196 updates
[2022-01-17 12:10:29,090][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:10:33,175][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:10:33,203][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 18 @ 2196 updates, score 4.269) (writing took 4.113137298263609 seconds)
[2022-01-17 12:10:33,203][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2022-01-17 12:10:33,216][train][INFO] - {"epoch": 18, "train_loss": "4.249", "train_ntokens": "1805.91", "train_nsentences": "4.97541", "train_prob_perplexity": "19.696", "train_code_perplexity": "19.681", "train_temp": "1.979", "train_loss_0": "4.074", "train_loss_1": "0.14", "train_loss_2": "0.035", "train_accuracy": "0.38699", "train_wps": "6777.1", "train_ups": "3.75", "train_wpb": "1805.9", "train_bsz": "5", "train_num_updates": "2196", "train_lr": "3.43125e-05", "train_gnorm": "1.621", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "682"}
[2022-01-17 12:10:33,312][fairseq.trainer][INFO] - begin training epoch 19
[2022-01-17 12:10:33,313][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:10:34,562][train_inner][INFO] - {"epoch": 19, "update": 18.033, "loss": "4.252", "ntokens": "1809.31", "nsentences": "4.985", "prob_perplexity": "19.564", "code_perplexity": "19.546", "temp": "1.979", "loss_0": "4.077", "loss_1": "0.14", "loss_2": "0.035", "accuracy": "0.38776", "wps": "6549.9", "ups": "3.62", "wpb": "1809.3", "bsz": "5", "num_updates": "2200", "lr": "3.4375e-05", "gnorm": "1.612", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "683"}
[2022-01-17 12:11:01,260][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:11:01,740][valid][INFO] - {"epoch": 19, "valid_loss": "4.269", "valid_ntokens": "1606.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.934", "valid_code_perplexity": "19.927", "valid_temp": "1.977", "valid_loss_0": "4.093", "valid_loss_1": "0.14", "valid_loss_2": "0.037", "valid_accuracy": "0.39185", "valid_wps": "28438.7", "valid_wpb": "1606.5", "valid_bsz": "4.5", "valid_num_updates": "2318", "valid_best_loss": "4.129"}
[2022-01-17 12:11:01,741][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 2318 updates
[2022-01-17 12:11:01,742][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:11:05,764][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:11:05,788][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 19 @ 2318 updates, score 4.269) (writing took 4.047118611633778 seconds)
[2022-01-17 12:11:05,789][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2022-01-17 12:11:05,801][train][INFO] - {"epoch": 19, "train_loss": "4.254", "train_ntokens": "1798.25", "train_nsentences": "4.97541", "train_prob_perplexity": "20.274", "train_code_perplexity": "20.255", "train_temp": "1.978", "train_loss_0": "4.079", "train_loss_1": "0.14", "train_loss_2": "0.035", "train_accuracy": "0.38397", "train_wps": "6735.2", "train_ups": "3.75", "train_wpb": "1798.3", "train_bsz": "5", "train_num_updates": "2318", "train_lr": "3.62188e-05", "train_gnorm": "1.577", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "714"}
[2022-01-17 12:11:05,855][fairseq.trainer][INFO] - begin training epoch 20
[2022-01-17 12:11:05,856][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:11:24,523][train_inner][INFO] - {"epoch": 20, "update": 19.672, "loss": "4.242", "ntokens": "1797.99", "nsentences": "4.97", "prob_perplexity": "20.323", "code_perplexity": "20.302", "temp": "1.977", "loss_0": "4.067", "loss_1": "0.14", "loss_2": "0.035", "accuracy": "0.38501", "wps": "7199.4", "ups": "4", "wpb": "1798", "bsz": "5", "num_updates": "2400", "lr": "3.75e-05", "gnorm": "1.59", "clip": "0", "train_wall": "45", "gb_free": "14.5", "wall": "733"}
[2022-01-17 12:11:33,601][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:11:34,095][valid][INFO] - {"epoch": 20, "valid_loss": "3.995", "valid_ntokens": "1560", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.003", "valid_code_perplexity": "18.977", "valid_temp": "1.976", "valid_loss_0": "3.822", "valid_loss_1": "0.14", "valid_loss_2": "0.034", "valid_accuracy": "0.42628", "valid_wps": "29446", "valid_wpb": "1560", "valid_bsz": "4.5", "valid_num_updates": "2440", "valid_best_loss": "3.995"}
[2022-01-17 12:11:34,097][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 2440 updates
[2022-01-17 12:11:34,098][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:11:38,059][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:11:47,693][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 2440 updates, score 3.995) (writing took 13.595737097784877 seconds)
[2022-01-17 12:11:47,694][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2022-01-17 12:11:47,708][train][INFO] - {"epoch": 20, "train_loss": "4.221", "train_ntokens": "1803.34", "train_nsentences": "4.97541", "train_prob_perplexity": "20.394", "train_code_perplexity": "20.373", "train_temp": "1.976", "train_loss_0": "4.047", "train_loss_1": "0.14", "train_loss_2": "0.034", "train_accuracy": "0.387", "train_wps": "5251.7", "train_ups": "2.91", "train_wpb": "1803.3", "train_bsz": "5", "train_num_updates": "2440", "train_lr": "3.8125e-05", "train_gnorm": "1.609", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "756"}
[2022-01-17 12:11:47,797][fairseq.trainer][INFO] - begin training epoch 21
[2022-01-17 12:11:47,798][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:12:15,654][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:12:16,160][valid][INFO] - {"epoch": 21, "valid_loss": "4.171", "valid_ntokens": "1643", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.015", "valid_code_perplexity": "19.003", "valid_temp": "1.975", "valid_loss_0": "3.999", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.40444", "valid_wps": "28641.2", "valid_wpb": "1643", "valid_bsz": "4.5", "valid_num_updates": "2562", "valid_best_loss": "3.995"}
[2022-01-17 12:12:16,161][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 2562 updates
[2022-01-17 12:12:16,162][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:12:20,203][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:12:20,224][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 21 @ 2562 updates, score 4.171) (writing took 4.0621439116075635 seconds)
[2022-01-17 12:12:20,224][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2022-01-17 12:12:20,237][train][INFO] - {"epoch": 21, "train_loss": "4.206", "train_ntokens": "1810.59", "train_nsentences": "4.97541", "train_prob_perplexity": "20.329", "train_code_perplexity": "20.313", "train_temp": "1.975", "train_loss_0": "4.033", "train_loss_1": "0.14", "train_loss_2": "0.033", "train_accuracy": "0.39164", "train_wps": "6793.3", "train_ups": "3.75", "train_wpb": "1810.6", "train_bsz": "5", "train_num_updates": "2562", "train_lr": "4.00313e-05", "train_gnorm": "1.598", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "789"}
[2022-01-17 12:12:20,279][fairseq.trainer][INFO] - begin training epoch 22
[2022-01-17 12:12:20,279][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:12:29,100][train_inner][INFO] - {"epoch": 22, "update": 21.311, "loss": "4.2", "ntokens": "1806.11", "nsentences": "4.97", "prob_perplexity": "20.263", "code_perplexity": "20.247", "temp": "1.975", "loss_0": "4.027", "loss_1": "0.14", "loss_2": "0.033", "accuracy": "0.39191", "wps": "5594.8", "ups": "3.1", "wpb": "1806.1", "bsz": "5", "num_updates": "2600", "lr": "4.0625e-05", "gnorm": "1.61", "clip": "0", "train_wall": "45", "gb_free": "19.4", "wall": "797"}
[2022-01-17 12:12:48,178][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:12:48,637][valid][INFO] - {"epoch": 22, "valid_loss": "4.098", "valid_ntokens": "1592", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.129", "valid_code_perplexity": "20.138", "valid_temp": "1.973", "valid_loss_0": "3.927", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.40861", "valid_wps": "28711.7", "valid_wpb": "1592", "valid_bsz": "4.5", "valid_num_updates": "2684", "valid_best_loss": "3.995"}
[2022-01-17 12:12:48,639][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 2684 updates
[2022-01-17 12:12:48,640][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:12:52,592][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:12:52,610][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 22 @ 2684 updates, score 4.098) (writing took 3.9711100002750754 seconds)
[2022-01-17 12:12:52,611][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2022-01-17 12:12:52,623][train][INFO] - {"epoch": 22, "train_loss": "4.163", "train_ntokens": "1804.66", "train_nsentences": "4.97541", "train_prob_perplexity": "20.068", "train_code_perplexity": "20.055", "train_temp": "1.974", "train_loss_0": "3.991", "train_loss_1": "0.14", "train_loss_2": "0.032", "train_accuracy": "0.39881", "train_wps": "6800.8", "train_ups": "3.77", "train_wpb": "1804.7", "train_bsz": "5", "train_num_updates": "2684", "train_lr": "4.19375e-05", "train_gnorm": "1.659", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "821"}
[2022-01-17 12:12:52,671][fairseq.trainer][INFO] - begin training epoch 23
[2022-01-17 12:12:52,672][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:13:19,229][train_inner][INFO] - {"epoch": 23, "update": 22.951, "loss": "4.174", "ntokens": "1804.61", "nsentences": "4.985", "prob_perplexity": "20.117", "code_perplexity": "20.104", "temp": "1.973", "loss_0": "4.003", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.39491", "wps": "7202", "ups": "3.99", "wpb": "1804.6", "bsz": "5", "num_updates": "2800", "lr": "4.375e-05", "gnorm": "1.625", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "848"}
[2022-01-17 12:13:20,552][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:13:21,000][valid][INFO] - {"epoch": 23, "valid_loss": "4.032", "valid_ntokens": "1565", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.75", "valid_code_perplexity": "20.736", "valid_temp": "1.972", "valid_loss_0": "3.86", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.42716", "valid_wps": "27017.1", "valid_wpb": "1565", "valid_bsz": "4.5", "valid_num_updates": "2806", "valid_best_loss": "3.995"}
[2022-01-17 12:13:21,002][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 2806 updates
[2022-01-17 12:13:21,002][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:13:24,935][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:13:24,949][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 23 @ 2806 updates, score 4.032) (writing took 3.947626614011824 seconds)
[2022-01-17 12:13:24,950][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2022-01-17 12:13:24,963][train][INFO] - {"epoch": 23, "train_loss": "4.178", "train_ntokens": "1798.75", "train_nsentences": "4.97541", "train_prob_perplexity": "20.17", "train_code_perplexity": "20.157", "train_temp": "1.973", "train_loss_0": "4.007", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.39213", "train_wps": "6788.5", "train_ups": "3.77", "train_wpb": "1798.7", "train_bsz": "5", "train_num_updates": "2806", "train_lr": "4.38437e-05", "train_gnorm": "1.594", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "853"}
[2022-01-17 12:13:25,009][fairseq.trainer][INFO] - begin training epoch 24
[2022-01-17 12:13:25,010][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:13:52,807][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:13:53,274][valid][INFO] - {"epoch": 24, "valid_loss": "4.166", "valid_ntokens": "1612.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.635", "valid_code_perplexity": "21.633", "valid_temp": "1.971", "valid_loss_0": "3.996", "valid_loss_1": "0.139", "valid_loss_2": "0.031", "valid_accuracy": "0.39039", "valid_wps": "28091.3", "valid_wpb": "1612.5", "valid_bsz": "4.5", "valid_num_updates": "2928", "valid_best_loss": "3.995"}
[2022-01-17 12:13:53,277][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 2928 updates
[2022-01-17 12:13:53,277][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:13:57,252][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:13:57,275][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 24 @ 2928 updates, score 4.166) (writing took 3.9986187163740396 seconds)
[2022-01-17 12:13:57,276][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2022-01-17 12:13:57,289][train][INFO] - {"epoch": 24, "train_loss": "4.168", "train_ntokens": "1801.61", "train_nsentences": "4.97541", "train_prob_perplexity": "20.423", "train_code_perplexity": "20.409", "train_temp": "1.972", "train_loss_0": "3.997", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.3924", "train_wps": "6802", "train_ups": "3.78", "train_wpb": "1801.6", "train_bsz": "5", "train_num_updates": "2928", "train_lr": "4.575e-05", "train_gnorm": "1.589", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "886"}
[2022-01-17 12:13:57,373][fairseq.trainer][INFO] - begin training epoch 25
[2022-01-17 12:13:57,373][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:14:14,044][train_inner][INFO] - {"epoch": 25, "update": 24.59, "loss": "4.153", "ntokens": "1803.38", "nsentences": "4.985", "prob_perplexity": "20.513", "code_perplexity": "20.5", "temp": "1.971", "loss_0": "3.983", "loss_1": "0.14", "loss_2": "0.031", "accuracy": "0.39349", "wps": "6581.4", "ups": "3.65", "wpb": "1803.4", "bsz": "5", "num_updates": "3000", "lr": "4.6875e-05", "gnorm": "1.592", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "902"}
[2022-01-17 12:14:25,220][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:14:25,688][valid][INFO] - {"epoch": 25, "valid_loss": "4.164", "valid_ntokens": "1628", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.327", "valid_code_perplexity": "21.292", "valid_temp": "1.97", "valid_loss_0": "3.993", "valid_loss_1": "0.139", "valid_loss_2": "0.031", "valid_accuracy": "0.38237", "valid_wps": "28031", "valid_wpb": "1628", "valid_bsz": "4.5", "valid_num_updates": "3050", "valid_best_loss": "3.995"}
[2022-01-17 12:14:25,690][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 3050 updates
[2022-01-17 12:14:25,691][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:14:29,653][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:14:29,672][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 25 @ 3050 updates, score 4.164) (writing took 3.9816002836450934 seconds)
[2022-01-17 12:14:29,673][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2022-01-17 12:14:29,686][train][INFO] - {"epoch": 25, "train_loss": "4.142", "train_ntokens": "1804.75", "train_nsentences": "4.97541", "train_prob_perplexity": "20.324", "train_code_perplexity": "20.311", "train_temp": "1.97", "train_loss_0": "3.972", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.39501", "train_wps": "6799.1", "train_ups": "3.77", "train_wpb": "1804.7", "train_bsz": "5", "train_num_updates": "3050", "train_lr": "4.76562e-05", "train_gnorm": "1.625", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "918"}
[2022-01-17 12:14:29,748][fairseq.trainer][INFO] - begin training epoch 26
[2022-01-17 12:14:29,749][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:14:57,650][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:14:58,100][valid][INFO] - {"epoch": 26, "valid_loss": "4.052", "valid_ntokens": "1642.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.227", "valid_code_perplexity": "20.229", "valid_temp": "1.969", "valid_loss_0": "3.882", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.42131", "valid_wps": "27860.5", "valid_wpb": "1642.5", "valid_bsz": "4.5", "valid_num_updates": "3172", "valid_best_loss": "3.995"}
[2022-01-17 12:14:58,102][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 3172 updates
[2022-01-17 12:14:58,102][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:15:02,038][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:15:02,058][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 26 @ 3172 updates, score 4.052) (writing took 3.9564392808824778 seconds)
[2022-01-17 12:15:02,059][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2022-01-17 12:15:02,072][train][INFO] - {"epoch": 26, "train_loss": "4.09", "train_ntokens": "1798.84", "train_nsentences": "4.97541", "train_prob_perplexity": "20.629", "train_code_perplexity": "20.618", "train_temp": "1.969", "train_loss_0": "3.92", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.40273", "train_wps": "6779.1", "train_ups": "3.77", "train_wpb": "1798.8", "train_bsz": "5", "train_num_updates": "3172", "train_lr": "4.95625e-05", "train_gnorm": "1.672", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "950"}
[2022-01-17 12:15:02,144][fairseq.trainer][INFO] - begin training epoch 27
[2022-01-17 12:15:02,145][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:15:08,820][train_inner][INFO] - {"epoch": 27, "update": 26.23, "loss": "4.109", "ntokens": "1800.81", "nsentences": "4.97", "prob_perplexity": "20.377", "code_perplexity": "20.366", "temp": "1.969", "loss_0": "3.939", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.40079", "wps": "6576.7", "ups": "3.65", "wpb": "1800.8", "bsz": "5", "num_updates": "3200", "lr": "5e-05", "gnorm": "1.659", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "957"}
[2022-01-17 12:15:29,979][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:15:30,442][valid][INFO] - {"epoch": 27, "valid_loss": "4.04", "valid_ntokens": "1701.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.719", "valid_code_perplexity": "19.714", "valid_temp": "1.967", "valid_loss_0": "3.87", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.40141", "valid_wps": "29131.8", "valid_wpb": "1701.5", "valid_bsz": "4.5", "valid_num_updates": "3294", "valid_best_loss": "3.995"}
[2022-01-17 12:15:30,444][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 3294 updates
[2022-01-17 12:15:30,445][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:15:34,474][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:15:34,497][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 27 @ 3294 updates, score 4.04) (writing took 4.0529195023700595 seconds)
[2022-01-17 12:15:34,497][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2022-01-17 12:15:34,511][train][INFO] - {"epoch": 27, "train_loss": "4.111", "train_ntokens": "1806.32", "train_nsentences": "4.97541", "train_prob_perplexity": "20.124", "train_code_perplexity": "20.116", "train_temp": "1.968", "train_loss_0": "3.941", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.40215", "train_wps": "6796.3", "train_ups": "3.76", "train_wpb": "1806.3", "train_bsz": "5", "train_num_updates": "3294", "train_lr": "5.14688e-05", "train_gnorm": "1.633", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "983"}
[2022-01-17 12:15:34,570][fairseq.trainer][INFO] - begin training epoch 28
[2022-01-17 12:15:34,570][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:15:59,064][train_inner][INFO] - {"epoch": 28, "update": 27.869, "loss": "4.114", "ntokens": "1812.11", "nsentences": "4.985", "prob_perplexity": "20.438", "code_perplexity": "20.43", "temp": "1.967", "loss_0": "3.944", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.39905", "wps": "7215", "ups": "3.98", "wpb": "1812.1", "bsz": "5", "num_updates": "3400", "lr": "5.3125e-05", "gnorm": "1.669", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "1007"}
[2022-01-17 12:16:02,508][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:16:02,968][valid][INFO] - {"epoch": 28, "valid_loss": "4.026", "valid_ntokens": "1616.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.739", "valid_code_perplexity": "20.719", "valid_temp": "1.966", "valid_loss_0": "3.856", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.41448", "valid_wps": "28306.8", "valid_wpb": "1616.5", "valid_bsz": "4.5", "valid_num_updates": "3416", "valid_best_loss": "3.995"}
[2022-01-17 12:16:02,970][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 3416 updates
[2022-01-17 12:16:02,970][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:16:07,063][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:16:07,076][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 28 @ 3416 updates, score 4.026) (writing took 4.106323461979628 seconds)
[2022-01-17 12:16:07,077][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2022-01-17 12:16:07,089][train][INFO] - {"epoch": 28, "train_loss": "4.111", "train_ntokens": "1806.05", "train_nsentences": "4.97541", "train_prob_perplexity": "20.825", "train_code_perplexity": "20.817", "train_temp": "1.967", "train_loss_0": "3.941", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.39695", "train_wps": "6765.9", "train_ups": "3.75", "train_wpb": "1806", "train_bsz": "5", "train_num_updates": "3416", "train_lr": "5.3375e-05", "train_gnorm": "1.707", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "1015"}
[2022-01-17 12:16:07,141][fairseq.trainer][INFO] - begin training epoch 29
[2022-01-17 12:16:07,142][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:16:34,943][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:16:35,407][valid][INFO] - {"epoch": 29, "valid_loss": "3.968", "valid_ntokens": "1654.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.508", "valid_code_perplexity": "20.474", "valid_temp": "1.965", "valid_loss_0": "3.797", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.43004", "valid_wps": "30198.8", "valid_wpb": "1654.5", "valid_bsz": "4.5", "valid_num_updates": "3538", "valid_best_loss": "3.968"}
[2022-01-17 12:16:35,409][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 3538 updates
[2022-01-17 12:16:35,410][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:16:39,389][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:16:47,729][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 29 @ 3538 updates, score 3.968) (writing took 12.320145682431757 seconds)
[2022-01-17 12:16:47,730][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2022-01-17 12:16:47,743][train][INFO] - {"epoch": 29, "train_loss": "4.075", "train_ntokens": "1800.55", "train_nsentences": "4.97541", "train_prob_perplexity": "20.328", "train_code_perplexity": "20.319", "train_temp": "1.966", "train_loss_0": "3.905", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.4078", "train_wps": "5405.1", "train_ups": "3", "train_wpb": "1800.5", "train_bsz": "5", "train_num_updates": "3538", "train_lr": "5.52812e-05", "train_gnorm": "1.751", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1056"}
[2022-01-17 12:16:47,789][fairseq.trainer][INFO] - begin training epoch 30
[2022-01-17 12:16:47,790][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:17:02,135][train_inner][INFO] - {"epoch": 30, "update": 29.508, "loss": "4.065", "ntokens": "1794.65", "nsentences": "4.97", "prob_perplexity": "20.302", "code_perplexity": "20.295", "temp": "1.965", "loss_0": "3.896", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.40798", "wps": "5692.1", "ups": "3.17", "wpb": "1794.7", "bsz": "5", "num_updates": "3600", "lr": "5.625e-05", "gnorm": "1.744", "clip": "0", "train_wall": "45", "gb_free": "14.6", "wall": "1070"}
[2022-01-17 12:17:15,520][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:17:16,039][valid][INFO] - {"epoch": 30, "valid_loss": "3.965", "valid_ntokens": "1630", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.133", "valid_code_perplexity": "18.124", "valid_temp": "1.964", "valid_loss_0": "3.795", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.43006", "valid_wps": "28954.8", "valid_wpb": "1630", "valid_bsz": "4.5", "valid_num_updates": "3660", "valid_best_loss": "3.965"}
[2022-01-17 12:17:16,041][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 3660 updates
[2022-01-17 12:17:16,042][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:17:20,125][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:17:28,904][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 30 @ 3660 updates, score 3.965) (writing took 12.862342250533402 seconds)
[2022-01-17 12:17:28,904][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2022-01-17 12:17:28,919][train][INFO] - {"epoch": 30, "train_loss": "4.058", "train_ntokens": "1794.41", "train_nsentences": "4.97541", "train_prob_perplexity": "20.07", "train_code_perplexity": "20.064", "train_temp": "1.964", "train_loss_0": "3.889", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.40879", "train_wps": "5318.6", "train_ups": "2.96", "train_wpb": "1794.4", "train_bsz": "5", "train_num_updates": "3660", "train_lr": "5.71875e-05", "train_gnorm": "1.742", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1097"}
[2022-01-17 12:17:29,010][fairseq.trainer][INFO] - begin training epoch 31
[2022-01-17 12:17:29,011][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:17:56,831][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:17:57,304][valid][INFO] - {"epoch": 31, "valid_loss": "4.018", "valid_ntokens": "1609.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.502", "valid_code_perplexity": "20.477", "valid_temp": "1.963", "valid_loss_0": "3.849", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.42218", "valid_wps": "27679.7", "valid_wpb": "1609.5", "valid_bsz": "4.5", "valid_num_updates": "3782", "valid_best_loss": "3.965"}
[2022-01-17 12:17:57,307][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 3782 updates
[2022-01-17 12:17:57,308][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:18:01,050][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:18:01,077][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 31 @ 3782 updates, score 4.018) (writing took 3.7698054388165474 seconds)
[2022-01-17 12:18:01,077][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2022-01-17 12:18:01,091][train][INFO] - {"epoch": 31, "train_loss": "4.034", "train_ntokens": "1804.09", "train_nsentences": "4.97541", "train_prob_perplexity": "20.606", "train_code_perplexity": "20.601", "train_temp": "1.963", "train_loss_0": "3.865", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41241", "train_wps": "6844.2", "train_ups": "3.79", "train_wpb": "1804.1", "train_bsz": "5", "train_num_updates": "3782", "train_lr": "5.90938e-05", "train_gnorm": "1.742", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1129"}
[2022-01-17 12:18:01,151][fairseq.trainer][INFO] - begin training epoch 32
[2022-01-17 12:18:01,151][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:18:05,455][train_inner][INFO] - {"epoch": 32, "update": 31.148, "loss": "4.048", "ntokens": "1794.61", "nsentences": "4.955", "prob_perplexity": "20.455", "code_perplexity": "20.449", "temp": "1.963", "loss_0": "3.879", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.41054", "wps": "5669.5", "ups": "3.16", "wpb": "1794.6", "bsz": "5", "num_updates": "3800", "lr": "5.9375e-05", "gnorm": "1.75", "clip": "0", "train_wall": "44", "gb_free": "14", "wall": "1134"}
[2022-01-17 12:18:29,028][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:18:29,507][valid][INFO] - {"epoch": 32, "valid_loss": "3.893", "valid_ntokens": "1604", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.446", "valid_code_perplexity": "19.453", "valid_temp": "1.961", "valid_loss_0": "3.723", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.42862", "valid_wps": "29897.1", "valid_wpb": "1604", "valid_bsz": "4.5", "valid_num_updates": "3904", "valid_best_loss": "3.893"}
[2022-01-17 12:18:29,508][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 3904 updates
[2022-01-17 12:18:29,509][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:18:33,390][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:18:42,045][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 32 @ 3904 updates, score 3.893) (writing took 12.53707221802324 seconds)
[2022-01-17 12:18:42,046][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2022-01-17 12:18:42,058][train][INFO] - {"epoch": 32, "train_loss": "4.073", "train_ntokens": "1800.78", "train_nsentences": "4.97541", "train_prob_perplexity": "20.308", "train_code_perplexity": "20.3", "train_temp": "1.962", "train_loss_0": "3.904", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.40222", "train_wps": "5364.4", "train_ups": "2.98", "train_wpb": "1800.8", "train_bsz": "5", "train_num_updates": "3904", "train_lr": "6.1e-05", "train_gnorm": "1.696", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "15.1", "train_wall": "1170"}
[2022-01-17 12:18:42,097][fairseq.trainer][INFO] - begin training epoch 33
[2022-01-17 12:18:42,097][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:19:04,187][train_inner][INFO] - {"epoch": 33, "update": 32.787, "loss": "4.066", "ntokens": "1810.56", "nsentences": "4.985", "prob_perplexity": "20.398", "code_perplexity": "20.39", "temp": "1.961", "loss_0": "3.897", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.40298", "wps": "6166.8", "ups": "3.41", "wpb": "1810.6", "bsz": "5", "num_updates": "4000", "lr": "6.25e-05", "gnorm": "1.68", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "1192"}
[2022-01-17 12:19:10,178][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:19:10,600][valid][INFO] - {"epoch": 33, "valid_loss": "4.023", "valid_ntokens": "1689.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.282", "valid_code_perplexity": "20.289", "valid_temp": "1.96", "valid_loss_0": "3.853", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.41728", "valid_wps": "28823.7", "valid_wpb": "1689.5", "valid_bsz": "4.5", "valid_num_updates": "4026", "valid_best_loss": "3.893"}
[2022-01-17 12:19:10,602][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 4026 updates
[2022-01-17 12:19:10,602][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:19:14,517][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:19:14,548][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 33 @ 4026 updates, score 4.023) (writing took 3.9457128336653113 seconds)
[2022-01-17 12:19:14,548][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2022-01-17 12:19:14,560][train][INFO] - {"epoch": 33, "train_loss": "4.041", "train_ntokens": "1810.75", "train_nsentences": "4.97541", "train_prob_perplexity": "20.534", "train_code_perplexity": "20.525", "train_temp": "1.961", "train_loss_0": "3.873", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.40837", "train_wps": "6799.3", "train_ups": "3.75", "train_wpb": "1810.7", "train_bsz": "5", "train_num_updates": "4026", "train_lr": "6.29063e-05", "train_gnorm": "1.675", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1203"}
[2022-01-17 12:19:14,619][fairseq.trainer][INFO] - begin training epoch 34
[2022-01-17 12:19:14,620][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:19:42,517][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:19:42,964][valid][INFO] - {"epoch": 34, "valid_loss": "4.064", "valid_ntokens": "1682", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.917", "valid_code_perplexity": "18.88", "valid_temp": "1.959", "valid_loss_0": "3.894", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.39685", "valid_wps": "30292.9", "valid_wpb": "1682", "valid_bsz": "4.5", "valid_num_updates": "4148", "valid_best_loss": "3.893"}
[2022-01-17 12:19:42,966][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 4148 updates
[2022-01-17 12:19:42,967][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:19:46,928][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:19:46,954][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 34 @ 4148 updates, score 4.064) (writing took 3.987903380766511 seconds)
[2022-01-17 12:19:46,955][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2022-01-17 12:19:46,968][train][INFO] - {"epoch": 34, "train_loss": "4.039", "train_ntokens": "1802.25", "train_nsentences": "4.97541", "train_prob_perplexity": "20.515", "train_code_perplexity": "20.507", "train_temp": "1.96", "train_loss_0": "3.871", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.40569", "train_wps": "6787.4", "train_ups": "3.77", "train_wpb": "1802.3", "train_bsz": "5", "train_num_updates": "4148", "train_lr": "6.48125e-05", "train_gnorm": "1.649", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1235"}
[2022-01-17 12:19:47,036][fairseq.trainer][INFO] - begin training epoch 35
[2022-01-17 12:19:47,037][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:19:59,026][train_inner][INFO] - {"epoch": 35, "update": 34.426, "loss": "4.026", "ntokens": "1805.67", "nsentences": "4.985", "prob_perplexity": "20.334", "code_perplexity": "20.327", "temp": "1.959", "loss_0": "3.858", "loss_1": "0.14", "loss_2": "0.028", "accuracy": "0.41037", "wps": "6587", "ups": "3.65", "wpb": "1805.7", "bsz": "5", "num_updates": "4200", "lr": "6.5625e-05", "gnorm": "1.645", "clip": "0", "train_wall": "45", "gb_free": "15.1", "wall": "1247"}
[2022-01-17 12:20:14,749][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:20:15,224][valid][INFO] - {"epoch": 35, "valid_loss": "4.059", "valid_ntokens": "1668.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.583", "valid_code_perplexity": "20.575", "valid_temp": "1.958", "valid_loss_0": "3.889", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.40725", "valid_wps": "30339.8", "valid_wpb": "1668.5", "valid_bsz": "4.5", "valid_num_updates": "4270", "valid_best_loss": "3.893"}
[2022-01-17 12:20:15,225][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 4270 updates
[2022-01-17 12:20:15,226][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:20:19,381][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:20:19,404][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 35 @ 4270 updates, score 4.059) (writing took 4.178222186863422 seconds)
[2022-01-17 12:20:19,404][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2022-01-17 12:20:19,417][train][INFO] - {"epoch": 35, "train_loss": "4.018", "train_ntokens": "1796.12", "train_nsentences": "4.97541", "train_prob_perplexity": "20.022", "train_code_perplexity": "20.015", "train_temp": "1.958", "train_loss_0": "3.85", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41367", "train_wps": "6755.6", "train_ups": "3.76", "train_wpb": "1796.1", "train_bsz": "5", "train_num_updates": "4270", "train_lr": "6.67187e-05", "train_gnorm": "1.62", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "1268"}
[2022-01-17 12:20:19,489][fairseq.trainer][INFO] - begin training epoch 36
[2022-01-17 12:20:19,490][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:20:47,311][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:20:47,812][valid][INFO] - {"epoch": 36, "valid_loss": "3.981", "valid_ntokens": "1689", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.479", "valid_code_perplexity": "19.471", "valid_temp": "1.957", "valid_loss_0": "3.81", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.4251", "valid_wps": "30351", "valid_wpb": "1689", "valid_bsz": "4.5", "valid_num_updates": "4392", "valid_best_loss": "3.893"}
[2022-01-17 12:20:47,814][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 4392 updates
[2022-01-17 12:20:47,814][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:20:51,773][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:20:51,797][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 36 @ 4392 updates, score 3.981) (writing took 3.9833089746534824 seconds)
[2022-01-17 12:20:51,798][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2022-01-17 12:20:51,812][train][INFO] - {"epoch": 36, "train_loss": "4.009", "train_ntokens": "1808.33", "train_nsentences": "4.97541", "train_prob_perplexity": "20.127", "train_code_perplexity": "20.121", "train_temp": "1.957", "train_loss_0": "3.84", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41473", "train_wps": "6813.1", "train_ups": "3.77", "train_wpb": "1808.3", "train_bsz": "5", "train_num_updates": "4392", "train_lr": "6.8625e-05", "train_gnorm": "1.626", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1300"}
[2022-01-17 12:20:51,866][fairseq.trainer][INFO] - begin training epoch 37
[2022-01-17 12:20:51,867][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:20:53,998][train_inner][INFO] - {"epoch": 37, "update": 36.066, "loss": "4.013", "ntokens": "1799.34", "nsentences": "4.97", "prob_perplexity": "20.149", "code_perplexity": "20.142", "temp": "1.957", "loss_0": "3.843", "loss_1": "0.14", "loss_2": "0.03", "accuracy": "0.41392", "wps": "6547.9", "ups": "3.64", "wpb": "1799.3", "bsz": "5", "num_updates": "4400", "lr": "6.875e-05", "gnorm": "1.624", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1302"}
[2022-01-17 12:21:19,634][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:21:20,079][valid][INFO] - {"epoch": 37, "valid_loss": "4.004", "valid_ntokens": "1713.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.384", "valid_code_perplexity": "18.375", "valid_temp": "1.955", "valid_loss_0": "3.834", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.42924", "valid_wps": "28556.1", "valid_wpb": "1713.5", "valid_bsz": "4.5", "valid_num_updates": "4514", "valid_best_loss": "3.893"}
[2022-01-17 12:21:20,081][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 4514 updates
[2022-01-17 12:21:20,081][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:21:24,126][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:21:24,150][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 37 @ 4514 updates, score 4.004) (writing took 4.068790791556239 seconds)
[2022-01-17 12:21:24,150][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2022-01-17 12:21:24,162][train][INFO] - {"epoch": 37, "train_loss": "4.015", "train_ntokens": "1809.52", "train_nsentences": "4.97541", "train_prob_perplexity": "20.549", "train_code_perplexity": "20.543", "train_temp": "1.956", "train_loss_0": "3.846", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41144", "train_wps": "6826.6", "train_ups": "3.77", "train_wpb": "1809.5", "train_bsz": "5", "train_num_updates": "4514", "train_lr": "7.05313e-05", "train_gnorm": "1.561", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.5", "train_wall": "1332"}
[2022-01-17 12:21:24,224][fairseq.trainer][INFO] - begin training epoch 38
[2022-01-17 12:21:24,225][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:21:43,941][train_inner][INFO] - {"epoch": 38, "update": 37.705, "loss": "4.01", "ntokens": "1809.44", "nsentences": "4.97", "prob_perplexity": "20.504", "code_perplexity": "20.498", "temp": "1.956", "loss_0": "3.842", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.41284", "wps": "7247.9", "ups": "4.01", "wpb": "1809.4", "bsz": "5", "num_updates": "4600", "lr": "7.1875e-05", "gnorm": "1.566", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1352"}
[2022-01-17 12:21:52,047][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:21:52,502][valid][INFO] - {"epoch": 38, "valid_loss": "3.849", "valid_ntokens": "1580", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.858", "valid_code_perplexity": "20.889", "valid_temp": "1.954", "valid_loss_0": "3.682", "valid_loss_1": "0.14", "valid_loss_2": "0.028", "valid_accuracy": "0.43734", "valid_wps": "26441.4", "valid_wpb": "1580", "valid_bsz": "4.5", "valid_num_updates": "4636", "valid_best_loss": "3.849"}
[2022-01-17 12:21:52,504][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 4636 updates
[2022-01-17 12:21:52,504][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:21:56,478][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:22:04,697][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 38 @ 4636 updates, score 3.849) (writing took 12.193374297581613 seconds)
[2022-01-17 12:22:04,698][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2022-01-17 12:22:04,710][train][INFO] - {"epoch": 38, "train_loss": "3.996", "train_ntokens": "1808.51", "train_nsentences": "4.97541", "train_prob_perplexity": "20.391", "train_code_perplexity": "20.383", "train_temp": "1.955", "train_loss_0": "3.828", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.41593", "train_wps": "5443.1", "train_ups": "3.01", "train_wpb": "1808.5", "train_bsz": "5", "train_num_updates": "4636", "train_lr": "7.24375e-05", "train_gnorm": "1.566", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1373"}
[2022-01-17 12:22:04,771][fairseq.trainer][INFO] - begin training epoch 39
[2022-01-17 12:22:04,772][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:22:32,988][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:22:33,429][valid][INFO] - {"epoch": 39, "valid_loss": "4.016", "valid_ntokens": "1700.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.028", "valid_code_perplexity": "21.992", "valid_temp": "1.953", "valid_loss_0": "3.846", "valid_loss_1": "0.139", "valid_loss_2": "0.031", "valid_accuracy": "0.40018", "valid_wps": "29101.8", "valid_wpb": "1700.5", "valid_bsz": "4.5", "valid_num_updates": "4758", "valid_best_loss": "3.849"}
[2022-01-17 12:22:33,430][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 4758 updates
[2022-01-17 12:22:33,431][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:22:37,429][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:22:37,454][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 39 @ 4758 updates, score 4.016) (writing took 4.023418345488608 seconds)
[2022-01-17 12:22:37,455][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2022-01-17 12:22:37,468][train][INFO] - {"epoch": 39, "train_loss": "3.997", "train_ntokens": "1803.24", "train_nsentences": "4.97541", "train_prob_perplexity": "20.69", "train_code_perplexity": "20.683", "train_temp": "1.954", "train_loss_0": "3.827", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41338", "train_wps": "6718.4", "train_ups": "3.73", "train_wpb": "1803.2", "train_bsz": "5", "train_num_updates": "4758", "train_lr": "7.43438e-05", "train_gnorm": "1.549", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "1406"}
[2022-01-17 12:22:37,527][fairseq.trainer][INFO] - begin training epoch 40
[2022-01-17 12:22:37,527][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:22:47,321][train_inner][INFO] - {"epoch": 40, "update": 39.344, "loss": "3.997", "ntokens": "1800.95", "nsentences": "4.97", "prob_perplexity": "20.678", "code_perplexity": "20.671", "temp": "1.954", "loss_0": "3.828", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.41398", "wps": "5684.2", "ups": "3.16", "wpb": "1801", "bsz": "5", "num_updates": "4800", "lr": "7.5e-05", "gnorm": "1.543", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "1416"}
[2022-01-17 12:23:05,336][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:23:05,775][valid][INFO] - {"epoch": 40, "valid_loss": "4.092", "valid_ntokens": "1636", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.859", "valid_code_perplexity": "20.85", "valid_temp": "1.952", "valid_loss_0": "3.923", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.40251", "valid_wps": "28268.4", "valid_wpb": "1636", "valid_bsz": "4.5", "valid_num_updates": "4880", "valid_best_loss": "3.849"}
[2022-01-17 12:23:05,777][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 4880 updates
[2022-01-17 12:23:05,777][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:23:09,707][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:23:09,725][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 40 @ 4880 updates, score 4.092) (writing took 3.948358886875212 seconds)
[2022-01-17 12:23:09,726][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2022-01-17 12:23:09,739][train][INFO] - {"epoch": 40, "train_loss": "3.989", "train_ntokens": "1798.77", "train_nsentences": "4.97541", "train_prob_perplexity": "20.79", "train_code_perplexity": "20.784", "train_temp": "1.952", "train_loss_0": "3.819", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41265", "train_wps": "6803", "train_ups": "3.78", "train_wpb": "1798.8", "train_bsz": "5", "train_num_updates": "4880", "train_lr": "7.625e-05", "train_gnorm": "1.512", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1438"}
[2022-01-17 12:23:09,783][fairseq.trainer][INFO] - begin training epoch 41
[2022-01-17 12:23:09,783][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:23:37,532][train_inner][INFO] - {"epoch": 41, "update": 40.984, "loss": "3.984", "ntokens": "1801.71", "nsentences": "4.985", "prob_perplexity": "20.907", "code_perplexity": "20.902", "temp": "1.952", "loss_0": "3.815", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.41255", "wps": "7178.9", "ups": "3.98", "wpb": "1801.7", "bsz": "5", "num_updates": "5000", "lr": "7.8125e-05", "gnorm": "1.489", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1466"}
[2022-01-17 12:23:37,994][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:23:38,446][valid][INFO] - {"epoch": 41, "valid_loss": "3.821", "valid_ntokens": "1554.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.409", "valid_code_perplexity": "21.402", "valid_temp": "1.951", "valid_loss_0": "3.652", "valid_loss_1": "0.139", "valid_loss_2": "0.029", "valid_accuracy": "0.44484", "valid_wps": "28085", "valid_wpb": "1554.5", "valid_bsz": "4.5", "valid_num_updates": "5002", "valid_best_loss": "3.821"}
[2022-01-17 12:23:38,447][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 5002 updates
[2022-01-17 12:23:38,448][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:23:42,467][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:23:51,578][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 41 @ 5002 updates, score 3.821) (writing took 13.130659871734679 seconds)
[2022-01-17 12:23:51,579][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2022-01-17 12:23:51,594][train][INFO] - {"epoch": 41, "train_loss": "3.988", "train_ntokens": "1799.29", "train_nsentences": "4.97541", "train_prob_perplexity": "20.998", "train_code_perplexity": "20.995", "train_temp": "1.951", "train_loss_0": "3.82", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41221", "train_wps": "5246.4", "train_ups": "2.92", "train_wpb": "1799.3", "train_bsz": "5", "train_num_updates": "5002", "train_lr": "7.81562e-05", "train_gnorm": "1.477", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1480"}
[2022-01-17 12:23:51,682][fairseq.trainer][INFO] - begin training epoch 42
[2022-01-17 12:23:51,683][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:24:19,584][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:24:20,016][valid][INFO] - {"epoch": 42, "valid_loss": "3.969", "valid_ntokens": "1651.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.002", "valid_code_perplexity": "20.988", "valid_temp": "1.949", "valid_loss_0": "3.8", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.40327", "valid_wps": "26901.2", "valid_wpb": "1651.5", "valid_bsz": "4.5", "valid_num_updates": "5124", "valid_best_loss": "3.821"}
[2022-01-17 12:24:20,017][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 5124 updates
[2022-01-17 12:24:20,018][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:24:23,936][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:24:23,964][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 42 @ 5124 updates, score 3.969) (writing took 3.9469476817175746 seconds)
[2022-01-17 12:24:23,965][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2022-01-17 12:24:23,978][train][INFO] - {"epoch": 42, "train_loss": "3.974", "train_ntokens": "1804.75", "train_nsentences": "4.97541", "train_prob_perplexity": "20.669", "train_code_perplexity": "20.664", "train_temp": "1.95", "train_loss_0": "3.806", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.41696", "train_wps": "6801.8", "train_ups": "3.77", "train_wpb": "1804.7", "train_bsz": "5", "train_num_updates": "5124", "train_lr": "8.00625e-05", "train_gnorm": "1.498", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1512"}
[2022-01-17 12:24:24,038][fairseq.trainer][INFO] - begin training epoch 43
[2022-01-17 12:24:24,038][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:24:41,612][train_inner][INFO] - {"epoch": 43, "update": 42.623, "loss": "3.98", "ntokens": "1801.63", "nsentences": "4.97", "prob_perplexity": "20.692", "code_perplexity": "20.687", "temp": "1.95", "loss_0": "3.812", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.41466", "wps": "5624.3", "ups": "3.12", "wpb": "1801.6", "bsz": "5", "num_updates": "5200", "lr": "8.125e-05", "gnorm": "1.477", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "1530"}
[2022-01-17 12:24:52,149][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:24:52,596][valid][INFO] - {"epoch": 43, "valid_loss": "4.044", "valid_ntokens": "1668.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.992", "valid_code_perplexity": "19.985", "valid_temp": "1.948", "valid_loss_0": "3.876", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.40965", "valid_wps": "29194.7", "valid_wpb": "1668.5", "valid_bsz": "4.5", "valid_num_updates": "5246", "valid_best_loss": "3.821"}
[2022-01-17 12:24:52,598][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 5246 updates
[2022-01-17 12:24:52,599][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:24:56,523][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:24:56,547][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 43 @ 5246 updates, score 4.044) (writing took 3.9490923546254635 seconds)
[2022-01-17 12:24:56,548][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2022-01-17 12:24:56,561][train][INFO] - {"epoch": 43, "train_loss": "3.98", "train_ntokens": "1796.16", "train_nsentences": "4.97541", "train_prob_perplexity": "20.737", "train_code_perplexity": "20.732", "train_temp": "1.949", "train_loss_0": "3.812", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41372", "train_wps": "6728", "train_ups": "3.75", "train_wpb": "1796.2", "train_bsz": "5", "train_num_updates": "5246", "train_lr": "8.19688e-05", "train_gnorm": "1.451", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1545"}
[2022-01-17 12:24:56,614][fairseq.trainer][INFO] - begin training epoch 44
[2022-01-17 12:24:56,615][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:25:24,534][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:25:24,964][valid][INFO] - {"epoch": 44, "valid_loss": "3.999", "valid_ntokens": "1683", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.78", "valid_code_perplexity": "20.773", "valid_temp": "1.947", "valid_loss_0": "3.831", "valid_loss_1": "0.14", "valid_loss_2": "0.028", "valid_accuracy": "0.42602", "valid_wps": "30195.3", "valid_wpb": "1683", "valid_bsz": "4.5", "valid_num_updates": "5368", "valid_best_loss": "3.821"}
[2022-01-17 12:25:24,966][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 5368 updates
[2022-01-17 12:25:24,966][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:25:28,846][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:25:28,864][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 44 @ 5368 updates, score 3.999) (writing took 3.8975546434521675 seconds)
[2022-01-17 12:25:28,864][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2022-01-17 12:25:28,877][train][INFO] - {"epoch": 44, "train_loss": "3.971", "train_ntokens": "1804.01", "train_nsentences": "4.97541", "train_prob_perplexity": "20.575", "train_code_perplexity": "20.57", "train_temp": "1.948", "train_loss_0": "3.802", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.41562", "train_wps": "6813.3", "train_ups": "3.78", "train_wpb": "1804", "train_bsz": "5", "train_num_updates": "5368", "train_lr": "8.3875e-05", "train_gnorm": "1.434", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "15.1", "train_wall": "1577"}
[2022-01-17 12:25:28,924][fairseq.trainer][INFO] - begin training epoch 45
[2022-01-17 12:25:28,925][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:25:36,523][train_inner][INFO] - {"epoch": 45, "update": 44.262, "loss": "3.965", "ntokens": "1806.07", "nsentences": "4.985", "prob_perplexity": "20.501", "code_perplexity": "20.496", "temp": "1.948", "loss_0": "3.796", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.41714", "wps": "6579.9", "ups": "3.64", "wpb": "1806.1", "bsz": "5", "num_updates": "5400", "lr": "8.4375e-05", "gnorm": "1.447", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "1585"}
[2022-01-17 12:25:57,016][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:25:57,504][valid][INFO] - {"epoch": 45, "valid_loss": "3.85", "valid_ntokens": "1584.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.315", "valid_code_perplexity": "19.318", "valid_temp": "1.946", "valid_loss_0": "3.683", "valid_loss_1": "0.14", "valid_loss_2": "0.027", "valid_accuracy": "0.44146", "valid_wps": "26376.9", "valid_wpb": "1584.5", "valid_bsz": "4.5", "valid_num_updates": "5490", "valid_best_loss": "3.821"}
[2022-01-17 12:25:57,506][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 5490 updates
[2022-01-17 12:25:57,507][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:26:01,512][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:26:01,535][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 45 @ 5490 updates, score 3.85) (writing took 4.029157263226807 seconds)
[2022-01-17 12:26:01,536][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2022-01-17 12:26:01,550][train][INFO] - {"epoch": 45, "train_loss": "3.967", "train_ntokens": "1807.76", "train_nsentences": "4.97541", "train_prob_perplexity": "20.163", "train_code_perplexity": "20.158", "train_temp": "1.946", "train_loss_0": "3.798", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41639", "train_wps": "6753", "train_ups": "3.74", "train_wpb": "1807.8", "train_bsz": "5", "train_num_updates": "5490", "train_lr": "8.57813e-05", "train_gnorm": "1.44", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "1610"}
[2022-01-17 12:26:01,604][fairseq.trainer][INFO] - begin training epoch 46
[2022-01-17 12:26:01,605][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:26:27,188][train_inner][INFO] - {"epoch": 46, "update": 45.902, "loss": "3.972", "ntokens": "1809.21", "nsentences": "4.985", "prob_perplexity": "20.388", "code_perplexity": "20.382", "temp": "1.946", "loss_0": "3.805", "loss_1": "0.14", "loss_2": "0.028", "accuracy": "0.4132", "wps": "7143.5", "ups": "3.95", "wpb": "1809.2", "bsz": "5", "num_updates": "5600", "lr": "8.75e-05", "gnorm": "1.412", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1635"}
[2022-01-17 12:26:29,810][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:26:30,256][valid][INFO] - {"epoch": 46, "valid_loss": "3.88", "valid_ntokens": "1640", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.27", "valid_code_perplexity": "20.281", "valid_temp": "1.945", "valid_loss_0": "3.711", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.43872", "valid_wps": "27308.4", "valid_wpb": "1640", "valid_bsz": "4.5", "valid_num_updates": "5612", "valid_best_loss": "3.821"}
[2022-01-17 12:26:30,258][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 5612 updates
[2022-01-17 12:26:30,258][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:26:34,329][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:26:34,364][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 46 @ 5612 updates, score 3.88) (writing took 4.105977972969413 seconds)
[2022-01-17 12:26:34,365][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2022-01-17 12:26:34,377][train][INFO] - {"epoch": 46, "train_loss": "3.964", "train_ntokens": "1806.31", "train_nsentences": "4.97541", "train_prob_perplexity": "20.52", "train_code_perplexity": "20.512", "train_temp": "1.945", "train_loss_0": "3.797", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.41343", "train_wps": "6715.6", "train_ups": "3.72", "train_wpb": "1806.3", "train_bsz": "5", "train_num_updates": "5612", "train_lr": "8.76875e-05", "train_gnorm": "1.413", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1643"}
[2022-01-17 12:26:34,481][fairseq.trainer][INFO] - begin training epoch 47
[2022-01-17 12:26:34,482][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:27:02,770][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:27:03,227][valid][INFO] - {"epoch": 47, "valid_loss": "3.883", "valid_ntokens": "1645", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.126", "valid_code_perplexity": "19.116", "valid_temp": "1.943", "valid_loss_0": "3.713", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.42553", "valid_wps": "28313.2", "valid_wpb": "1645", "valid_bsz": "4.5", "valid_num_updates": "5734", "valid_best_loss": "3.821"}
[2022-01-17 12:27:03,229][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 5734 updates
[2022-01-17 12:27:03,230][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:27:07,163][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:27:07,186][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 47 @ 5734 updates, score 3.883) (writing took 3.956857423298061 seconds)
[2022-01-17 12:27:07,187][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2022-01-17 12:27:07,198][train][INFO] - {"epoch": 47, "train_loss": "3.953", "train_ntokens": "1803.79", "train_nsentences": "4.97541", "train_prob_perplexity": "20.736", "train_code_perplexity": "20.731", "train_temp": "1.944", "train_loss_0": "3.785", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.41769", "train_wps": "6707.2", "train_ups": "3.72", "train_wpb": "1803.8", "train_bsz": "5", "train_num_updates": "5734", "train_lr": "8.95938e-05", "train_gnorm": "1.397", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1675"}
[2022-01-17 12:27:07,246][fairseq.trainer][INFO] - begin training epoch 48
[2022-01-17 12:27:07,247][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:27:22,511][train_inner][INFO] - {"epoch": 48, "update": 47.541, "loss": "3.953", "ntokens": "1797.14", "nsentences": "4.955", "prob_perplexity": "20.627", "code_perplexity": "20.622", "temp": "1.944", "loss_0": "3.785", "loss_1": "0.14", "loss_2": "0.028", "accuracy": "0.41693", "wps": "6498.4", "ups": "3.62", "wpb": "1797.1", "bsz": "5", "num_updates": "5800", "lr": "9.0625e-05", "gnorm": "1.403", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1691"}
[2022-01-17 12:27:35,227][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:27:35,664][valid][INFO] - {"epoch": 48, "valid_loss": "4.017", "valid_ntokens": "1678.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.16", "valid_code_perplexity": "20.155", "valid_temp": "1.942", "valid_loss_0": "3.848", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.41257", "valid_wps": "29442.1", "valid_wpb": "1678.5", "valid_bsz": "4.5", "valid_num_updates": "5856", "valid_best_loss": "3.821"}
[2022-01-17 12:27:35,667][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 5856 updates
[2022-01-17 12:27:35,667][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:27:39,594][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:27:39,622][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 48 @ 5856 updates, score 4.017) (writing took 3.95515045709908 seconds)
[2022-01-17 12:27:39,622][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2022-01-17 12:27:39,634][train][INFO] - {"epoch": 48, "train_loss": "3.946", "train_ntokens": "1800.34", "train_nsentences": "4.97541", "train_prob_perplexity": "20.409", "train_code_perplexity": "20.404", "train_temp": "1.943", "train_loss_0": "3.779", "train_loss_1": "0.14", "train_loss_2": "0.028", "train_accuracy": "0.41716", "train_wps": "6774.1", "train_ups": "3.76", "train_wpb": "1800.3", "train_bsz": "5", "train_num_updates": "5856", "train_lr": "9.15e-05", "train_gnorm": "1.388", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "1708"}
[2022-01-17 12:27:39,687][fairseq.trainer][INFO] - begin training epoch 49
[2022-01-17 12:27:39,688][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:28:07,770][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:28:08,209][valid][INFO] - {"epoch": 49, "valid_loss": "3.931", "valid_ntokens": "1644.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.896", "valid_code_perplexity": "20.882", "valid_temp": "1.941", "valid_loss_0": "3.762", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.40894", "valid_wps": "28350.9", "valid_wpb": "1644.5", "valid_bsz": "4.5", "valid_num_updates": "5978", "valid_best_loss": "3.821"}
[2022-01-17 12:28:08,211][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 5978 updates
[2022-01-17 12:28:08,212][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:28:12,114][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:28:12,137][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 49 @ 5978 updates, score 3.931) (writing took 3.926043680869043 seconds)
[2022-01-17 12:28:12,138][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2022-01-17 12:28:12,150][train][INFO] - {"epoch": 49, "train_loss": "3.934", "train_ntokens": "1788.85", "train_nsentences": "4.97541", "train_prob_perplexity": "20.807", "train_code_perplexity": "20.803", "train_temp": "1.942", "train_loss_0": "3.765", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41974", "train_wps": "6714.4", "train_ups": "3.75", "train_wpb": "1788.9", "train_bsz": "5", "train_num_updates": "5978", "train_lr": "9.34062e-05", "train_gnorm": "1.364", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1740"}
[2022-01-17 12:28:12,202][fairseq.trainer][INFO] - begin training epoch 50
[2022-01-17 12:28:12,203][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:28:17,591][train_inner][INFO] - {"epoch": 50, "update": 49.18, "loss": "3.928", "ntokens": "1794.38", "nsentences": "4.985", "prob_perplexity": "20.709", "code_perplexity": "20.704", "temp": "1.942", "loss_0": "3.76", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.42082", "wps": "6517", "ups": "3.63", "wpb": "1794.4", "bsz": "5", "num_updates": "6000", "lr": "9.375e-05", "gnorm": "1.368", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1746"}
[2022-01-17 12:28:40,347][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:28:40,810][valid][INFO] - {"epoch": 50, "valid_loss": "3.914", "valid_ntokens": "1651.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.662", "valid_code_perplexity": "21.636", "valid_temp": "1.94", "valid_loss_0": "3.744", "valid_loss_1": "0.139", "valid_loss_2": "0.031", "valid_accuracy": "0.41326", "valid_wps": "29164.6", "valid_wpb": "1651.5", "valid_bsz": "4.5", "valid_num_updates": "6100", "valid_best_loss": "3.821"}
[2022-01-17 12:28:40,813][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 6100 updates
[2022-01-17 12:28:40,813][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:28:44,737][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:28:44,767][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 50 @ 6100 updates, score 3.914) (writing took 3.9540752498432994 seconds)
[2022-01-17 12:28:44,767][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2022-01-17 12:28:44,780][train][INFO] - {"epoch": 50, "train_loss": "3.945", "train_ntokens": "1796.07", "train_nsentences": "4.97541", "train_prob_perplexity": "21.084", "train_code_perplexity": "21.078", "train_temp": "1.941", "train_loss_0": "3.776", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41475", "train_wps": "6717.9", "train_ups": "3.74", "train_wpb": "1796.1", "train_bsz": "5", "train_num_updates": "6100", "train_lr": "9.53125e-05", "train_gnorm": "1.33", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "1773"}
[2022-01-17 12:28:44,843][fairseq.trainer][INFO] - begin training epoch 51
[2022-01-17 12:28:44,844][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:29:08,005][train_inner][INFO] - {"epoch": 51, "update": 50.82, "loss": "3.949", "ntokens": "1793.2", "nsentences": "4.97", "prob_perplexity": "21.066", "code_perplexity": "21.062", "temp": "1.94", "loss_0": "3.781", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.41161", "wps": "7115.9", "ups": "3.97", "wpb": "1793.2", "bsz": "5", "num_updates": "6200", "lr": "9.6875e-05", "gnorm": "1.326", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1796"}
[2022-01-17 12:29:13,008][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:29:13,458][valid][INFO] - {"epoch": 51, "valid_loss": "3.839", "valid_ntokens": "1667", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.447", "valid_code_perplexity": "22.451", "valid_temp": "1.939", "valid_loss_0": "3.669", "valid_loss_1": "0.139", "valid_loss_2": "0.031", "valid_accuracy": "0.41902", "valid_wps": "28902.9", "valid_wpb": "1667", "valid_bsz": "4.5", "valid_num_updates": "6222", "valid_best_loss": "3.821"}
[2022-01-17 12:29:13,460][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 6222 updates
[2022-01-17 12:29:13,461][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:29:17,434][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:29:17,460][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 51 @ 6222 updates, score 3.839) (writing took 3.999471983872354 seconds)
[2022-01-17 12:29:17,460][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2022-01-17 12:29:17,472][train][INFO] - {"epoch": 51, "train_loss": "3.945", "train_ntokens": "1792.16", "train_nsentences": "4.97541", "train_prob_perplexity": "21.12", "train_code_perplexity": "21.118", "train_temp": "1.939", "train_loss_0": "3.777", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.41088", "train_wps": "6690.5", "train_ups": "3.73", "train_wpb": "1792.2", "train_bsz": "5", "train_num_updates": "6222", "train_lr": "9.72188e-05", "train_gnorm": "1.316", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1806"}
[2022-01-17 12:29:17,528][fairseq.trainer][INFO] - begin training epoch 52
[2022-01-17 12:29:17,529][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:29:45,618][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:29:46,075][valid][INFO] - {"epoch": 52, "valid_loss": "3.94", "valid_ntokens": "1608.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.299", "valid_code_perplexity": "20.291", "valid_temp": "1.938", "valid_loss_0": "3.772", "valid_loss_1": "0.14", "valid_loss_2": "0.028", "valid_accuracy": "0.40286", "valid_wps": "28539.7", "valid_wpb": "1608.5", "valid_bsz": "4.5", "valid_num_updates": "6344", "valid_best_loss": "3.821"}
[2022-01-17 12:29:46,077][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 6344 updates
[2022-01-17 12:29:46,077][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:29:49,987][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:29:50,011][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 52 @ 6344 updates, score 3.94) (writing took 3.934797789901495 seconds)
[2022-01-17 12:29:50,012][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2022-01-17 12:29:50,024][train][INFO] - {"epoch": 52, "train_loss": "3.953", "train_ntokens": "1801.72", "train_nsentences": "4.97541", "train_prob_perplexity": "21.251", "train_code_perplexity": "21.247", "train_temp": "1.938", "train_loss_0": "3.785", "train_loss_1": "0.139", "train_loss_2": "0.029", "train_accuracy": "0.40824", "train_wps": "6755.2", "train_ups": "3.75", "train_wpb": "1801.7", "train_bsz": "5", "train_num_updates": "6344", "train_lr": "9.9125e-05", "train_gnorm": "1.31", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "1838"}
[2022-01-17 12:29:50,072][fairseq.trainer][INFO] - begin training epoch 53
[2022-01-17 12:29:50,073][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:30:02,977][train_inner][INFO] - {"epoch": 53, "update": 52.459, "loss": "3.951", "ntokens": "1795.47", "nsentences": "4.97", "prob_perplexity": "21.359", "code_perplexity": "21.355", "temp": "1.938", "loss_0": "3.782", "loss_1": "0.139", "loss_2": "0.029", "accuracy": "0.40804", "wps": "6533.9", "ups": "3.64", "wpb": "1795.5", "bsz": "5", "num_updates": "6400", "lr": "0.0001", "gnorm": "1.308", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1851"}
[2022-01-17 12:30:18,097][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:30:18,542][valid][INFO] - {"epoch": 53, "valid_loss": "4.031", "valid_ntokens": "1680.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.144", "valid_code_perplexity": "21.132", "valid_temp": "1.936", "valid_loss_0": "3.863", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.40077", "valid_wps": "29695.4", "valid_wpb": "1680.5", "valid_bsz": "4.5", "valid_num_updates": "6466", "valid_best_loss": "3.821"}
[2022-01-17 12:30:18,543][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 6466 updates
[2022-01-17 12:30:18,544][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:30:22,462][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:30:22,482][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 53 @ 6466 updates, score 4.031) (writing took 3.938474638387561 seconds)
[2022-01-17 12:30:22,483][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2022-01-17 12:30:22,495][train][INFO] - {"epoch": 53, "train_loss": "3.95", "train_ntokens": "1806.84", "train_nsentences": "4.97541", "train_prob_perplexity": "21.551", "train_code_perplexity": "21.548", "train_temp": "1.937", "train_loss_0": "3.783", "train_loss_1": "0.139", "train_loss_2": "0.028", "train_accuracy": "0.40621", "train_wps": "6791.3", "train_ups": "3.76", "train_wpb": "1806.8", "train_bsz": "5", "train_num_updates": "6466", "train_lr": "0.000101031", "train_gnorm": "1.287", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "1871"}
[2022-01-17 12:30:22,540][fairseq.trainer][INFO] - begin training epoch 54
[2022-01-17 12:30:22,541][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:30:50,534][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:30:50,985][valid][INFO] - {"epoch": 54, "valid_loss": "3.857", "valid_ntokens": "1579.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.486", "valid_code_perplexity": "20.493", "valid_temp": "1.935", "valid_loss_0": "3.687", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.42672", "valid_wps": "25863.4", "valid_wpb": "1579.5", "valid_bsz": "4.5", "valid_num_updates": "6588", "valid_best_loss": "3.821"}
[2022-01-17 12:30:50,987][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 6588 updates
[2022-01-17 12:30:50,988][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:30:54,897][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:30:54,923][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 54 @ 6588 updates, score 3.857) (writing took 3.935932967811823 seconds)
[2022-01-17 12:30:54,924][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2022-01-17 12:30:54,937][train][INFO] - {"epoch": 54, "train_loss": "3.932", "train_ntokens": "1802.9", "train_nsentences": "4.97541", "train_prob_perplexity": "21.495", "train_code_perplexity": "21.493", "train_temp": "1.936", "train_loss_0": "3.763", "train_loss_1": "0.139", "train_loss_2": "0.029", "train_accuracy": "0.41053", "train_wps": "6782.7", "train_ups": "3.76", "train_wpb": "1802.9", "train_bsz": "5", "train_num_updates": "6588", "train_lr": "0.000102938", "train_gnorm": "1.296", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "1903"}
[2022-01-17 12:30:54,994][fairseq.trainer][INFO] - begin training epoch 55
[2022-01-17 12:30:54,995][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:30:58,084][train_inner][INFO] - {"epoch": 55, "update": 54.098, "loss": "3.937", "ntokens": "1811.69", "nsentences": "4.985", "prob_perplexity": "21.518", "code_perplexity": "21.514", "temp": "1.936", "loss_0": "3.769", "loss_1": "0.139", "loss_2": "0.029", "accuracy": "0.40925", "wps": "6576.7", "ups": "3.63", "wpb": "1811.7", "bsz": "5", "num_updates": "6600", "lr": "0.000103125", "gnorm": "1.285", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1906"}
[2022-01-17 12:31:23,033][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:31:23,494][valid][INFO] - {"epoch": 55, "valid_loss": "4.066", "valid_ntokens": "1648", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.293", "valid_code_perplexity": "22.282", "valid_temp": "1.934", "valid_loss_0": "3.896", "valid_loss_1": "0.139", "valid_loss_2": "0.03", "valid_accuracy": "0.39108", "valid_wps": "30030.4", "valid_wpb": "1648", "valid_bsz": "4.5", "valid_num_updates": "6710", "valid_best_loss": "3.821"}
[2022-01-17 12:31:23,496][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 6710 updates
[2022-01-17 12:31:23,497][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:31:27,455][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:31:27,482][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 55 @ 6710 updates, score 4.066) (writing took 3.9861389277502894 seconds)
[2022-01-17 12:31:27,483][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2022-01-17 12:31:27,495][train][INFO] - {"epoch": 55, "train_loss": "3.941", "train_ntokens": "1812.07", "train_nsentences": "4.97541", "train_prob_perplexity": "21.263", "train_code_perplexity": "21.26", "train_temp": "1.935", "train_loss_0": "3.772", "train_loss_1": "0.139", "train_loss_2": "0.029", "train_accuracy": "0.40888", "train_wps": "6792.8", "train_ups": "3.75", "train_wpb": "1812.1", "train_bsz": "5", "train_num_updates": "6710", "train_lr": "0.000104844", "train_gnorm": "1.259", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.5", "train_wall": "1936"}
[2022-01-17 12:31:27,542][fairseq.trainer][INFO] - begin training epoch 56
[2022-01-17 12:31:27,543][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:31:48,301][train_inner][INFO] - {"epoch": 56, "update": 55.738, "loss": "3.947", "ntokens": "1812.46", "nsentences": "4.985", "prob_perplexity": "21.322", "code_perplexity": "21.319", "temp": "1.934", "loss_0": "3.778", "loss_1": "0.139", "loss_2": "0.03", "accuracy": "0.40731", "wps": "7220.4", "ups": "3.98", "wpb": "1812.5", "bsz": "5", "num_updates": "6800", "lr": "0.00010625", "gnorm": "1.248", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "1957"}
[2022-01-17 12:31:55,462][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:31:55,879][valid][INFO] - {"epoch": 56, "valid_loss": "3.964", "valid_ntokens": "1662", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.789", "valid_code_perplexity": "19.785", "valid_temp": "1.933", "valid_loss_0": "3.795", "valid_loss_1": "0.14", "valid_loss_2": "0.029", "valid_accuracy": "0.40102", "valid_wps": "26651.1", "valid_wpb": "1662", "valid_bsz": "4.5", "valid_num_updates": "6832", "valid_best_loss": "3.821"}
[2022-01-17 12:31:55,881][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 6832 updates
[2022-01-17 12:31:55,881][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:31:59,786][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:31:59,813][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 56 @ 6832 updates, score 3.964) (writing took 3.9326786547899246 seconds)
[2022-01-17 12:31:59,814][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2022-01-17 12:31:59,827][train][INFO] - {"epoch": 56, "train_loss": "3.951", "train_ntokens": "1803.66", "train_nsentences": "4.97541", "train_prob_perplexity": "21.347", "train_code_perplexity": "21.342", "train_temp": "1.933", "train_loss_0": "3.782", "train_loss_1": "0.139", "train_loss_2": "0.03", "train_accuracy": "0.40572", "train_wps": "6808.7", "train_ups": "3.77", "train_wpb": "1803.7", "train_bsz": "5", "train_num_updates": "6832", "train_lr": "0.00010675", "train_gnorm": "1.244", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "1968"}
[2022-01-17 12:31:59,911][fairseq.trainer][INFO] - begin training epoch 57
[2022-01-17 12:31:59,912][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:32:27,977][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:32:28,409][valid][INFO] - {"epoch": 57, "valid_loss": "3.918", "valid_ntokens": "1643", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.265", "valid_code_perplexity": "21.255", "valid_temp": "1.932", "valid_loss_0": "3.748", "valid_loss_1": "0.139", "valid_loss_2": "0.031", "valid_accuracy": "0.40962", "valid_wps": "29377.8", "valid_wpb": "1643", "valid_bsz": "4.5", "valid_num_updates": "6954", "valid_best_loss": "3.821"}
[2022-01-17 12:32:28,411][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 6954 updates
[2022-01-17 12:32:28,412][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:32:32,307][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:32:32,334][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 57 @ 6954 updates, score 3.918) (writing took 3.9225870510563254 seconds)
[2022-01-17 12:32:32,335][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2022-01-17 12:32:32,348][train][INFO] - {"epoch": 57, "train_loss": "3.939", "train_ntokens": "1802.51", "train_nsentences": "4.97541", "train_prob_perplexity": "21.109", "train_code_perplexity": "21.104", "train_temp": "1.932", "train_loss_0": "3.771", "train_loss_1": "0.14", "train_loss_2": "0.029", "train_accuracy": "0.40806", "train_wps": "6764.7", "train_ups": "3.75", "train_wpb": "1802.5", "train_bsz": "5", "train_num_updates": "6954", "train_lr": "0.000108656", "train_gnorm": "1.227", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2001"}
[2022-01-17 12:32:32,422][fairseq.trainer][INFO] - begin training epoch 58
[2022-01-17 12:32:32,423][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:32:43,146][train_inner][INFO] - {"epoch": 58, "update": 57.377, "loss": "3.935", "ntokens": "1792.3", "nsentences": "4.955", "prob_perplexity": "21.025", "code_perplexity": "21.021", "temp": "1.932", "loss_0": "3.766", "loss_1": "0.14", "loss_2": "0.029", "accuracy": "0.40972", "wps": "6537.6", "ups": "3.65", "wpb": "1792.3", "bsz": "5", "num_updates": "7000", "lr": "0.000109375", "gnorm": "1.24", "clip": "0", "train_wall": "45", "gb_free": "14.6", "wall": "2011"}
[2022-01-17 12:33:00,563][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:33:01,013][valid][INFO] - {"epoch": 58, "valid_loss": "3.939", "valid_ntokens": "1645", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.796", "valid_code_perplexity": "21.782", "valid_temp": "1.93", "valid_loss_0": "3.771", "valid_loss_1": "0.139", "valid_loss_2": "0.03", "valid_accuracy": "0.40669", "valid_wps": "27146", "valid_wpb": "1645", "valid_bsz": "4.5", "valid_num_updates": "7076", "valid_best_loss": "3.821"}
[2022-01-17 12:33:01,016][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 7076 updates
[2022-01-17 12:33:01,017][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:33:04,915][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:33:04,937][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 58 @ 7076 updates, score 3.939) (writing took 3.9209081111475825 seconds)
[2022-01-17 12:33:04,937][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2022-01-17 12:33:04,950][train][INFO] - {"epoch": 58, "train_loss": "3.922", "train_ntokens": "1800.66", "train_nsentences": "4.97541", "train_prob_perplexity": "21.139", "train_code_perplexity": "21.136", "train_temp": "1.931", "train_loss_0": "3.752", "train_loss_1": "0.14", "train_loss_2": "0.031", "train_accuracy": "0.41254", "train_wps": "6741", "train_ups": "3.74", "train_wpb": "1800.7", "train_bsz": "5", "train_num_updates": "7076", "train_lr": "0.000110562", "train_gnorm": "1.23", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2033"}
[2022-01-17 12:33:05,015][fairseq.trainer][INFO] - begin training epoch 59
[2022-01-17 12:33:05,016][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:33:32,901][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:33:33,336][valid][INFO] - {"epoch": 59, "valid_loss": "3.827", "valid_ntokens": "1496", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.586", "valid_code_perplexity": "20.592", "valid_temp": "1.929", "valid_loss_0": "3.657", "valid_loss_1": "0.14", "valid_loss_2": "0.03", "valid_accuracy": "0.43282", "valid_wps": "26434.9", "valid_wpb": "1496", "valid_bsz": "4.5", "valid_num_updates": "7198", "valid_best_loss": "3.821"}
[2022-01-17 12:33:33,338][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 7198 updates
[2022-01-17 12:33:33,338][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:33:37,222][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:33:37,237][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 59 @ 7198 updates, score 3.827) (writing took 3.899163876660168 seconds)
[2022-01-17 12:33:37,238][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2022-01-17 12:33:37,251][train][INFO] - {"epoch": 59, "train_loss": "3.928", "train_ntokens": "1796.62", "train_nsentences": "4.97541", "train_prob_perplexity": "21.111", "train_code_perplexity": "21.108", "train_temp": "1.93", "train_loss_0": "3.759", "train_loss_1": "0.14", "train_loss_2": "0.03", "train_accuracy": "0.41214", "train_wps": "6788.7", "train_ups": "3.78", "train_wpb": "1796.6", "train_bsz": "5", "train_num_updates": "7198", "train_lr": "0.000112469", "train_gnorm": "1.201", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2066"}
[2022-01-17 12:33:37,312][fairseq.trainer][INFO] - begin training epoch 60
[2022-01-17 12:33:37,313][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:33:38,113][train_inner][INFO] - {"epoch": 60, "update": 59.016, "loss": "3.93", "ntokens": "1802.79", "nsentences": "4.985", "prob_perplexity": "21.234", "code_perplexity": "21.23", "temp": "1.93", "loss_0": "3.761", "loss_1": "0.139", "loss_2": "0.03", "accuracy": "0.41087", "wps": "6561.1", "ups": "3.64", "wpb": "1802.8", "bsz": "5", "num_updates": "7200", "lr": "0.0001125", "gnorm": "1.205", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "2066"}
[2022-01-17 12:34:05,485][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:34:05,970][valid][INFO] - {"epoch": 60, "valid_loss": "3.985", "valid_ntokens": "1680.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.186", "valid_code_perplexity": "20.178", "valid_temp": "1.928", "valid_loss_0": "3.813", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.40018", "valid_wps": "29597.4", "valid_wpb": "1680.5", "valid_bsz": "4.5", "valid_num_updates": "7320", "valid_best_loss": "3.821"}
[2022-01-17 12:34:05,972][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 7320 updates
[2022-01-17 12:34:05,973][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:34:09,902][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:34:09,927][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 60 @ 7320 updates, score 3.985) (writing took 3.954319846816361 seconds)
[2022-01-17 12:34:09,927][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2022-01-17 12:34:09,940][train][INFO] - {"epoch": 60, "train_loss": "3.919", "train_ntokens": "1796.71", "train_nsentences": "4.97541", "train_prob_perplexity": "21.29", "train_code_perplexity": "21.284", "train_temp": "1.929", "train_loss_0": "3.75", "train_loss_1": "0.139", "train_loss_2": "0.03", "train_accuracy": "0.41149", "train_wps": "6708.2", "train_ups": "3.73", "train_wpb": "1796.7", "train_bsz": "5", "train_num_updates": "7320", "train_lr": "0.000114375", "train_gnorm": "1.202", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "2098"}
[2022-01-17 12:34:10,010][fairseq.trainer][INFO] - begin training epoch 61
[2022-01-17 12:34:10,011][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:34:28,370][train_inner][INFO] - {"epoch": 61, "update": 60.656, "loss": "3.915", "ntokens": "1798.95", "nsentences": "4.97", "prob_perplexity": "21.322", "code_perplexity": "21.317", "temp": "1.928", "loss_0": "3.745", "loss_1": "0.139", "loss_2": "0.03", "accuracy": "0.41282", "wps": "7160.8", "ups": "3.98", "wpb": "1799", "bsz": "5", "num_updates": "7400", "lr": "0.000115625", "gnorm": "1.2", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2117"}
[2022-01-17 12:34:37,963][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:34:38,404][valid][INFO] - {"epoch": 61, "valid_loss": "3.859", "valid_ntokens": "1631.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.912", "valid_code_perplexity": "22.907", "valid_temp": "1.927", "valid_loss_0": "3.684", "valid_loss_1": "0.139", "valid_loss_2": "0.036", "valid_accuracy": "0.41373", "valid_wps": "27399.1", "valid_wpb": "1631.5", "valid_bsz": "4.5", "valid_num_updates": "7442", "valid_best_loss": "3.821"}
[2022-01-17 12:34:38,406][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 7442 updates
[2022-01-17 12:34:38,407][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:34:42,363][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:34:42,388][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 61 @ 7442 updates, score 3.859) (writing took 3.981441631913185 seconds)
[2022-01-17 12:34:42,388][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2022-01-17 12:34:42,401][train][INFO] - {"epoch": 61, "train_loss": "3.909", "train_ntokens": "1800.99", "train_nsentences": "4.97541", "train_prob_perplexity": "21.279", "train_code_perplexity": "21.276", "train_temp": "1.928", "train_loss_0": "3.738", "train_loss_1": "0.139", "train_loss_2": "0.031", "train_accuracy": "0.41534", "train_wps": "6771.4", "train_ups": "3.76", "train_wpb": "1801", "train_bsz": "5", "train_num_updates": "7442", "train_lr": "0.000116281", "train_gnorm": "1.207", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "15.1", "train_wall": "2131"}
[2022-01-17 12:34:42,468][fairseq.trainer][INFO] - begin training epoch 62
[2022-01-17 12:34:42,469][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:35:10,541][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:35:10,993][valid][INFO] - {"epoch": 62, "valid_loss": "3.944", "valid_ntokens": "1685.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.675", "valid_code_perplexity": "20.661", "valid_temp": "1.926", "valid_loss_0": "3.773", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.4067", "valid_wps": "28802", "valid_wpb": "1685.5", "valid_bsz": "4.5", "valid_num_updates": "7564", "valid_best_loss": "3.821"}
[2022-01-17 12:35:10,995][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 7564 updates
[2022-01-17 12:35:10,996][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:35:14,899][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:35:14,923][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 62 @ 7564 updates, score 3.944) (writing took 3.9274842236191034 seconds)
[2022-01-17 12:35:14,923][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2022-01-17 12:35:14,936][train][INFO] - {"epoch": 62, "train_loss": "3.924", "train_ntokens": "1808.52", "train_nsentences": "4.97541", "train_prob_perplexity": "21.365", "train_code_perplexity": "21.361", "train_temp": "1.926", "train_loss_0": "3.754", "train_loss_1": "0.139", "train_loss_2": "0.031", "train_accuracy": "0.40998", "train_wps": "6784.2", "train_ups": "3.75", "train_wpb": "1808.5", "train_bsz": "5", "train_num_updates": "7564", "train_lr": "0.000118188", "train_gnorm": "1.162", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2163"}
[2022-01-17 12:35:15,014][fairseq.trainer][INFO] - begin training epoch 63
[2022-01-17 12:35:15,014][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:35:23,412][train_inner][INFO] - {"epoch": 63, "update": 62.295, "loss": "3.92", "ntokens": "1805.64", "nsentences": "4.985", "prob_perplexity": "21.393", "code_perplexity": "21.389", "temp": "1.926", "loss_0": "3.749", "loss_1": "0.139", "loss_2": "0.031", "accuracy": "0.41117", "wps": "6562.6", "ups": "3.63", "wpb": "1805.6", "bsz": "5", "num_updates": "7600", "lr": "0.00011875", "gnorm": "1.167", "clip": "0", "train_wall": "45", "gb_free": "14.5", "wall": "2172"}
[2022-01-17 12:35:42,777][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:35:43,250][valid][INFO] - {"epoch": 63, "valid_loss": "3.94", "valid_ntokens": "1682.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.504", "valid_code_perplexity": "20.505", "valid_temp": "1.925", "valid_loss_0": "3.77", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.4208", "valid_wps": "27438.3", "valid_wpb": "1682.5", "valid_bsz": "4.5", "valid_num_updates": "7686", "valid_best_loss": "3.821"}
[2022-01-17 12:35:43,252][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 7686 updates
[2022-01-17 12:35:43,253][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:35:47,167][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:35:47,192][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 63 @ 7686 updates, score 3.94) (writing took 3.939413678832352 seconds)
[2022-01-17 12:35:47,192][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2022-01-17 12:35:47,205][train][INFO] - {"epoch": 63, "train_loss": "3.916", "train_ntokens": "1797.3", "train_nsentences": "4.97541", "train_prob_perplexity": "21.2", "train_code_perplexity": "21.195", "train_temp": "1.925", "train_loss_0": "3.746", "train_loss_1": "0.139", "train_loss_2": "0.031", "train_accuracy": "0.41138", "train_wps": "6797.8", "train_ups": "3.78", "train_wpb": "1797.3", "train_bsz": "5", "train_num_updates": "7686", "train_lr": "0.000120094", "train_gnorm": "1.141", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2195"}
[2022-01-17 12:35:47,288][fairseq.trainer][INFO] - begin training epoch 64
[2022-01-17 12:35:47,289][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:36:13,450][train_inner][INFO] - {"epoch": 64, "update": 63.934, "loss": "3.91", "ntokens": "1798.33", "nsentences": "4.97", "prob_perplexity": "21.183", "code_perplexity": "21.178", "temp": "1.924", "loss_0": "3.741", "loss_1": "0.139", "loss_2": "0.03", "accuracy": "0.41074", "wps": "7189.7", "ups": "4", "wpb": "1798.3", "bsz": "5", "num_updates": "7800", "lr": "0.000121875", "gnorm": "1.137", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2222"}
[2022-01-17 12:36:15,272][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:36:15,731][valid][INFO] - {"epoch": 64, "valid_loss": "3.902", "valid_ntokens": "1650.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.437", "valid_code_perplexity": "21.434", "valid_temp": "1.923", "valid_loss_0": "3.729", "valid_loss_1": "0.139", "valid_loss_2": "0.033", "valid_accuracy": "0.42351", "valid_wps": "28224.4", "valid_wpb": "1650.5", "valid_bsz": "4.5", "valid_num_updates": "7808", "valid_best_loss": "3.821"}
[2022-01-17 12:36:15,733][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 7808 updates
[2022-01-17 12:36:15,733][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:36:19,635][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:36:19,664][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 64 @ 7808 updates, score 3.902) (writing took 3.9310823436826468 seconds)
[2022-01-17 12:36:19,664][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2022-01-17 12:36:19,677][train][INFO] - {"epoch": 64, "train_loss": "3.907", "train_ntokens": "1800.2", "train_nsentences": "4.97541", "train_prob_perplexity": "21.357", "train_code_perplexity": "21.353", "train_temp": "1.924", "train_loss_0": "3.737", "train_loss_1": "0.139", "train_loss_2": "0.03", "train_accuracy": "0.40915", "train_wps": "6766.2", "train_ups": "3.76", "train_wpb": "1800.2", "train_bsz": "5", "train_num_updates": "7808", "train_lr": "0.000122", "train_gnorm": "1.129", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2228"}
[2022-01-17 12:36:19,729][fairseq.trainer][INFO] - begin training epoch 65
[2022-01-17 12:36:19,730][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:36:47,584][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:36:48,067][valid][INFO] - {"epoch": 65, "valid_loss": "3.869", "valid_ntokens": "1674.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.202", "valid_code_perplexity": "22.194", "valid_temp": "1.922", "valid_loss_0": "3.697", "valid_loss_1": "0.139", "valid_loss_2": "0.033", "valid_accuracy": "0.43177", "valid_wps": "29008.3", "valid_wpb": "1674.5", "valid_bsz": "4.5", "valid_num_updates": "7930", "valid_best_loss": "3.821"}
[2022-01-17 12:36:48,069][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 7930 updates
[2022-01-17 12:36:48,069][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:36:51,956][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:36:51,974][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 65 @ 7930 updates, score 3.869) (writing took 3.9051750041544437 seconds)
[2022-01-17 12:36:51,974][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2022-01-17 12:36:51,987][train][INFO] - {"epoch": 65, "train_loss": "3.909", "train_ntokens": "1801.9", "train_nsentences": "4.97541", "train_prob_perplexity": "21.511", "train_code_perplexity": "21.507", "train_temp": "1.923", "train_loss_0": "3.739", "train_loss_1": "0.139", "train_loss_2": "0.031", "train_accuracy": "0.41128", "train_wps": "6806.6", "train_ups": "3.78", "train_wpb": "1801.9", "train_bsz": "5", "train_num_updates": "7930", "train_lr": "0.000123906", "train_gnorm": "1.113", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2260"}
[2022-01-17 12:36:52,044][fairseq.trainer][INFO] - begin training epoch 66
[2022-01-17 12:36:52,045][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:37:08,214][train_inner][INFO] - {"epoch": 66, "update": 65.574, "loss": "3.917", "ntokens": "1796.66", "nsentences": "4.97", "prob_perplexity": "21.453", "code_perplexity": "21.45", "temp": "1.923", "loss_0": "3.746", "loss_1": "0.139", "loss_2": "0.031", "accuracy": "0.40965", "wps": "6563", "ups": "3.65", "wpb": "1796.7", "bsz": "5", "num_updates": "8000", "lr": "0.000125", "gnorm": "1.11", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2276"}
[2022-01-17 12:37:20,187][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:37:20,684][valid][INFO] - {"epoch": 66, "valid_loss": "3.919", "valid_ntokens": "1679", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.688", "valid_code_perplexity": "22.694", "valid_temp": "1.921", "valid_loss_0": "3.749", "valid_loss_1": "0.139", "valid_loss_2": "0.03", "valid_accuracy": "0.41364", "valid_wps": "28503.5", "valid_wpb": "1679", "valid_bsz": "4.5", "valid_num_updates": "8052", "valid_best_loss": "3.821"}
[2022-01-17 12:37:20,686][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 8052 updates
[2022-01-17 12:37:20,687][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:37:24,539][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:37:24,572][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 66 @ 8052 updates, score 3.919) (writing took 3.8856292571872473 seconds)
[2022-01-17 12:37:24,572][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2022-01-17 12:37:24,585][train][INFO] - {"epoch": 66, "train_loss": "3.925", "train_ntokens": "1805.79", "train_nsentences": "4.97541", "train_prob_perplexity": "21.295", "train_code_perplexity": "21.291", "train_temp": "1.922", "train_loss_0": "3.754", "train_loss_1": "0.139", "train_loss_2": "0.031", "train_accuracy": "0.40912", "train_wps": "6760.9", "train_ups": "3.74", "train_wpb": "1805.8", "train_bsz": "5", "train_num_updates": "8052", "train_lr": "0.000125813", "train_gnorm": "1.091", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2293"}
[2022-01-17 12:37:24,638][fairseq.trainer][INFO] - begin training epoch 67
[2022-01-17 12:37:24,638][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:37:52,777][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:37:53,237][valid][INFO] - {"epoch": 67, "valid_loss": "3.886", "valid_ntokens": "1638", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.752", "valid_code_perplexity": "21.75", "valid_temp": "1.92", "valid_loss_0": "3.716", "valid_loss_1": "0.139", "valid_loss_2": "0.031", "valid_accuracy": "0.41392", "valid_wps": "28977.8", "valid_wpb": "1638", "valid_bsz": "4.5", "valid_num_updates": "8174", "valid_best_loss": "3.821"}
[2022-01-17 12:37:53,239][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 8174 updates
[2022-01-17 12:37:53,240][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:37:57,243][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:37:57,273][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 67 @ 8174 updates, score 3.886) (writing took 4.034238249994814 seconds)
[2022-01-17 12:37:57,274][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2022-01-17 12:37:57,287][train][INFO] - {"epoch": 67, "train_loss": "3.913", "train_ntokens": "1802.57", "train_nsentences": "4.97541", "train_prob_perplexity": "21.674", "train_code_perplexity": "21.671", "train_temp": "1.92", "train_loss_0": "3.743", "train_loss_1": "0.139", "train_loss_2": "0.03", "train_accuracy": "0.40671", "train_wps": "6727.5", "train_ups": "3.73", "train_wpb": "1802.6", "train_bsz": "5", "train_num_updates": "8174", "train_lr": "0.000127719", "train_gnorm": "1.08", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.5", "train_wall": "2326"}
[2022-01-17 12:37:57,361][fairseq.trainer][INFO] - begin training epoch 68
[2022-01-17 12:37:57,361][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:38:03,424][train_inner][INFO] - {"epoch": 68, "update": 67.213, "loss": "3.913", "ntokens": "1804.76", "nsentences": "4.97", "prob_perplexity": "21.429", "code_perplexity": "21.425", "temp": "1.921", "loss_0": "3.743", "loss_1": "0.139", "loss_2": "0.03", "accuracy": "0.40854", "wps": "6539.4", "ups": "3.62", "wpb": "1804.8", "bsz": "5", "num_updates": "8200", "lr": "0.000128125", "gnorm": "1.084", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2332"}
[2022-01-17 12:38:25,348][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:38:25,793][valid][INFO] - {"epoch": 68, "valid_loss": "3.878", "valid_ntokens": "1684", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.475", "valid_code_perplexity": "21.468", "valid_temp": "1.919", "valid_loss_0": "3.707", "valid_loss_1": "0.139", "valid_loss_2": "0.032", "valid_accuracy": "0.40677", "valid_wps": "28636.1", "valid_wpb": "1684", "valid_bsz": "4.5", "valid_num_updates": "8296", "valid_best_loss": "3.821"}
[2022-01-17 12:38:25,796][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 8296 updates
[2022-01-17 12:38:25,797][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:38:29,777][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:38:29,804][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 68 @ 8296 updates, score 3.878) (writing took 4.007489571347833 seconds)
[2022-01-17 12:38:29,804][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2022-01-17 12:38:29,818][train][INFO] - {"epoch": 68, "train_loss": "3.885", "train_ntokens": "1793.43", "train_nsentences": "4.97541", "train_prob_perplexity": "21.398", "train_code_perplexity": "21.392", "train_temp": "1.919", "train_loss_0": "3.715", "train_loss_1": "0.139", "train_loss_2": "0.031", "train_accuracy": "0.41231", "train_wps": "6728.6", "train_ups": "3.75", "train_wpb": "1793.4", "train_bsz": "5", "train_num_updates": "8296", "train_lr": "0.000129625", "train_gnorm": "1.079", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "2358"}
[2022-01-17 12:38:29,887][fairseq.trainer][INFO] - begin training epoch 69
[2022-01-17 12:38:29,888][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:38:53,821][train_inner][INFO] - {"epoch": 69, "update": 68.852, "loss": "3.897", "ntokens": "1800.15", "nsentences": "4.985", "prob_perplexity": "21.366", "code_perplexity": "21.361", "temp": "1.919", "loss_0": "3.726", "loss_1": "0.139", "loss_2": "0.031", "accuracy": "0.41224", "wps": "7145.6", "ups": "3.97", "wpb": "1800.2", "bsz": "5", "num_updates": "8400", "lr": "0.00013125", "gnorm": "1.067", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2382"}
[2022-01-17 12:38:57,928][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:38:58,388][valid][INFO] - {"epoch": 69, "valid_loss": "3.74", "valid_ntokens": "1596", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.441", "valid_code_perplexity": "21.442", "valid_temp": "1.918", "valid_loss_0": "3.569", "valid_loss_1": "0.139", "valid_loss_2": "0.032", "valid_accuracy": "0.44016", "valid_wps": "30203.9", "valid_wpb": "1596", "valid_bsz": "4.5", "valid_num_updates": "8418", "valid_best_loss": "3.74"}
[2022-01-17 12:38:58,390][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 8418 updates
[2022-01-17 12:38:58,390][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:39:02,278][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:39:11,985][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 69 @ 8418 updates, score 3.74) (writing took 13.595441026613116 seconds)
[2022-01-17 12:39:11,986][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2022-01-17 12:39:12,001][train][INFO] - {"epoch": 69, "train_loss": "3.912", "train_ntokens": "1799.67", "train_nsentences": "4.97541", "train_prob_perplexity": "21.323", "train_code_perplexity": "21.318", "train_temp": "1.918", "train_loss_0": "3.741", "train_loss_1": "0.139", "train_loss_2": "0.031", "train_accuracy": "0.41035", "train_wps": "5206.8", "train_ups": "2.89", "train_wpb": "1799.7", "train_bsz": "5", "train_num_updates": "8418", "train_lr": "0.000131531", "train_gnorm": "1.064", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2400"}
[2022-01-17 12:39:12,053][fairseq.trainer][INFO] - begin training epoch 70
[2022-01-17 12:39:12,054][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:39:40,020][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:39:40,517][valid][INFO] - {"epoch": 70, "valid_loss": "3.764", "valid_ntokens": "1603.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "19.451", "valid_code_perplexity": "19.441", "valid_temp": "1.916", "valid_loss_0": "3.593", "valid_loss_1": "0.14", "valid_loss_2": "0.031", "valid_accuracy": "0.44185", "valid_wps": "26640.4", "valid_wpb": "1603.5", "valid_bsz": "4.5", "valid_num_updates": "8540", "valid_best_loss": "3.74"}
[2022-01-17 12:39:40,519][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 8540 updates
[2022-01-17 12:39:40,520][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:39:44,422][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:39:44,436][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 70 @ 8540 updates, score 3.764) (writing took 3.9165306240320206 seconds)
[2022-01-17 12:39:44,436][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2022-01-17 12:39:44,449][train][INFO] - {"epoch": 70, "train_loss": "3.904", "train_ntokens": "1806.01", "train_nsentences": "4.97541", "train_prob_perplexity": "21.418", "train_code_perplexity": "21.412", "train_temp": "1.917", "train_loss_0": "3.732", "train_loss_1": "0.139", "train_loss_2": "0.033", "train_accuracy": "0.41206", "train_wps": "6793", "train_ups": "3.76", "train_wpb": "1806", "train_bsz": "5", "train_num_updates": "8540", "train_lr": "0.000133437", "train_gnorm": "1.054", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2433"}
[2022-01-17 12:39:44,521][fairseq.trainer][INFO] - begin training epoch 71
[2022-01-17 12:39:44,522][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:39:58,598][train_inner][INFO] - {"epoch": 71, "update": 70.492, "loss": "3.904", "ntokens": "1803.12", "nsentences": "4.97", "prob_perplexity": "21.446", "code_perplexity": "21.442", "temp": "1.917", "loss_0": "3.732", "loss_1": "0.139", "loss_2": "0.032", "accuracy": "0.41082", "wps": "5568.2", "ups": "3.09", "wpb": "1803.1", "bsz": "5", "num_updates": "8600", "lr": "0.000134375", "gnorm": "1.058", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2447"}
[2022-01-17 12:40:12,810][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:40:13,236][valid][INFO] - {"epoch": 71, "valid_loss": "3.809", "valid_ntokens": "1592", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.64", "valid_code_perplexity": "20.634", "valid_temp": "1.915", "valid_loss_0": "3.637", "valid_loss_1": "0.14", "valid_loss_2": "0.033", "valid_accuracy": "0.43247", "valid_wps": "27937.3", "valid_wpb": "1592", "valid_bsz": "4.5", "valid_num_updates": "8662", "valid_best_loss": "3.74"}
[2022-01-17 12:40:13,237][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 8662 updates
[2022-01-17 12:40:13,238][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:40:17,170][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:40:17,194][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 71 @ 8662 updates, score 3.809) (writing took 3.956315014511347 seconds)
[2022-01-17 12:40:17,194][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2022-01-17 12:40:17,207][train][INFO] - {"epoch": 71, "train_loss": "3.893", "train_ntokens": "1805", "train_nsentences": "4.97541", "train_prob_perplexity": "21.28", "train_code_perplexity": "21.28", "train_temp": "1.916", "train_loss_0": "3.722", "train_loss_1": "0.139", "train_loss_2": "0.032", "train_accuracy": "0.41333", "train_wps": "6725", "train_ups": "3.73", "train_wpb": "1805", "train_bsz": "5", "train_num_updates": "8662", "train_lr": "0.000135344", "train_gnorm": "1.039", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "2465"}
[2022-01-17 12:40:17,248][fairseq.trainer][INFO] - begin training epoch 72
[2022-01-17 12:40:17,249][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:40:45,257][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:40:45,695][valid][INFO] - {"epoch": 72, "valid_loss": "3.887", "valid_ntokens": "1672.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "18.524", "valid_code_perplexity": "18.532", "valid_temp": "1.914", "valid_loss_0": "3.714", "valid_loss_1": "0.14", "valid_loss_2": "0.032", "valid_accuracy": "0.41555", "valid_wps": "28783.1", "valid_wpb": "1672.5", "valid_bsz": "4.5", "valid_num_updates": "8784", "valid_best_loss": "3.74"}
[2022-01-17 12:40:45,696][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 8784 updates
[2022-01-17 12:40:45,697][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:40:49,339][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:40:49,366][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 72 @ 8784 updates, score 3.887) (writing took 3.669135577045381 seconds)
[2022-01-17 12:40:49,366][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2022-01-17 12:40:49,379][train][INFO] - {"epoch": 72, "train_loss": "3.897", "train_ntokens": "1807.22", "train_nsentences": "4.97541", "train_prob_perplexity": "21.565", "train_code_perplexity": "21.561", "train_temp": "1.915", "train_loss_0": "3.725", "train_loss_1": "0.139", "train_loss_2": "0.033", "train_accuracy": "0.41353", "train_wps": "6855.9", "train_ups": "3.79", "train_wpb": "1807.2", "train_bsz": "5", "train_num_updates": "8784", "train_lr": "0.00013725", "train_gnorm": "1.031", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2498"}
[2022-01-17 12:40:49,457][fairseq.trainer][INFO] - begin training epoch 73
[2022-01-17 12:40:49,458][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:40:53,425][train_inner][INFO] - {"epoch": 73, "update": 72.131, "loss": "3.894", "ntokens": "1810.61", "nsentences": "4.985", "prob_perplexity": "21.504", "code_perplexity": "21.502", "temp": "1.915", "loss_0": "3.723", "loss_1": "0.139", "loss_2": "0.032", "accuracy": "0.41344", "wps": "6606.4", "ups": "3.65", "wpb": "1810.6", "bsz": "5", "num_updates": "8800", "lr": "0.0001375", "gnorm": "1.023", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2502"}
[2022-01-17 12:41:17,643][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:41:18,068][valid][INFO] - {"epoch": 73, "valid_loss": "3.919", "valid_ntokens": "1711", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.925", "valid_code_perplexity": "21.932", "valid_temp": "1.913", "valid_loss_0": "3.747", "valid_loss_1": "0.139", "valid_loss_2": "0.033", "valid_accuracy": "0.40444", "valid_wps": "29298.1", "valid_wpb": "1711", "valid_bsz": "4.5", "valid_num_updates": "8906", "valid_best_loss": "3.74"}
[2022-01-17 12:41:18,070][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 8906 updates
[2022-01-17 12:41:18,070][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:41:22,011][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:41:22,037][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 73 @ 8906 updates, score 3.919) (writing took 3.9669988863170147 seconds)
[2022-01-17 12:41:22,037][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2022-01-17 12:41:22,049][train][INFO] - {"epoch": 73, "train_loss": "3.902", "train_ntokens": "1800.2", "train_nsentences": "4.97541", "train_prob_perplexity": "21.585", "train_code_perplexity": "21.58", "train_temp": "1.913", "train_loss_0": "3.729", "train_loss_1": "0.139", "train_loss_2": "0.034", "train_accuracy": "0.41014", "train_wps": "6725.1", "train_ups": "3.74", "train_wpb": "1800.2", "train_bsz": "5", "train_num_updates": "8906", "train_lr": "0.000139156", "train_gnorm": "1.021", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "2530"}
[2022-01-17 12:41:22,095][fairseq.trainer][INFO] - begin training epoch 74
[2022-01-17 12:41:22,095][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:41:43,844][train_inner][INFO] - {"epoch": 74, "update": 73.77, "loss": "3.898", "ntokens": "1793.87", "nsentences": "4.97", "prob_perplexity": "21.595", "code_perplexity": "21.592", "temp": "1.913", "loss_0": "3.725", "loss_1": "0.139", "loss_2": "0.034", "accuracy": "0.4093", "wps": "7118.2", "ups": "3.97", "wpb": "1793.9", "bsz": "5", "num_updates": "9000", "lr": "0.000140625", "gnorm": "1.018", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2552"}
[2022-01-17 12:41:50,246][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:41:50,733][valid][INFO] - {"epoch": 74, "valid_loss": "3.814", "valid_ntokens": "1663", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.572", "valid_code_perplexity": "22.575", "valid_temp": "1.912", "valid_loss_0": "3.646", "valid_loss_1": "0.139", "valid_loss_2": "0.029", "valid_accuracy": "0.41311", "valid_wps": "28135.5", "valid_wpb": "1663", "valid_bsz": "4.5", "valid_num_updates": "9028", "valid_best_loss": "3.74"}
[2022-01-17 12:41:50,736][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 9028 updates
[2022-01-17 12:41:50,736][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:41:54,644][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:41:54,667][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 74 @ 9028 updates, score 3.814) (writing took 3.931345656514168 seconds)
[2022-01-17 12:41:54,668][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2022-01-17 12:41:54,680][train][INFO] - {"epoch": 74, "train_loss": "3.895", "train_ntokens": "1798.27", "train_nsentences": "4.97541", "train_prob_perplexity": "21.533", "train_code_perplexity": "21.532", "train_temp": "1.912", "train_loss_0": "3.722", "train_loss_1": "0.139", "train_loss_2": "0.033", "train_accuracy": "0.40925", "train_wps": "6726", "train_ups": "3.74", "train_wpb": "1798.3", "train_bsz": "5", "train_num_updates": "9028", "train_lr": "0.000141063", "train_gnorm": "1.003", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2563"}
[2022-01-17 12:41:54,746][fairseq.trainer][INFO] - begin training epoch 75
[2022-01-17 12:41:54,746][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:42:22,974][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:42:23,449][valid][INFO] - {"epoch": 75, "valid_loss": "3.828", "valid_ntokens": "1597.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.679", "valid_code_perplexity": "21.677", "valid_temp": "1.911", "valid_loss_0": "3.657", "valid_loss_1": "0.139", "valid_loss_2": "0.032", "valid_accuracy": "0.41596", "valid_wps": "28706.2", "valid_wpb": "1597.5", "valid_bsz": "4.5", "valid_num_updates": "9150", "valid_best_loss": "3.74"}
[2022-01-17 12:42:23,451][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 9150 updates
[2022-01-17 12:42:23,451][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:42:27,373][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:42:27,398][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 75 @ 9150 updates, score 3.828) (writing took 3.9475614465773106 seconds)
[2022-01-17 12:42:27,399][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2022-01-17 12:42:27,412][train][INFO] - {"epoch": 75, "train_loss": "3.895", "train_ntokens": "1808.46", "train_nsentences": "4.97541", "train_prob_perplexity": "21.68", "train_code_perplexity": "21.677", "train_temp": "1.911", "train_loss_0": "3.723", "train_loss_1": "0.139", "train_loss_2": "0.033", "train_accuracy": "0.40968", "train_wps": "6743.4", "train_ups": "3.73", "train_wpb": "1808.5", "train_bsz": "5", "train_num_updates": "9150", "train_lr": "0.000142969", "train_gnorm": "1.001", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2596"}
[2022-01-17 12:42:27,474][fairseq.trainer][INFO] - begin training epoch 76
[2022-01-17 12:42:27,474][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:42:39,186][train_inner][INFO] - {"epoch": 76, "update": 75.41, "loss": "3.9", "ntokens": "1813.73", "nsentences": "4.985", "prob_perplexity": "21.7", "code_perplexity": "21.696", "temp": "1.911", "loss_0": "3.729", "loss_1": "0.139", "loss_2": "0.032", "accuracy": "0.40894", "wps": "6556.2", "ups": "3.61", "wpb": "1813.7", "bsz": "5", "num_updates": "9200", "lr": "0.00014375", "gnorm": "0.992", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2607"}
[2022-01-17 12:42:55,701][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:42:56,131][valid][INFO] - {"epoch": 76, "valid_loss": "4.113", "valid_ntokens": "1755.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.438", "valid_code_perplexity": "21.414", "valid_temp": "1.909", "valid_loss_0": "3.939", "valid_loss_1": "0.139", "valid_loss_2": "0.035", "valid_accuracy": "0.3677", "valid_wps": "31184.6", "valid_wpb": "1755.5", "valid_bsz": "4.5", "valid_num_updates": "9272", "valid_best_loss": "3.74"}
[2022-01-17 12:42:56,132][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 9272 updates
[2022-01-17 12:42:56,133][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:43:00,042][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:43:00,072][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 76 @ 9272 updates, score 4.113) (writing took 3.939274627715349 seconds)
[2022-01-17 12:43:00,072][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2022-01-17 12:43:00,085][train][INFO] - {"epoch": 76, "train_loss": "3.905", "train_ntokens": "1811.81", "train_nsentences": "4.97541", "train_prob_perplexity": "21.599", "train_code_perplexity": "21.596", "train_temp": "1.91", "train_loss_0": "3.734", "train_loss_1": "0.139", "train_loss_2": "0.032", "train_accuracy": "0.40788", "train_wps": "6768", "train_ups": "3.74", "train_wpb": "1811.8", "train_bsz": "5", "train_num_updates": "9272", "train_lr": "0.000144875", "train_gnorm": "0.991", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2628"}
[2022-01-17 12:43:00,154][fairseq.trainer][INFO] - begin training epoch 77
[2022-01-17 12:43:00,154][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:43:28,075][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:43:28,505][valid][INFO] - {"epoch": 77, "valid_loss": "3.95", "valid_ntokens": "1679.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.907", "valid_code_perplexity": "20.902", "valid_temp": "1.908", "valid_loss_0": "3.777", "valid_loss_1": "0.14", "valid_loss_2": "0.033", "valid_accuracy": "0.39149", "valid_wps": "29129", "valid_wpb": "1679.5", "valid_bsz": "4.5", "valid_num_updates": "9394", "valid_best_loss": "3.74"}
[2022-01-17 12:43:28,506][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 9394 updates
[2022-01-17 12:43:28,507][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:43:32,357][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:43:32,381][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 77 @ 9394 updates, score 3.95) (writing took 3.874925539828837 seconds)
[2022-01-17 12:43:32,382][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2022-01-17 12:43:32,394][train][INFO] - {"epoch": 77, "train_loss": "3.883", "train_ntokens": "1803.23", "train_nsentences": "4.97541", "train_prob_perplexity": "21.708", "train_code_perplexity": "21.706", "train_temp": "1.909", "train_loss_0": "3.711", "train_loss_1": "0.139", "train_loss_2": "0.033", "train_accuracy": "0.41212", "train_wps": "6811.6", "train_ups": "3.78", "train_wpb": "1803.2", "train_bsz": "5", "train_num_updates": "9394", "train_lr": "0.000146781", "train_gnorm": "0.984", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "2661"}
[2022-01-17 12:43:32,454][fairseq.trainer][INFO] - begin training epoch 78
[2022-01-17 12:43:32,454][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:43:34,118][train_inner][INFO] - {"epoch": 78, "update": 77.049, "loss": "3.89", "ntokens": "1805.05", "nsentences": "4.97", "prob_perplexity": "21.526", "code_perplexity": "21.524", "temp": "1.909", "loss_0": "3.717", "loss_1": "0.139", "loss_2": "0.033", "accuracy": "0.41133", "wps": "6573.8", "ups": "3.64", "wpb": "1805", "bsz": "5", "num_updates": "9400", "lr": "0.000146875", "gnorm": "0.989", "clip": "0", "train_wall": "45", "gb_free": "15.1", "wall": "2662"}
[2022-01-17 12:44:00,512][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:44:01,026][valid][INFO] - {"epoch": 78, "valid_loss": "3.905", "valid_ntokens": "1671.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.363", "valid_code_perplexity": "21.364", "valid_temp": "1.907", "valid_loss_0": "3.734", "valid_loss_1": "0.139", "valid_loss_2": "0.032", "valid_accuracy": "0.40562", "valid_wps": "29130.4", "valid_wpb": "1671.5", "valid_bsz": "4.5", "valid_num_updates": "9516", "valid_best_loss": "3.74"}
[2022-01-17 12:44:01,028][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 9516 updates
[2022-01-17 12:44:01,029][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:44:05,159][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:44:05,184][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 78 @ 9516 updates, score 3.905) (writing took 4.155617906711996 seconds)
[2022-01-17 12:44:05,184][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2022-01-17 12:44:05,201][train][INFO] - {"epoch": 78, "train_loss": "3.915", "train_ntokens": "1817.22", "train_nsentences": "4.97541", "train_prob_perplexity": "21.487", "train_code_perplexity": "21.485", "train_temp": "1.908", "train_loss_0": "3.742", "train_loss_1": "0.139", "train_loss_2": "0.033", "train_accuracy": "0.40553", "train_wps": "6761.2", "train_ups": "3.72", "train_wpb": "1817.2", "train_bsz": "5", "train_num_updates": "9516", "train_lr": "0.000148688", "train_gnorm": "0.936", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "2693"}
[2022-01-17 12:44:05,275][fairseq.trainer][INFO] - begin training epoch 79
[2022-01-17 12:44:05,276][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:44:24,787][train_inner][INFO] - {"epoch": 79, "update": 78.689, "loss": "3.91", "ntokens": "1809.04", "nsentences": "4.97", "prob_perplexity": "21.533", "code_perplexity": "21.53", "temp": "1.907", "loss_0": "3.737", "loss_1": "0.139", "loss_2": "0.033", "accuracy": "0.40659", "wps": "7142.5", "ups": "3.95", "wpb": "1809", "bsz": "5", "num_updates": "9600", "lr": "0.00015", "gnorm": "0.959", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2713"}
[2022-01-17 12:44:33,364][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:44:33,834][valid][INFO] - {"epoch": 79, "valid_loss": "3.866", "valid_ntokens": "1614.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.343", "valid_code_perplexity": "20.33", "valid_temp": "1.906", "valid_loss_0": "3.693", "valid_loss_1": "0.14", "valid_loss_2": "0.033", "valid_accuracy": "0.41158", "valid_wps": "28437.8", "valid_wpb": "1614.5", "valid_bsz": "4.5", "valid_num_updates": "9638", "valid_best_loss": "3.74"}
[2022-01-17 12:44:33,837][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 9638 updates
[2022-01-17 12:44:33,838][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:44:37,838][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:44:37,861][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 79 @ 9638 updates, score 3.866) (writing took 4.024351053871214 seconds)
[2022-01-17 12:44:37,862][fairseq_cli.train][INFO] - end of epoch 79 (average epoch stats below)
[2022-01-17 12:44:37,875][train][INFO] - {"epoch": 79, "train_loss": "3.904", "train_ntokens": "1805.95", "train_nsentences": "4.97541", "train_prob_perplexity": "21.638", "train_code_perplexity": "21.634", "train_temp": "1.906", "train_loss_0": "3.731", "train_loss_1": "0.139", "train_loss_2": "0.034", "train_accuracy": "0.4073", "train_wps": "6745.9", "train_ups": "3.74", "train_wpb": "1806", "train_bsz": "5", "train_num_updates": "9638", "train_lr": "0.000150594", "train_gnorm": "0.982", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2726"}
[2022-01-17 12:44:37,934][fairseq.trainer][INFO] - begin training epoch 80
[2022-01-17 12:44:37,935][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:45:05,991][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:45:06,428][valid][INFO] - {"epoch": 80, "valid_loss": "3.798", "valid_ntokens": "1564.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.436", "valid_code_perplexity": "20.433", "valid_temp": "1.905", "valid_loss_0": "3.622", "valid_loss_1": "0.14", "valid_loss_2": "0.037", "valid_accuracy": "0.42633", "valid_wps": "28728.7", "valid_wpb": "1564.5", "valid_bsz": "4.5", "valid_num_updates": "9760", "valid_best_loss": "3.74"}
[2022-01-17 12:45:06,430][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 80 @ 9760 updates
[2022-01-17 12:45:06,430][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:45:10,344][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:45:10,364][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 80 @ 9760 updates, score 3.798) (writing took 3.9343288522213697 seconds)
[2022-01-17 12:45:10,365][fairseq_cli.train][INFO] - end of epoch 80 (average epoch stats below)
[2022-01-17 12:45:10,379][train][INFO] - {"epoch": 80, "train_loss": "3.887", "train_ntokens": "1799.88", "train_nsentences": "4.97541", "train_prob_perplexity": "21.768", "train_code_perplexity": "21.764", "train_temp": "1.905", "train_loss_0": "3.716", "train_loss_1": "0.139", "train_loss_2": "0.032", "train_accuracy": "0.40803", "train_wps": "6758.6", "train_ups": "3.76", "train_wpb": "1799.9", "train_bsz": "5", "train_num_updates": "9760", "train_lr": "0.0001525", "train_gnorm": "0.952", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "2759"}
[2022-01-17 12:45:10,431][fairseq.trainer][INFO] - begin training epoch 81
[2022-01-17 12:45:10,431][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:45:19,926][train_inner][INFO] - {"epoch": 81, "update": 80.328, "loss": "3.883", "ntokens": "1806.67", "nsentences": "4.985", "prob_perplexity": "21.75", "code_perplexity": "21.746", "temp": "1.905", "loss_0": "3.711", "loss_1": "0.139", "loss_2": "0.033", "accuracy": "0.40864", "wps": "6555", "ups": "3.63", "wpb": "1806.7", "bsz": "5", "num_updates": "9800", "lr": "0.000153125", "gnorm": "0.947", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2768"}
[2022-01-17 12:45:38,541][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:45:38,969][valid][INFO] - {"epoch": 81, "valid_loss": "3.776", "valid_ntokens": "1593.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "20.784", "valid_code_perplexity": "20.765", "valid_temp": "1.904", "valid_loss_0": "3.603", "valid_loss_1": "0.14", "valid_loss_2": "0.033", "valid_accuracy": "0.42736", "valid_wps": "27540.8", "valid_wpb": "1593.5", "valid_bsz": "4.5", "valid_num_updates": "9882", "valid_best_loss": "3.74"}
[2022-01-17 12:45:38,971][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 81 @ 9882 updates
[2022-01-17 12:45:38,972][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:45:42,922][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:45:42,954][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 81 @ 9882 updates, score 3.776) (writing took 3.982620242983103 seconds)
[2022-01-17 12:45:42,954][fairseq_cli.train][INFO] - end of epoch 81 (average epoch stats below)
[2022-01-17 12:45:42,967][train][INFO] - {"epoch": 81, "train_loss": "3.877", "train_ntokens": "1796.61", "train_nsentences": "4.97541", "train_prob_perplexity": "21.792", "train_code_perplexity": "21.788", "train_temp": "1.904", "train_loss_0": "3.703", "train_loss_1": "0.139", "train_loss_2": "0.035", "train_accuracy": "0.41036", "train_wps": "6728.5", "train_ups": "3.75", "train_wpb": "1796.6", "train_bsz": "5", "train_num_updates": "9882", "train_lr": "0.000154406", "train_gnorm": "0.953", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "14", "train_wall": "2791"}
[2022-01-17 12:45:43,015][fairseq.trainer][INFO] - begin training epoch 82
[2022-01-17 12:45:43,016][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:46:10,033][train_inner][INFO] - {"epoch": 82, "update": 81.967, "loss": "3.886", "ntokens": "1792.69", "nsentences": "4.97", "prob_perplexity": "21.684", "code_perplexity": "21.681", "temp": "1.903", "loss_0": "3.712", "loss_1": "0.139", "loss_2": "0.034", "accuracy": "0.41136", "wps": "7157.7", "ups": "3.99", "wpb": "1792.7", "bsz": "5", "num_updates": "10000", "lr": "0.00015625", "gnorm": "0.956", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2818"}
[2022-01-17 12:46:10,937][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:46:11,393][valid][INFO] - {"epoch": 82, "valid_loss": "3.865", "valid_ntokens": "1660.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.531", "valid_code_perplexity": "21.529", "valid_temp": "1.902", "valid_loss_0": "3.691", "valid_loss_1": "0.139", "valid_loss_2": "0.034", "valid_accuracy": "0.40831", "valid_wps": "26695", "valid_wpb": "1660.5", "valid_bsz": "4.5", "valid_num_updates": "10004", "valid_best_loss": "3.74"}
[2022-01-17 12:46:11,395][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 82 @ 10004 updates
[2022-01-17 12:46:11,396][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:46:15,383][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:46:15,412][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 82 @ 10004 updates, score 3.865) (writing took 4.017166281118989 seconds)
[2022-01-17 12:46:15,413][fairseq_cli.train][INFO] - end of epoch 82 (average epoch stats below)
[2022-01-17 12:46:15,426][train][INFO] - {"epoch": 82, "train_loss": "3.882", "train_ntokens": "1798.05", "train_nsentences": "4.97541", "train_prob_perplexity": "21.589", "train_code_perplexity": "21.587", "train_temp": "1.903", "train_loss_0": "3.709", "train_loss_1": "0.139", "train_loss_2": "0.034", "train_accuracy": "0.41322", "train_wps": "6760.9", "train_ups": "3.76", "train_wpb": "1798", "train_bsz": "5", "train_num_updates": "10004", "train_lr": "0.000156312", "train_gnorm": "0.942", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.5", "train_wall": "2824"}
[2022-01-17 12:46:15,518][fairseq.trainer][INFO] - begin training epoch 83
[2022-01-17 12:46:15,519][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:46:43,603][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:46:44,035][valid][INFO] - {"epoch": 83, "valid_loss": "3.758", "valid_ntokens": "1579.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.238", "valid_code_perplexity": "22.238", "valid_temp": "1.901", "valid_loss_0": "3.584", "valid_loss_1": "0.139", "valid_loss_2": "0.035", "valid_accuracy": "0.43115", "valid_wps": "27806.4", "valid_wpb": "1579.5", "valid_bsz": "4.5", "valid_num_updates": "10126", "valid_best_loss": "3.74"}
[2022-01-17 12:46:44,036][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 83 @ 10126 updates
[2022-01-17 12:46:44,037][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:46:48,098][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:46:48,123][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 83 @ 10126 updates, score 3.758) (writing took 4.086148004978895 seconds)
[2022-01-17 12:46:48,123][fairseq_cli.train][INFO] - end of epoch 83 (average epoch stats below)
[2022-01-17 12:46:48,136][train][INFO] - {"epoch": 83, "train_loss": "3.856", "train_ntokens": "1796.72", "train_nsentences": "4.97541", "train_prob_perplexity": "22.16", "train_code_perplexity": "22.156", "train_temp": "1.902", "train_loss_0": "3.683", "train_loss_1": "0.139", "train_loss_2": "0.034", "train_accuracy": "0.41315", "train_wps": "6704", "train_ups": "3.73", "train_wpb": "1796.7", "train_bsz": "5", "train_num_updates": "10126", "train_lr": "0.000158219", "train_gnorm": "0.937", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "2856"}
[2022-01-17 12:46:48,211][fairseq.trainer][INFO] - begin training epoch 84
[2022-01-17 12:46:48,212][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:47:05,137][train_inner][INFO] - {"epoch": 84, "update": 83.607, "loss": "3.863", "ntokens": "1799.23", "nsentences": "4.97", "prob_perplexity": "22.035", "code_perplexity": "22.032", "temp": "1.902", "loss_0": "3.689", "loss_1": "0.139", "loss_2": "0.034", "accuracy": "0.41317", "wps": "6531.9", "ups": "3.63", "wpb": "1799.2", "bsz": "5", "num_updates": "10200", "lr": "0.000159375", "gnorm": "0.945", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "2873"}
[2022-01-17 12:47:16,000][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:47:16,442][valid][INFO] - {"epoch": 84, "valid_loss": "3.786", "valid_ntokens": "1616", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.399", "valid_code_perplexity": "22.398", "valid_temp": "1.9", "valid_loss_0": "3.61", "valid_loss_1": "0.139", "valid_loss_2": "0.036", "valid_accuracy": "0.4245", "valid_wps": "27114.3", "valid_wpb": "1616", "valid_bsz": "4.5", "valid_num_updates": "10248", "valid_best_loss": "3.74"}
[2022-01-17 12:47:16,444][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 84 @ 10248 updates
[2022-01-17 12:47:16,444][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:47:20,360][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:47:20,368][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 84 @ 10248 updates, score 3.786) (writing took 3.9243056792765856 seconds)
[2022-01-17 12:47:20,369][fairseq_cli.train][INFO] - end of epoch 84 (average epoch stats below)
[2022-01-17 12:47:20,381][train][INFO] - {"epoch": 84, "train_loss": "3.867", "train_ntokens": "1806.11", "train_nsentences": "4.97541", "train_prob_perplexity": "21.895", "train_code_perplexity": "21.894", "train_temp": "1.901", "train_loss_0": "3.692", "train_loss_1": "0.139", "train_loss_2": "0.035", "train_accuracy": "0.41519", "train_wps": "6836", "train_ups": "3.78", "train_wpb": "1806.1", "train_bsz": "5", "train_num_updates": "10248", "train_lr": "0.000160125", "train_gnorm": "0.944", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2889"}
[2022-01-17 12:47:20,434][fairseq.trainer][INFO] - begin training epoch 85
[2022-01-17 12:47:20,435][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:47:48,420][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:47:48,918][valid][INFO] - {"epoch": 85, "valid_loss": "3.896", "valid_ntokens": "1665", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.87", "valid_code_perplexity": "21.862", "valid_temp": "1.899", "valid_loss_0": "3.722", "valid_loss_1": "0.139", "valid_loss_2": "0.034", "valid_accuracy": "0.4033", "valid_wps": "28216", "valid_wpb": "1665", "valid_bsz": "4.5", "valid_num_updates": "10370", "valid_best_loss": "3.74"}
[2022-01-17 12:47:48,920][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 85 @ 10370 updates
[2022-01-17 12:47:48,920][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:47:52,929][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:47:52,951][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 85 @ 10370 updates, score 3.896) (writing took 4.031564391218126 seconds)
[2022-01-17 12:47:52,952][fairseq_cli.train][INFO] - end of epoch 85 (average epoch stats below)
[2022-01-17 12:47:52,965][train][INFO] - {"epoch": 85, "train_loss": "3.876", "train_ntokens": "1805.55", "train_nsentences": "4.97541", "train_prob_perplexity": "22.177", "train_code_perplexity": "22.174", "train_temp": "1.9", "train_loss_0": "3.702", "train_loss_1": "0.139", "train_loss_2": "0.036", "train_accuracy": "0.41034", "train_wps": "6763", "train_ups": "3.75", "train_wpb": "1805.5", "train_bsz": "5", "train_num_updates": "10370", "train_lr": "0.000162031", "train_gnorm": "0.928", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2921"}
[2022-01-17 12:47:53,041][fairseq.trainer][INFO] - begin training epoch 86
[2022-01-17 12:47:53,042][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:48:00,192][train_inner][INFO] - {"epoch": 86, "update": 85.246, "loss": "3.876", "ntokens": "1809.68", "nsentences": "4.985", "prob_perplexity": "22.138", "code_perplexity": "22.137", "temp": "1.9", "loss_0": "3.701", "loss_1": "0.139", "loss_2": "0.035", "accuracy": "0.41221", "wps": "6575.6", "ups": "3.63", "wpb": "1809.7", "bsz": "5", "num_updates": "10400", "lr": "0.0001625", "gnorm": "0.925", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2928"}
[2022-01-17 12:48:21,060][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:48:21,539][valid][INFO] - {"epoch": 86, "valid_loss": "3.757", "valid_ntokens": "1600.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.84", "valid_code_perplexity": "22.837", "valid_temp": "1.898", "valid_loss_0": "3.58", "valid_loss_1": "0.139", "valid_loss_2": "0.039", "valid_accuracy": "0.43768", "valid_wps": "27434.8", "valid_wpb": "1600.5", "valid_bsz": "4.5", "valid_num_updates": "10492", "valid_best_loss": "3.74"}
[2022-01-17 12:48:21,542][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 86 @ 10492 updates
[2022-01-17 12:48:21,542][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:48:25,589][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:48:25,622][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 86 @ 10492 updates, score 3.757) (writing took 4.080340631306171 seconds)
[2022-01-17 12:48:25,622][fairseq_cli.train][INFO] - end of epoch 86 (average epoch stats below)
[2022-01-17 12:48:25,636][train][INFO] - {"epoch": 86, "train_loss": "3.85", "train_ntokens": "1798.53", "train_nsentences": "4.97541", "train_prob_perplexity": "22.325", "train_code_perplexity": "22.323", "train_temp": "1.898", "train_loss_0": "3.675", "train_loss_1": "0.139", "train_loss_2": "0.036", "train_accuracy": "0.41681", "train_wps": "6718.8", "train_ups": "3.74", "train_wpb": "1798.5", "train_bsz": "5", "train_num_updates": "10492", "train_lr": "0.000163938", "train_gnorm": "0.917", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "2954"}
[2022-01-17 12:48:25,722][fairseq.trainer][INFO] - begin training epoch 87
[2022-01-17 12:48:25,722][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:48:50,261][train_inner][INFO] - {"epoch": 87, "update": 86.885, "loss": "3.849", "ntokens": "1796.83", "nsentences": "4.97", "prob_perplexity": "22.216", "code_perplexity": "22.214", "temp": "1.898", "loss_0": "3.675", "loss_1": "0.139", "loss_2": "0.035", "accuracy": "0.41688", "wps": "7179.3", "ups": "4", "wpb": "1796.8", "bsz": "5", "num_updates": "10600", "lr": "0.000165625", "gnorm": "0.919", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "2979"}
[2022-01-17 12:48:53,456][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:48:53,919][valid][INFO] - {"epoch": 87, "valid_loss": "3.865", "valid_ntokens": "1646", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.359", "valid_code_perplexity": "21.36", "valid_temp": "1.897", "valid_loss_0": "3.689", "valid_loss_1": "0.139", "valid_loss_2": "0.037", "valid_accuracy": "0.42953", "valid_wps": "28185.6", "valid_wpb": "1646", "valid_bsz": "4.5", "valid_num_updates": "10614", "valid_best_loss": "3.74"}
[2022-01-17 12:48:53,921][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 87 @ 10614 updates
[2022-01-17 12:48:53,921][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:48:57,782][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:48:57,798][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 87 @ 10614 updates, score 3.865) (writing took 3.877604794688523 seconds)
[2022-01-17 12:48:57,799][fairseq_cli.train][INFO] - end of epoch 87 (average epoch stats below)
[2022-01-17 12:48:57,811][train][INFO] - {"epoch": 87, "train_loss": "3.861", "train_ntokens": "1802.54", "train_nsentences": "4.97541", "train_prob_perplexity": "22.058", "train_code_perplexity": "22.057", "train_temp": "1.897", "train_loss_0": "3.687", "train_loss_1": "0.139", "train_loss_2": "0.035", "train_accuracy": "0.41562", "train_wps": "6837.4", "train_ups": "3.79", "train_wpb": "1802.5", "train_bsz": "5", "train_num_updates": "10614", "train_lr": "0.000165844", "train_gnorm": "0.924", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "2986"}
[2022-01-17 12:48:57,876][fairseq.trainer][INFO] - begin training epoch 88
[2022-01-17 12:48:57,877][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:49:25,569][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:49:26,051][valid][INFO] - {"epoch": 88, "valid_loss": "3.929", "valid_ntokens": "1622.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.079", "valid_code_perplexity": "22.089", "valid_temp": "1.895", "valid_loss_0": "3.749", "valid_loss_1": "0.139", "valid_loss_2": "0.041", "valid_accuracy": "0.40586", "valid_wps": "28436.5", "valid_wpb": "1622.5", "valid_bsz": "4.5", "valid_num_updates": "10736", "valid_best_loss": "3.74"}
[2022-01-17 12:49:26,053][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 88 @ 10736 updates
[2022-01-17 12:49:26,053][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:49:29,952][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:49:29,978][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 88 @ 10736 updates, score 3.929) (writing took 3.9248546855524182 seconds)
[2022-01-17 12:49:29,978][fairseq_cli.train][INFO] - end of epoch 88 (average epoch stats below)
[2022-01-17 12:49:29,991][train][INFO] - {"epoch": 88, "train_loss": "3.841", "train_ntokens": "1790.4", "train_nsentences": "4.97541", "train_prob_perplexity": "22.157", "train_code_perplexity": "22.155", "train_temp": "1.896", "train_loss_0": "3.665", "train_loss_1": "0.139", "train_loss_2": "0.037", "train_accuracy": "0.41723", "train_wps": "6790.5", "train_ups": "3.79", "train_wpb": "1790.4", "train_bsz": "5", "train_num_updates": "10736", "train_lr": "0.00016775", "train_gnorm": "0.898", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "3018"}
[2022-01-17 12:49:30,069][fairseq.trainer][INFO] - begin training epoch 89
[2022-01-17 12:49:30,069][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:49:44,887][train_inner][INFO] - {"epoch": 89, "update": 88.525, "loss": "3.85", "ntokens": "1793.04", "nsentences": "4.97", "prob_perplexity": "22.06", "code_perplexity": "22.058", "temp": "1.896", "loss_0": "3.674", "loss_1": "0.139", "loss_2": "0.037", "accuracy": "0.41594", "wps": "6566.3", "ups": "3.66", "wpb": "1793", "bsz": "5", "num_updates": "10800", "lr": "0.00016875", "gnorm": "0.901", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "3033"}
[2022-01-17 12:49:57,982][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:49:58,455][valid][INFO] - {"epoch": 89, "valid_loss": "3.935", "valid_ntokens": "1657", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.423", "valid_code_perplexity": "21.399", "valid_temp": "1.894", "valid_loss_0": "3.761", "valid_loss_1": "0.139", "valid_loss_2": "0.034", "valid_accuracy": "0.40736", "valid_wps": "29863.5", "valid_wpb": "1657", "valid_bsz": "4.5", "valid_num_updates": "10858", "valid_best_loss": "3.74"}
[2022-01-17 12:49:58,456][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 89 @ 10858 updates
[2022-01-17 12:49:58,457][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:50:02,374][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:50:02,402][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 89 @ 10858 updates, score 3.935) (writing took 3.945944705978036 seconds)
[2022-01-17 12:50:02,403][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2022-01-17 12:50:02,421][train][INFO] - {"epoch": 89, "train_loss": "3.873", "train_ntokens": "1803.02", "train_nsentences": "4.97541", "train_prob_perplexity": "21.995", "train_code_perplexity": "21.994", "train_temp": "1.895", "train_loss_0": "3.696", "train_loss_1": "0.139", "train_loss_2": "0.037", "train_accuracy": "0.41186", "train_wps": "6786.7", "train_ups": "3.76", "train_wpb": "1803", "train_bsz": "5", "train_num_updates": "10858", "train_lr": "0.000169656", "train_gnorm": "0.897", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "3051"}
[2022-01-17 12:50:02,492][fairseq.trainer][INFO] - begin training epoch 90
[2022-01-17 12:50:02,493][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:50:30,362][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:50:30,847][valid][INFO] - {"epoch": 90, "valid_loss": "3.806", "valid_ntokens": "1641.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.888", "valid_code_perplexity": "22.892", "valid_temp": "1.893", "valid_loss_0": "3.63", "valid_loss_1": "0.139", "valid_loss_2": "0.037", "valid_accuracy": "0.4106", "valid_wps": "28625", "valid_wpb": "1641.5", "valid_bsz": "4.5", "valid_num_updates": "10980", "valid_best_loss": "3.74"}
[2022-01-17 12:50:30,849][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 10980 updates
[2022-01-17 12:50:30,849][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:50:34,680][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:50:34,698][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 90 @ 10980 updates, score 3.806) (writing took 3.8494976609945297 seconds)
[2022-01-17 12:50:34,699][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2022-01-17 12:50:34,713][train][INFO] - {"epoch": 90, "train_loss": "3.843", "train_ntokens": "1795.89", "train_nsentences": "4.97541", "train_prob_perplexity": "22.334", "train_code_perplexity": "22.332", "train_temp": "1.894", "train_loss_0": "3.668", "train_loss_1": "0.139", "train_loss_2": "0.036", "train_accuracy": "0.41555", "train_wps": "6788", "train_ups": "3.78", "train_wpb": "1795.9", "train_bsz": "5", "train_num_updates": "10980", "train_lr": "0.000171563", "train_gnorm": "0.889", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "3083"}
[2022-01-17 12:50:34,758][fairseq.trainer][INFO] - begin training epoch 91
[2022-01-17 12:50:34,759][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:50:39,647][train_inner][INFO] - {"epoch": 91, "update": 90.164, "loss": "3.852", "ntokens": "1801.96", "nsentences": "4.985", "prob_perplexity": "22.275", "code_perplexity": "22.272", "temp": "1.894", "loss_0": "3.676", "loss_1": "0.139", "loss_2": "0.037", "accuracy": "0.41403", "wps": "6582.8", "ups": "3.65", "wpb": "1802", "bsz": "5", "num_updates": "11000", "lr": "0.000171875", "gnorm": "0.887", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "3088"}
[2022-01-17 12:51:02,816][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:51:03,274][valid][INFO] - {"epoch": 91, "valid_loss": "3.757", "valid_ntokens": "1624", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.762", "valid_code_perplexity": "21.769", "valid_temp": "1.892", "valid_loss_0": "3.576", "valid_loss_1": "0.139", "valid_loss_2": "0.042", "valid_accuracy": "0.4178", "valid_wps": "27283", "valid_wpb": "1624", "valid_bsz": "4.5", "valid_num_updates": "11102", "valid_best_loss": "3.74"}
[2022-01-17 12:51:03,276][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 91 @ 11102 updates
[2022-01-17 12:51:03,277][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:51:07,219][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:51:07,244][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 91 @ 11102 updates, score 3.757) (writing took 3.968122429214418 seconds)
[2022-01-17 12:51:07,245][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2022-01-17 12:51:07,257][train][INFO] - {"epoch": 91, "train_loss": "3.819", "train_ntokens": "1799.9", "train_nsentences": "4.97541", "train_prob_perplexity": "22.529", "train_code_perplexity": "22.525", "train_temp": "1.893", "train_loss_0": "3.642", "train_loss_1": "0.139", "train_loss_2": "0.037", "train_accuracy": "0.4173", "train_wps": "6750", "train_ups": "3.75", "train_wpb": "1799.9", "train_bsz": "5", "train_num_updates": "11102", "train_lr": "0.000173469", "train_gnorm": "0.893", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14.6", "train_wall": "3116"}
[2022-01-17 12:51:07,306][fairseq.trainer][INFO] - begin training epoch 92
[2022-01-17 12:51:07,307][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:51:29,679][train_inner][INFO] - {"epoch": 92, "update": 91.803, "loss": "3.823", "ntokens": "1796.82", "nsentences": "4.97", "prob_perplexity": "22.379", "code_perplexity": "22.374", "temp": "1.892", "loss_0": "3.646", "loss_1": "0.139", "loss_2": "0.038", "accuracy": "0.41768", "wps": "7184.5", "ups": "4", "wpb": "1796.8", "bsz": "5", "num_updates": "11200", "lr": "0.000175", "gnorm": "0.893", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "3138"}
[2022-01-17 12:51:35,102][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:51:35,558][valid][INFO] - {"epoch": 92, "valid_loss": "3.678", "valid_ntokens": "1584", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.057", "valid_code_perplexity": "22.055", "valid_temp": "1.891", "valid_loss_0": "3.497", "valid_loss_1": "0.139", "valid_loss_2": "0.042", "valid_accuracy": "0.44192", "valid_wps": "26558", "valid_wpb": "1584", "valid_bsz": "4.5", "valid_num_updates": "11224", "valid_best_loss": "3.678"}
[2022-01-17 12:51:35,560][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 11224 updates
[2022-01-17 12:51:35,561][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:51:39,512][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:51:47,321][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 92 @ 11224 updates, score 3.678) (writing took 11.760545378550887 seconds)
[2022-01-17 12:51:47,322][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2022-01-17 12:51:47,335][train][INFO] - {"epoch": 92, "train_loss": "3.838", "train_ntokens": "1800.61", "train_nsentences": "4.97541", "train_prob_perplexity": "22.26", "train_code_perplexity": "22.255", "train_temp": "1.891", "train_loss_0": "3.66", "train_loss_1": "0.139", "train_loss_2": "0.039", "train_accuracy": "0.41646", "train_wps": "5483", "train_ups": "3.05", "train_wpb": "1800.6", "train_bsz": "5", "train_num_updates": "11224", "train_lr": "0.000175375", "train_gnorm": "0.889", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "3156"}
[2022-01-17 12:51:47,418][fairseq.trainer][INFO] - begin training epoch 93
[2022-01-17 12:51:47,419][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:52:15,257][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:52:15,718][valid][INFO] - {"epoch": 93, "valid_loss": "3.816", "valid_ntokens": "1651.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.957", "valid_code_perplexity": "21.937", "valid_temp": "1.89", "valid_loss_0": "3.638", "valid_loss_1": "0.139", "valid_loss_2": "0.039", "valid_accuracy": "0.41508", "valid_wps": "26521.6", "valid_wpb": "1651.5", "valid_bsz": "4.5", "valid_num_updates": "11346", "valid_best_loss": "3.678"}
[2022-01-17 12:52:15,720][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 93 @ 11346 updates
[2022-01-17 12:52:15,721][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:52:19,638][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:52:19,661][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 93 @ 11346 updates, score 3.816) (writing took 3.940817467868328 seconds)
[2022-01-17 12:52:19,662][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2022-01-17 12:52:19,674][train][INFO] - {"epoch": 93, "train_loss": "3.82", "train_ntokens": "1797.11", "train_nsentences": "4.97541", "train_prob_perplexity": "21.879", "train_code_perplexity": "21.874", "train_temp": "1.89", "train_loss_0": "3.642", "train_loss_1": "0.139", "train_loss_2": "0.039", "train_accuracy": "0.41946", "train_wps": "6782.3", "train_ups": "3.77", "train_wpb": "1797.1", "train_bsz": "5", "train_num_updates": "11346", "train_lr": "0.000177281", "train_gnorm": "0.876", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "3188"}
[2022-01-17 12:52:19,726][fairseq.trainer][INFO] - begin training epoch 94
[2022-01-17 12:52:19,727][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:52:32,263][train_inner][INFO] - {"epoch": 94, "update": 93.443, "loss": "3.821", "ntokens": "1801.74", "nsentences": "4.985", "prob_perplexity": "22.078", "code_perplexity": "22.074", "temp": "1.89", "loss_0": "3.643", "loss_1": "0.139", "loss_2": "0.039", "accuracy": "0.41888", "wps": "5759", "ups": "3.2", "wpb": "1801.7", "bsz": "5", "num_updates": "11400", "lr": "0.000178125", "gnorm": "0.874", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "3201"}
[2022-01-17 12:52:47,579][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:52:48,033][valid][INFO] - {"epoch": 94, "valid_loss": "3.774", "valid_ntokens": "1694", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.431", "valid_code_perplexity": "21.434", "valid_temp": "1.889", "valid_loss_0": "3.594", "valid_loss_1": "0.139", "valid_loss_2": "0.041", "valid_accuracy": "0.43506", "valid_wps": "28278.4", "valid_wpb": "1694", "valid_bsz": "4.5", "valid_num_updates": "11468", "valid_best_loss": "3.678"}
[2022-01-17 12:52:48,035][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 11468 updates
[2022-01-17 12:52:48,036][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:52:51,979][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:52:52,003][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 94 @ 11468 updates, score 3.774) (writing took 3.9680814146995544 seconds)
[2022-01-17 12:52:52,004][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2022-01-17 12:52:52,017][train][INFO] - {"epoch": 94, "train_loss": "3.811", "train_ntokens": "1795.25", "train_nsentences": "4.97541", "train_prob_perplexity": "22.187", "train_code_perplexity": "22.183", "train_temp": "1.889", "train_loss_0": "3.634", "train_loss_1": "0.139", "train_loss_2": "0.038", "train_accuracy": "0.42114", "train_wps": "6774.5", "train_ups": "3.77", "train_wpb": "1795.2", "train_bsz": "5", "train_num_updates": "11468", "train_lr": "0.000179188", "train_gnorm": "0.869", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "3220"}
[2022-01-17 12:52:52,101][fairseq.trainer][INFO] - begin training epoch 95
[2022-01-17 12:52:52,102][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:53:20,281][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:53:20,735][valid][INFO] - {"epoch": 95, "valid_loss": "3.758", "valid_ntokens": "1618.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.719", "valid_code_perplexity": "22.715", "valid_temp": "1.887", "valid_loss_0": "3.578", "valid_loss_1": "0.139", "valid_loss_2": "0.041", "valid_accuracy": "0.41736", "valid_wps": "26718.2", "valid_wpb": "1618.5", "valid_bsz": "4.5", "valid_num_updates": "11590", "valid_best_loss": "3.678"}
[2022-01-17 12:53:20,737][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 95 @ 11590 updates
[2022-01-17 12:53:20,738][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:53:24,527][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:53:24,557][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 95 @ 11590 updates, score 3.758) (writing took 3.819799527525902 seconds)
[2022-01-17 12:53:24,558][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2022-01-17 12:53:24,570][train][INFO] - {"epoch": 95, "train_loss": "3.815", "train_ntokens": "1802.27", "train_nsentences": "4.97541", "train_prob_perplexity": "22.105", "train_code_perplexity": "22.101", "train_temp": "1.888", "train_loss_0": "3.639", "train_loss_1": "0.139", "train_loss_2": "0.037", "train_accuracy": "0.41989", "train_wps": "6757.1", "train_ups": "3.75", "train_wpb": "1802.3", "train_bsz": "5", "train_num_updates": "11590", "train_lr": "0.000181094", "train_gnorm": "0.86", "train_clip": "0", "train_train_wall": "28", "train_gb_free": "13.4", "train_wall": "3253"}
[2022-01-17 12:53:24,636][fairseq.trainer][INFO] - begin training epoch 96
[2022-01-17 12:53:24,637][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:53:27,241][train_inner][INFO] - {"epoch": 96, "update": 95.082, "loss": "3.816", "ntokens": "1797.76", "nsentences": "4.97", "prob_perplexity": "22.12", "code_perplexity": "22.115", "temp": "1.888", "loss_0": "3.639", "loss_1": "0.139", "loss_2": "0.037", "accuracy": "0.42073", "wps": "6541.4", "ups": "3.64", "wpb": "1797.8", "bsz": "5", "num_updates": "11600", "lr": "0.00018125", "gnorm": "0.869", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "3256"}
[2022-01-17 12:53:52,394][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:53:52,847][valid][INFO] - {"epoch": 96, "valid_loss": "3.759", "valid_ntokens": "1630.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.812", "valid_code_perplexity": "22.813", "valid_temp": "1.886", "valid_loss_0": "3.579", "valid_loss_1": "0.139", "valid_loss_2": "0.04", "valid_accuracy": "0.42778", "valid_wps": "26469.8", "valid_wpb": "1630.5", "valid_bsz": "4.5", "valid_num_updates": "11712", "valid_best_loss": "3.678"}
[2022-01-17 12:53:52,850][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 11712 updates
[2022-01-17 12:53:52,851][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:53:56,770][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:53:56,790][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 96 @ 11712 updates, score 3.759) (writing took 3.940376628190279 seconds)
[2022-01-17 12:53:56,791][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2022-01-17 12:53:56,803][train][INFO] - {"epoch": 96, "train_loss": "3.823", "train_ntokens": "1810.28", "train_nsentences": "4.97541", "train_prob_perplexity": "22.336", "train_code_perplexity": "22.333", "train_temp": "1.887", "train_loss_0": "3.646", "train_loss_1": "0.139", "train_loss_2": "0.038", "train_accuracy": "0.41612", "train_wps": "6854.5", "train_ups": "3.79", "train_wpb": "1810.3", "train_bsz": "5", "train_num_updates": "11712", "train_lr": "0.000183", "train_gnorm": "0.852", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "3285"}
[2022-01-17 12:53:56,848][fairseq.trainer][INFO] - begin training epoch 97
[2022-01-17 12:53:56,848][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:54:17,157][train_inner][INFO] - {"epoch": 97, "update": 96.721, "loss": "3.816", "ntokens": "1808.52", "nsentences": "4.985", "prob_perplexity": "22.269", "code_perplexity": "22.266", "temp": "1.886", "loss_0": "3.638", "loss_1": "0.139", "loss_2": "0.039", "accuracy": "0.41798", "wps": "7248.2", "ups": "4.01", "wpb": "1808.5", "bsz": "5", "num_updates": "11800", "lr": "0.000184375", "gnorm": "0.847", "clip": "0", "train_wall": "45", "gb_free": "13.5", "wall": "3305"}
[2022-01-17 12:54:24,792][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:54:25,268][valid][INFO] - {"epoch": 97, "valid_loss": "3.773", "valid_ntokens": "1619.5", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.754", "valid_code_perplexity": "21.742", "valid_temp": "1.885", "valid_loss_0": "3.594", "valid_loss_1": "0.139", "valid_loss_2": "0.04", "valid_accuracy": "0.44273", "valid_wps": "27671.3", "valid_wpb": "1619.5", "valid_bsz": "4.5", "valid_num_updates": "11834", "valid_best_loss": "3.678"}
[2022-01-17 12:54:25,270][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 97 @ 11834 updates
[2022-01-17 12:54:25,270][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:54:29,242][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:54:29,271][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 97 @ 11834 updates, score 3.773) (writing took 4.001101613976061 seconds)
[2022-01-17 12:54:29,271][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2022-01-17 12:54:29,284][train][INFO] - {"epoch": 97, "train_loss": "3.814", "train_ntokens": "1800.3", "train_nsentences": "4.97541", "train_prob_perplexity": "22.081", "train_code_perplexity": "22.078", "train_temp": "1.886", "train_loss_0": "3.635", "train_loss_1": "0.139", "train_loss_2": "0.04", "train_accuracy": "0.4211", "train_wps": "6764.7", "train_ups": "3.76", "train_wpb": "1800.3", "train_bsz": "5", "train_num_updates": "11834", "train_lr": "0.000184906", "train_gnorm": "0.843", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "3318"}
[2022-01-17 12:54:29,360][fairseq.trainer][INFO] - begin training epoch 98
[2022-01-17 12:54:29,361][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:54:57,311][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:54:57,778][valid][INFO] - {"epoch": 98, "valid_loss": "3.706", "valid_ntokens": "1630", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.546", "valid_code_perplexity": "22.551", "valid_temp": "1.884", "valid_loss_0": "3.529", "valid_loss_1": "0.139", "valid_loss_2": "0.038", "valid_accuracy": "0.44571", "valid_wps": "27085.7", "valid_wpb": "1630", "valid_bsz": "4.5", "valid_num_updates": "11956", "valid_best_loss": "3.678"}
[2022-01-17 12:54:57,780][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 11956 updates
[2022-01-17 12:54:57,781][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:55:01,686][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:55:01,708][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 98 @ 11956 updates, score 3.706) (writing took 3.928142367862165 seconds)
[2022-01-17 12:55:01,709][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2022-01-17 12:55:01,723][train][INFO] - {"epoch": 98, "train_loss": "3.805", "train_ntokens": "1809.23", "train_nsentences": "4.97541", "train_prob_perplexity": "22.16", "train_code_perplexity": "22.157", "train_temp": "1.885", "train_loss_0": "3.628", "train_loss_1": "0.139", "train_loss_2": "0.038", "train_accuracy": "0.42039", "train_wps": "6807.2", "train_ups": "3.76", "train_wpb": "1809.2", "train_bsz": "5", "train_num_updates": "11956", "train_lr": "0.000186812", "train_gnorm": "0.858", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.5", "train_wall": "3350"}
[2022-01-17 12:55:01,792][fairseq.trainer][INFO] - begin training epoch 99
[2022-01-17 12:55:01,793][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:55:12,185][train_inner][INFO] - {"epoch": 99, "update": 98.361, "loss": "3.812", "ntokens": "1807.09", "nsentences": "4.97", "prob_perplexity": "22.123", "code_perplexity": "22.119", "temp": "1.884", "loss_0": "3.634", "loss_1": "0.139", "loss_2": "0.039", "accuracy": "0.4204", "wps": "6569.5", "ups": "3.64", "wpb": "1807.1", "bsz": "5", "num_updates": "12000", "lr": "0.0001875", "gnorm": "0.853", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "3360"}
[2022-01-17 12:55:29,755][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:55:30,223][valid][INFO] - {"epoch": 99, "valid_loss": "3.818", "valid_ntokens": "1615", "valid_nsentences": "4.5", "valid_prob_perplexity": "23.334", "valid_code_perplexity": "23.327", "valid_temp": "1.883", "valid_loss_0": "3.641", "valid_loss_1": "0.139", "valid_loss_2": "0.038", "valid_accuracy": "0.39938", "valid_wps": "26980.9", "valid_wpb": "1615", "valid_bsz": "4.5", "valid_num_updates": "12078", "valid_best_loss": "3.678"}
[2022-01-17 12:55:30,225][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 99 @ 12078 updates
[2022-01-17 12:55:30,225][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:55:34,164][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:55:34,192][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 99 @ 12078 updates, score 3.818) (writing took 3.967261826619506 seconds)
[2022-01-17 12:55:34,192][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2022-01-17 12:55:34,205][train][INFO] - {"epoch": 99, "train_loss": "3.819", "train_ntokens": "1802.8", "train_nsentences": "4.97541", "train_prob_perplexity": "22.269", "train_code_perplexity": "22.264", "train_temp": "1.883", "train_loss_0": "3.639", "train_loss_1": "0.139", "train_loss_2": "0.04", "train_accuracy": "0.41892", "train_wps": "6773.7", "train_ups": "3.76", "train_wpb": "1802.8", "train_bsz": "5", "train_num_updates": "12078", "train_lr": "0.000188719", "train_gnorm": "0.846", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "14", "train_wall": "3382"}
[2022-01-17 12:55:34,286][fairseq.trainer][INFO] - begin training epoch 100
[2022-01-17 12:55:34,287][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:56:02,137][train_inner][INFO] - {"epoch": 100, "update": 100.0, "loss": "3.814", "ntokens": "1796.3", "nsentences": "4.97", "prob_perplexity": "22.408", "code_perplexity": "22.404", "temp": "1.883", "loss_0": "3.636", "loss_1": "0.139", "loss_2": "0.039", "accuracy": "0.41901", "wps": "7194.1", "ups": "4", "wpb": "1796.3", "bsz": "5", "num_updates": "12200", "lr": "0.000190625", "gnorm": "0.841", "clip": "0", "train_wall": "45", "gb_free": "13.4", "wall": "3410"}
[2022-01-17 12:56:02,139][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:56:02,633][valid][INFO] - {"epoch": 100, "valid_loss": "3.629", "valid_ntokens": "1598", "valid_nsentences": "4.5", "valid_prob_perplexity": "21.446", "valid_code_perplexity": "21.452", "valid_temp": "1.882", "valid_loss_0": "3.451", "valid_loss_1": "0.139", "valid_loss_2": "0.039", "valid_accuracy": "0.45181", "valid_wps": "30217.4", "valid_wpb": "1598", "valid_bsz": "4.5", "valid_num_updates": "12200", "valid_best_loss": "3.629"}
[2022-01-17 12:56:02,635][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 12200 updates
[2022-01-17 12:56:02,635][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:56:06,506][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-17 12:56:14,397][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 100 @ 12200 updates, score 3.629) (writing took 11.762180802412331 seconds)
[2022-01-17 12:56:14,398][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2022-01-17 12:56:14,412][train][INFO] - {"epoch": 100, "train_loss": "3.811", "train_ntokens": "1796.92", "train_nsentences": "4.97541", "train_prob_perplexity": "22.525", "train_code_perplexity": "22.521", "train_temp": "1.882", "train_loss_0": "3.633", "train_loss_1": "0.139", "train_loss_2": "0.039", "train_accuracy": "0.41917", "train_wps": "5454.4", "train_ups": "3.04", "train_wpb": "1796.9", "train_bsz": "5", "train_num_updates": "12200", "train_lr": "0.000190625", "train_gnorm": "0.842", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "3423"}
[2022-01-17 12:56:14,495][fairseq.trainer][INFO] - begin training epoch 101
[2022-01-17 12:56:14,496][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:56:42,326][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:56:42,786][valid][INFO] - {"epoch": 101, "valid_loss": "3.71", "valid_ntokens": "1620", "valid_nsentences": "4.5", "valid_prob_perplexity": "23.445", "valid_code_perplexity": "23.435", "valid_temp": "1.881", "valid_loss_0": "3.532", "valid_loss_1": "0.139", "valid_loss_2": "0.039", "valid_accuracy": "0.43364", "valid_wps": "28014.4", "valid_wpb": "1620", "valid_bsz": "4.5", "valid_num_updates": "12322", "valid_best_loss": "3.629"}
[2022-01-17 12:56:42,789][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 101 @ 12322 updates
[2022-01-17 12:56:42,789][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:56:46,655][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:56:46,668][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 101 @ 12322 updates, score 3.71) (writing took 3.878808884881437 seconds)
[2022-01-17 12:56:46,668][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2022-01-17 12:56:46,680][train][INFO] - {"epoch": 101, "train_loss": "3.809", "train_ntokens": "1798.75", "train_nsentences": "4.97541", "train_prob_perplexity": "22.517", "train_code_perplexity": "22.514", "train_temp": "1.881", "train_loss_0": "3.63", "train_loss_1": "0.139", "train_loss_2": "0.04", "train_accuracy": "0.41889", "train_wps": "6803.3", "train_ups": "3.78", "train_wpb": "1798.7", "train_bsz": "5", "train_num_updates": "12322", "train_lr": "0.000192531", "train_gnorm": "0.826", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "3455"}
[2022-01-17 12:56:46,731][fairseq.trainer][INFO] - begin training epoch 102
[2022-01-17 12:56:46,731][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-17 12:57:04,658][train_inner][INFO] - {"epoch": 102, "update": 101.639, "loss": "3.807", "ntokens": "1795.65", "nsentences": "4.97", "prob_perplexity": "22.466", "code_perplexity": "22.462", "temp": "1.881", "loss_0": "3.627", "loss_1": "0.139", "loss_2": "0.04", "accuracy": "0.41997", "wps": "5745.4", "ups": "3.2", "wpb": "1795.7", "bsz": "5", "num_updates": "12400", "lr": "0.00019375", "gnorm": "0.831", "clip": "0", "train_wall": "45", "gb_free": "14", "wall": "3473"}
[2022-01-17 12:57:14,604][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-17 12:57:15,095][valid][INFO] - {"epoch": 102, "valid_loss": "3.745", "valid_ntokens": "1581", "valid_nsentences": "4.5", "valid_prob_perplexity": "22.446", "valid_code_perplexity": "22.441", "valid_temp": "1.879", "valid_loss_0": "3.561", "valid_loss_1": "0.139", "valid_loss_2": "0.045", "valid_accuracy": "0.44813", "valid_wps": "26555", "valid_wpb": "1581", "valid_bsz": "4.5", "valid_num_updates": "12444", "valid_best_loss": "3.629"}
[2022-01-17 12:57:15,097][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 12444 updates
[2022-01-17 12:57:15,098][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:57:19,021][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2022-01-17 12:57:19,040][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 102 @ 12444 updates, score 3.745) (writing took 3.942406540736556 seconds)
[2022-01-17 12:57:19,040][fairseq_cli.train][INFO] - end of epoch 102 (average epoch stats below)
[2022-01-17 12:57:19,053][train][INFO] - {"epoch": 102, "train_loss": "3.794", "train_ntokens": "1793.71", "train_nsentences": "4.97541", "train_prob_perplexity": "22.5", "train_code_perplexity": "22.495", "train_temp": "1.88", "train_loss_0": "3.615", "train_loss_1": "0.139", "train_loss_2": "0.04", "train_accuracy": "0.42212", "train_wps": "6762.4", "train_ups": "3.77", "train_wpb": "1793.7", "train_bsz": "5", "train_num_updates": "12444", "train_lr": "0.000194438", "train_gnorm": "0.83", "train_clip": "0", "train_train_wall": "27", "train_gb_free": "13.4", "train_wall": "3487"}
[2022-01-17 12:57:19,119][fairseq.trainer][INFO] - begin training epoch 103
[2022-01-17 12:57:19,120][fairseq_cli.train][INFO] - Start iterating over samples
