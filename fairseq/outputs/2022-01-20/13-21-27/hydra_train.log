[2022-01-20 13:21:27,768][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': 'tb_logs', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1400000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1400000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.1, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'checkpoint_activations': False}, 'task': {'_name': 'audio_pretraining', 'data': '/local/scratch/bestalex/cut_10s_mixed_usv/train-clean-100', 'labels': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 250000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': 'none'}, 'criterion': {'_name': 'wav2vec', 'infonce': True, 'loss_weights': [0.1, 10.0], 'log_keys': ['prob_perplexity', 'code_perplexity', 'temp']}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-01-20 13:21:29,027][fairseq_cli.train][INFO] - Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
        (3): GELU()
      )
      (1): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (2): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (3): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (4): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (5): Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
      (6): Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU()
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.1, inplace=False)
  (dropout_features): Dropout(p=0.1, inplace=False)
  (quantizer): GumbelVectorQuantizer(
    (weight_proj): Linear(in_features=512, out_features=640, bias=True)
  )
  (project_q): Linear(in_features=256, out_features=256, bias=True)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU()
    )
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): Linear(in_features=768, out_features=256, bias=True)
)
[2022-01-20 13:21:29,029][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2022-01-20 13:21:29,029][fairseq_cli.train][INFO] - model: Wav2Vec2Model
[2022-01-20 13:21:29,029][fairseq_cli.train][INFO] - criterion: Wav2vecCriterion
[2022-01-20 13:21:29,030][fairseq_cli.train][INFO] - num. shared model params: 95,044,608 (num. trained: 95,044,608)
[2022-01-20 13:21:29,031][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-01-20 13:21:29,035][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 272, skipped 0 samples
[2022-01-20 13:21:32,437][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias
[2022-01-20 13:21:32,437][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias
[2022-01-20 13:21:32,437][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias
[2022-01-20 13:21:32,437][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias
[2022-01-20 13:21:32,437][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias
[2022-01-20 13:21:32,438][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias
[2022-01-20 13:21:32,438][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-01-20 13:21:32,438][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA Quadro RTX 6000                  
[2022-01-20 13:21:32,438][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-01-20 13:21:32,438][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2022-01-20 13:21:32,439][fairseq_cli.train][INFO] - max tokens per device = 1400000 and max sentences per device = None
[2022-01-20 13:21:32,440][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2022-01-20 13:21:32,440][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2022-01-20 13:21:32,440][fairseq.trainer][INFO] - loading train data for epoch 1
[2022-01-20 13:21:32,472][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 26428, skipped 22 samples
[2022-01-20 13:21:32,687][fairseq.trainer][INFO] - NOTE: your device may support faster training with --fp16 or --amp
[2022-01-20 13:21:32,699][fairseq.trainer][INFO] - begin training epoch 1
[2022-01-20 13:21:32,700][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-20 13:23:09,680][train_inner][INFO] - {"epoch": 1, "update": 0.047, "loss": "9.431", "ntokens": "1789.8", "nsentences": "6.14", "prob_perplexity": "369.115", "code_perplexity": "353.262", "temp": "1.999", "loss_0": "6.684", "loss_1": "0.061", "loss_2": "2.685", "accuracy": "0.01203", "wps": "3711.9", "ups": "2.07", "wpb": "1789.8", "bsz": "6.1", "num_updates": "200", "lr": "3.125e-06", "gnorm": "1.544", "clip": "0", "train_wall": "96", "gb_free": "11.7", "wall": "97"}
[2022-01-20 13:24:46,179][train_inner][INFO] - {"epoch": 1, "update": 0.093, "loss": "7.033", "ntokens": "1771.79", "nsentences": "5.995", "prob_perplexity": "511.615", "code_perplexity": "493.954", "temp": "1.997", "loss_0": "6.662", "loss_1": "0.029", "loss_2": "0.342", "accuracy": "0.01192", "wps": "3672.6", "ups": "2.07", "wpb": "1771.8", "bsz": "6", "num_updates": "400", "lr": "6.25e-06", "gnorm": "0.257", "clip": "0", "train_wall": "96", "gb_free": "10.2", "wall": "194"}
[2022-01-20 13:26:23,339][train_inner][INFO] - {"epoch": 1, "update": 0.14, "loss": "6.744", "ntokens": "1770.28", "nsentences": "6.18", "prob_perplexity": "565.54", "code_perplexity": "548.768", "temp": "1.995", "loss_0": "6.66", "loss_1": "0.017", "loss_2": "0.068", "accuracy": "0.01163", "wps": "3644.6", "ups": "2.06", "wpb": "1770.3", "bsz": "6.2", "num_updates": "600", "lr": "9.375e-06", "gnorm": "0.122", "clip": "0", "train_wall": "97", "gb_free": "11.8", "wall": "291"}
[2022-01-20 13:28:01,501][train_inner][INFO] - {"epoch": 1, "update": 0.186, "loss": "6.701", "ntokens": "1783.98", "nsentences": "6.205", "prob_perplexity": "581.642", "code_perplexity": "564.974", "temp": "1.993", "loss_0": "6.659", "loss_1": "0.013", "loss_2": "0.029", "accuracy": "0.01173", "wps": "3635.3", "ups": "2.04", "wpb": "1784", "bsz": "6.2", "num_updates": "800", "lr": "1.25e-05", "gnorm": "0.092", "clip": "0", "train_wall": "98", "gb_free": "13.9", "wall": "389"}
[2022-01-20 13:29:37,861][train_inner][INFO] - {"epoch": 1, "update": 0.233, "loss": "6.689", "ntokens": "1782.64", "nsentences": "6.01", "prob_perplexity": "580.987", "code_perplexity": "563.071", "temp": "1.991", "loss_0": "6.659", "loss_1": "0.013", "loss_2": "0.018", "accuracy": "0.01126", "wps": "3700.5", "ups": "2.08", "wpb": "1782.6", "bsz": "6", "num_updates": "1000", "lr": "1.5625e-05", "gnorm": "0.08", "clip": "0", "train_wall": "96", "gb_free": "11.4", "wall": "485"}
[2022-01-20 13:31:15,407][train_inner][INFO] - {"epoch": 1, "update": 0.28, "loss": "6.684", "ntokens": "1785.88", "nsentences": "6.12", "prob_perplexity": "585.05", "code_perplexity": "566.773", "temp": "1.989", "loss_0": "6.659", "loss_1": "0.012", "loss_2": "0.013", "accuracy": "0.01149", "wps": "3662.1", "ups": "2.05", "wpb": "1785.9", "bsz": "6.1", "num_updates": "1200", "lr": "1.875e-05", "gnorm": "0.068", "clip": "0", "train_wall": "97", "gb_free": "13.3", "wall": "583"}
[2022-01-20 13:32:51,664][train_inner][INFO] - {"epoch": 1, "update": 0.326, "loss": "6.682", "ntokens": "1770.07", "nsentences": "6.1", "prob_perplexity": "583.501", "code_perplexity": "564.903", "temp": "1.987", "loss_0": "6.659", "loss_1": "0.013", "loss_2": "0.01", "accuracy": "0.01167", "wps": "3678.3", "ups": "2.08", "wpb": "1770.1", "bsz": "6.1", "num_updates": "1400", "lr": "2.1875e-05", "gnorm": "0.063", "clip": "0", "train_wall": "96", "gb_free": "17.9", "wall": "679"}
[2022-01-20 13:34:28,285][train_inner][INFO] - {"epoch": 1, "update": 0.373, "loss": "6.68", "ntokens": "1755.94", "nsentences": "6.105", "prob_perplexity": "582.712", "code_perplexity": "563.512", "temp": "1.985", "loss_0": "6.658", "loss_1": "0.013", "loss_2": "0.009", "accuracy": "0.01207", "wps": "3635.2", "ups": "2.07", "wpb": "1755.9", "bsz": "6.1", "num_updates": "1600", "lr": "2.5e-05", "gnorm": "0.059", "clip": "0", "train_wall": "96", "gb_free": "12.4", "wall": "776"}
[2022-01-20 13:36:06,354][train_inner][INFO] - {"epoch": 1, "update": 0.419, "loss": "6.679", "ntokens": "1781.09", "nsentences": "6.1", "prob_perplexity": "583.735", "code_perplexity": "564.203", "temp": "1.983", "loss_0": "6.659", "loss_1": "0.013", "loss_2": "0.008", "accuracy": "0.0119", "wps": "3632.8", "ups": "2.04", "wpb": "1781.1", "bsz": "6.1", "num_updates": "1800", "lr": "2.8125e-05", "gnorm": "0.056", "clip": "0", "train_wall": "98", "gb_free": "12.3", "wall": "874"}
[2022-01-20 13:37:43,930][train_inner][INFO] - {"epoch": 1, "update": 0.466, "loss": "6.678", "ntokens": "1784.09", "nsentences": "6.285", "prob_perplexity": "582.985", "code_perplexity": "564.383", "temp": "1.981", "loss_0": "6.658", "loss_1": "0.013", "loss_2": "0.007", "accuracy": "0.01184", "wps": "3657.3", "ups": "2.05", "wpb": "1784.1", "bsz": "6.3", "num_updates": "2000", "lr": "3.125e-05", "gnorm": "0.058", "clip": "0", "train_wall": "97", "gb_free": "11.6", "wall": "971"}
[2022-01-20 13:39:21,135][train_inner][INFO] - {"epoch": 1, "update": 0.513, "loss": "6.657", "ntokens": "1788.58", "nsentences": "6.26", "prob_perplexity": "558.161", "code_perplexity": "540.645", "temp": "1.979", "loss_0": "6.632", "loss_1": "0.018", "loss_2": "0.007", "accuracy": "0.01879", "wps": "3680.5", "ups": "2.06", "wpb": "1788.6", "bsz": "6.3", "num_updates": "2200", "lr": "3.4375e-05", "gnorm": "0.169", "clip": "0", "train_wall": "97", "gb_free": "13", "wall": "1069"}
[2022-01-20 13:40:58,227][train_inner][INFO] - {"epoch": 1, "update": 0.559, "loss": "6.596", "ntokens": "1768.79", "nsentences": "6.115", "prob_perplexity": "498.526", "code_perplexity": "484.301", "temp": "1.977", "loss_0": "6.554", "loss_1": "0.032", "loss_2": "0.01", "accuracy": "0.02839", "wps": "3644", "ups": "2.06", "wpb": "1768.8", "bsz": "6.1", "num_updates": "2400", "lr": "3.75e-05", "gnorm": "0.365", "clip": "0", "train_wall": "97", "gb_free": "10", "wall": "1166"}
[2022-01-20 13:42:35,490][train_inner][INFO] - {"epoch": 1, "update": 0.606, "loss": "6.376", "ntokens": "1782.74", "nsentences": "6.015", "prob_perplexity": "368.185", "code_perplexity": "360.26", "temp": "1.975", "loss_0": "6.302", "loss_1": "0.061", "loss_2": "0.013", "accuracy": "0.04181", "wps": "3666.3", "ups": "2.06", "wpb": "1782.7", "bsz": "6", "num_updates": "2600", "lr": "4.0625e-05", "gnorm": "0.79", "clip": "0", "train_wall": "97", "gb_free": "10.8", "wall": "1263"}
[2022-01-20 13:44:11,703][train_inner][INFO] - {"epoch": 1, "update": 0.653, "loss": "6.271", "ntokens": "1777.37", "nsentences": "6.285", "prob_perplexity": "274.668", "code_perplexity": "268.855", "temp": "1.973", "loss_0": "6.173", "loss_1": "0.082", "loss_2": "0.016", "accuracy": "0.0457", "wps": "3695.1", "ups": "2.08", "wpb": "1777.4", "bsz": "6.3", "num_updates": "2800", "lr": "4.375e-05", "gnorm": "1.051", "clip": "0", "train_wall": "96", "gb_free": "12.5", "wall": "1359"}
[2022-01-20 13:45:47,301][train_inner][INFO] - {"epoch": 1, "update": 0.699, "loss": "6.115", "ntokens": "1759.72", "nsentences": "6.35", "prob_perplexity": "163.517", "code_perplexity": "160.086", "temp": "1.971", "loss_0": "5.989", "loss_1": "0.107", "loss_2": "0.018", "accuracy": "0.06442", "wps": "3682", "ups": "2.09", "wpb": "1759.7", "bsz": "6.3", "num_updates": "3000", "lr": "4.6875e-05", "gnorm": "1.322", "clip": "0", "train_wall": "95", "gb_free": "11.2", "wall": "1455"}
[2022-01-20 13:47:25,124][train_inner][INFO] - {"epoch": 1, "update": 0.746, "loss": "6.009", "ntokens": "1787.82", "nsentences": "6.19", "prob_perplexity": "102.163", "code_perplexity": "100.199", "temp": "1.969", "loss_0": "5.869", "loss_1": "0.121", "loss_2": "0.02", "accuracy": "0.0949", "wps": "3655.7", "ups": "2.04", "wpb": "1787.8", "bsz": "6.2", "num_updates": "3200", "lr": "5e-05", "gnorm": "1.455", "clip": "0", "train_wall": "97", "gb_free": "10.9", "wall": "1553"}
[2022-01-20 13:49:01,180][train_inner][INFO] - {"epoch": 1, "update": 0.792, "loss": "5.863", "ntokens": "1760.42", "nsentences": "6.155", "prob_perplexity": "60.645", "code_perplexity": "59.703", "temp": "1.967", "loss_0": "5.711", "loss_1": "0.131", "loss_2": "0.021", "accuracy": "0.14406", "wps": "3665.9", "ups": "2.08", "wpb": "1760.4", "bsz": "6.2", "num_updates": "3400", "lr": "5.3125e-05", "gnorm": "1.643", "clip": "0", "train_wall": "96", "gb_free": "10.9", "wall": "1649"}
[2022-01-20 13:50:38,527][train_inner][INFO] - {"epoch": 1, "update": 0.839, "loss": "5.739", "ntokens": "1776.55", "nsentences": "6.105", "prob_perplexity": "49.066", "code_perplexity": "48.491", "temp": "1.965", "loss_0": "5.584", "loss_1": "0.133", "loss_2": "0.022", "accuracy": "0.1579", "wps": "3650.4", "ups": "2.05", "wpb": "1776.5", "bsz": "6.1", "num_updates": "3600", "lr": "5.625e-05", "gnorm": "1.711", "clip": "0", "train_wall": "97", "gb_free": "12.1", "wall": "1746"}
[2022-01-20 13:52:15,543][train_inner][INFO] - {"epoch": 1, "update": 0.886, "loss": "5.644", "ntokens": "1772.18", "nsentences": "6.1", "prob_perplexity": "43.9", "code_perplexity": "43.476", "temp": "1.963", "loss_0": "5.486", "loss_1": "0.134", "loss_2": "0.023", "accuracy": "0.17182", "wps": "3653.9", "ups": "2.06", "wpb": "1772.2", "bsz": "6.1", "num_updates": "3800", "lr": "5.9375e-05", "gnorm": "1.726", "clip": "0", "train_wall": "97", "gb_free": "12.2", "wall": "1843"}
[2022-01-20 13:53:53,136][train_inner][INFO] - {"epoch": 1, "update": 0.932, "loss": "5.584", "ntokens": "1794.38", "nsentences": "6.315", "prob_perplexity": "40.314", "code_perplexity": "39.993", "temp": "1.961", "loss_0": "5.425", "loss_1": "0.135", "loss_2": "0.024", "accuracy": "0.18924", "wps": "3677.8", "ups": "2.05", "wpb": "1794.4", "bsz": "6.3", "num_updates": "4000", "lr": "6.25e-05", "gnorm": "1.732", "clip": "0", "train_wall": "97", "gb_free": "11.7", "wall": "1941"}
[2022-01-20 13:55:30,616][train_inner][INFO] - {"epoch": 1, "update": 0.979, "loss": "5.527", "ntokens": "1790.29", "nsentences": "6.18", "prob_perplexity": "39.669", "code_perplexity": "39.424", "temp": "1.959", "loss_0": "5.367", "loss_1": "0.135", "loss_2": "0.025", "accuracy": "0.19165", "wps": "3673.6", "ups": "2.05", "wpb": "1790.3", "bsz": "6.2", "num_updates": "4200", "lr": "6.5625e-05", "gnorm": "1.646", "clip": "0", "train_wall": "97", "gb_free": "12.5", "wall": "2038"}
[2022-01-20 13:56:14,842][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-20 13:56:21,586][valid][INFO] - {"epoch": 1, "valid_loss": "5.493", "valid_ntokens": "1720.24", "valid_nsentences": "6.04444", "valid_prob_perplexity": "36.933", "valid_code_perplexity": "36.746", "valid_temp": "1.958", "valid_loss_0": "5.331", "valid_loss_1": "0.136", "valid_loss_2": "0.026", "valid_accuracy": "0.18773", "valid_wps": "11845.3", "valid_wpb": "1720.2", "valid_bsz": "6", "valid_num_updates": "4291"}
[2022-01-20 13:56:21,588][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 4291 updates
[2022-01-20 13:56:21,589][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 13:56:33,300][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 13:57:08,466][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 4291 updates, score 5.493) (writing took 46.877678780816495 seconds)
[2022-01-20 13:57:08,466][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2022-01-20 13:57:08,497][train][INFO] - {"epoch": 1, "train_loss": "6.473", "train_ntokens": "1777.72", "train_nsentences": "6.15894", "train_prob_perplexity": "360.931", "train_code_perplexity": "350.091", "train_temp": "1.979", "train_loss_0": "6.252", "train_loss_1": "0.063", "train_loss_2": "0.159", "train_accuracy": "0.06323", "train_wps": "3572.5", "train_ups": "2.01", "train_wpb": "1777.7", "train_bsz": "6.2", "train_num_updates": "4291", "train_lr": "6.70469e-05", "train_gnorm": "0.781", "train_clip": "0", "train_train_wall": "2074", "train_gb_free": "10.6", "train_wall": "2136"}
[2022-01-20 13:57:08,531][fairseq.trainer][INFO] - begin training epoch 2
[2022-01-20 13:57:08,532][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-20 13:58:01,321][train_inner][INFO] - {"epoch": 2, "update": 1.025, "loss": "5.458", "ntokens": "1769.76", "nsentences": "6.105", "prob_perplexity": "38.854", "code_perplexity": "38.638", "temp": "1.957", "loss_0": "5.297", "loss_1": "0.136", "loss_2": "0.026", "accuracy": "0.20056", "wps": "2348.9", "ups": "1.33", "wpb": "1769.8", "bsz": "6.1", "num_updates": "4400", "lr": "6.875e-05", "gnorm": "1.609", "clip": "0", "train_wall": "96", "gb_free": "12.1", "wall": "2189"}
[2022-01-20 13:59:39,246][train_inner][INFO] - {"epoch": 2, "update": 1.072, "loss": "5.461", "ntokens": "1793.92", "nsentences": "6.215", "prob_perplexity": "37.533", "code_perplexity": "37.362", "temp": "1.956", "loss_0": "5.3", "loss_1": "0.136", "loss_2": "0.025", "accuracy": "0.20173", "wps": "3664.3", "ups": "2.04", "wpb": "1793.9", "bsz": "6.2", "num_updates": "4600", "lr": "7.1875e-05", "gnorm": "1.537", "clip": "0", "train_wall": "98", "gb_free": "10.9", "wall": "2287"}
[2022-01-20 14:01:15,732][train_inner][INFO] - {"epoch": 2, "update": 1.119, "loss": "5.432", "ntokens": "1772.68", "nsentences": "6.135", "prob_perplexity": "39.533", "code_perplexity": "39.361", "temp": "1.954", "loss_0": "5.272", "loss_1": "0.135", "loss_2": "0.025", "accuracy": "0.19606", "wps": "3675", "ups": "2.07", "wpb": "1772.7", "bsz": "6.1", "num_updates": "4800", "lr": "7.5e-05", "gnorm": "1.48", "clip": "0", "train_wall": "96", "gb_free": "11.2", "wall": "2383"}
[2022-01-20 14:02:52,548][train_inner][INFO] - {"epoch": 2, "update": 1.165, "loss": "5.374", "ntokens": "1775.62", "nsentences": "6.035", "prob_perplexity": "38.17", "code_perplexity": "38.038", "temp": "1.952", "loss_0": "5.212", "loss_1": "0.136", "loss_2": "0.027", "accuracy": "0.20997", "wps": "3668.5", "ups": "2.07", "wpb": "1775.6", "bsz": "6", "num_updates": "5000", "lr": "7.8125e-05", "gnorm": "1.468", "clip": "0", "train_wall": "96", "gb_free": "12.5", "wall": "2480"}
[2022-01-20 14:04:29,247][train_inner][INFO] - {"epoch": 2, "update": 1.212, "loss": "5.362", "ntokens": "1779.04", "nsentences": "6.15", "prob_perplexity": "37.838", "code_perplexity": "37.704", "temp": "1.95", "loss_0": "5.2", "loss_1": "0.136", "loss_2": "0.027", "accuracy": "0.21088", "wps": "3680.1", "ups": "2.07", "wpb": "1779", "bsz": "6.2", "num_updates": "5200", "lr": "8.125e-05", "gnorm": "1.396", "clip": "0", "train_wall": "96", "gb_free": "10.6", "wall": "2577"}
[2022-01-20 14:06:06,234][train_inner][INFO] - {"epoch": 2, "update": 1.258, "loss": "5.317", "ntokens": "1779.52", "nsentences": "6.37", "prob_perplexity": "37.811", "code_perplexity": "37.694", "temp": "1.948", "loss_0": "5.154", "loss_1": "0.136", "loss_2": "0.027", "accuracy": "0.21797", "wps": "3670.1", "ups": "2.06", "wpb": "1779.5", "bsz": "6.4", "num_updates": "5400", "lr": "8.4375e-05", "gnorm": "1.391", "clip": "0", "train_wall": "97", "gb_free": "12.5", "wall": "2674"}
[2022-01-20 14:07:43,557][train_inner][INFO] - {"epoch": 2, "update": 1.305, "loss": "5.266", "ntokens": "1771.58", "nsentences": "6.39", "prob_perplexity": "38.116", "code_perplexity": "37.994", "temp": "1.946", "loss_0": "5.104", "loss_1": "0.136", "loss_2": "0.026", "accuracy": "0.22065", "wps": "3641.1", "ups": "2.06", "wpb": "1771.6", "bsz": "6.4", "num_updates": "5600", "lr": "8.75e-05", "gnorm": "1.331", "clip": "0", "train_wall": "97", "gb_free": "12.2", "wall": "2771"}
[2022-01-20 14:09:20,402][train_inner][INFO] - {"epoch": 2, "update": 1.352, "loss": "5.278", "ntokens": "1790.28", "nsentences": "5.78", "prob_perplexity": "38.555", "code_perplexity": "38.449", "temp": "1.944", "loss_0": "5.116", "loss_1": "0.136", "loss_2": "0.027", "accuracy": "0.22258", "wps": "3697.7", "ups": "2.07", "wpb": "1790.3", "bsz": "5.8", "num_updates": "5800", "lr": "9.0625e-05", "gnorm": "1.269", "clip": "0", "train_wall": "96", "gb_free": "10.8", "wall": "2868"}
[2022-01-20 14:10:58,089][train_inner][INFO] - {"epoch": 2, "update": 1.398, "loss": "5.257", "ntokens": "1773.34", "nsentences": "6.07", "prob_perplexity": "39.45", "code_perplexity": "39.347", "temp": "1.942", "loss_0": "5.093", "loss_1": "0.135", "loss_2": "0.028", "accuracy": "0.22016", "wps": "3631.1", "ups": "2.05", "wpb": "1773.3", "bsz": "6.1", "num_updates": "6000", "lr": "9.375e-05", "gnorm": "1.251", "clip": "0", "train_wall": "97", "gb_free": "13.5", "wall": "2966"}
[2022-01-20 14:12:35,343][train_inner][INFO] - {"epoch": 2, "update": 1.445, "loss": "5.229", "ntokens": "1786.48", "nsentences": "6.41", "prob_perplexity": "38.532", "code_perplexity": "38.442", "temp": "1.94", "loss_0": "5.066", "loss_1": "0.136", "loss_2": "0.027", "accuracy": "0.22385", "wps": "3674.4", "ups": "2.06", "wpb": "1786.5", "bsz": "6.4", "num_updates": "6200", "lr": "9.6875e-05", "gnorm": "1.209", "clip": "0", "train_wall": "97", "gb_free": "10.4", "wall": "3063"}
[2022-01-20 14:14:12,998][train_inner][INFO] - {"epoch": 2, "update": 1.491, "loss": "5.208", "ntokens": "1777.41", "nsentences": "6.365", "prob_perplexity": "39.723", "code_perplexity": "39.634", "temp": "1.938", "loss_0": "5.046", "loss_1": "0.135", "loss_2": "0.027", "accuracy": "0.2241", "wps": "3640.7", "ups": "2.05", "wpb": "1777.4", "bsz": "6.4", "num_updates": "6400", "lr": "0.0001", "gnorm": "1.178", "clip": "0", "train_wall": "97", "gb_free": "10.6", "wall": "3161"}
[2022-01-20 14:15:50,083][train_inner][INFO] - {"epoch": 2, "update": 1.538, "loss": "5.188", "ntokens": "1773.69", "nsentences": "6.235", "prob_perplexity": "39.907", "code_perplexity": "39.822", "temp": "1.936", "loss_0": "5.024", "loss_1": "0.135", "loss_2": "0.029", "accuracy": "0.22894", "wps": "3654.4", "ups": "2.06", "wpb": "1773.7", "bsz": "6.2", "num_updates": "6600", "lr": "0.000103125", "gnorm": "1.175", "clip": "0", "train_wall": "97", "gb_free": "16.9", "wall": "3258"}
[2022-01-20 14:17:27,088][train_inner][INFO] - {"epoch": 2, "update": 1.585, "loss": "5.169", "ntokens": "1786.47", "nsentences": "6.23", "prob_perplexity": "41.127", "code_perplexity": "41.041", "temp": "1.934", "loss_0": "5.004", "loss_1": "0.135", "loss_2": "0.03", "accuracy": "0.2264", "wps": "3683.7", "ups": "2.06", "wpb": "1786.5", "bsz": "6.2", "num_updates": "6800", "lr": "0.00010625", "gnorm": "1.169", "clip": "0", "train_wall": "97", "gb_free": "12.3", "wall": "3355"}
[2022-01-20 14:19:03,606][train_inner][INFO] - {"epoch": 2, "update": 1.631, "loss": "5.101", "ntokens": "1760.09", "nsentences": "6.055", "prob_perplexity": "42.745", "code_perplexity": "42.668", "temp": "1.932", "loss_0": "4.937", "loss_1": "0.135", "loss_2": "0.03", "accuracy": "0.22857", "wps": "3647.7", "ups": "2.07", "wpb": "1760.1", "bsz": "6.1", "num_updates": "7000", "lr": "0.000109375", "gnorm": "1.16", "clip": "0", "train_wall": "96", "gb_free": "11.6", "wall": "3451"}
[2022-01-20 14:20:39,901][train_inner][INFO] - {"epoch": 2, "update": 1.678, "loss": "5.112", "ntokens": "1781.08", "nsentences": "6.14", "prob_perplexity": "41.709", "code_perplexity": "41.65", "temp": "1.93", "loss_0": "4.946", "loss_1": "0.135", "loss_2": "0.031", "accuracy": "0.23298", "wps": "3699.7", "ups": "2.08", "wpb": "1781.1", "bsz": "6.1", "num_updates": "7200", "lr": "0.0001125", "gnorm": "1.141", "clip": "0", "train_wall": "96", "gb_free": "10.9", "wall": "3547"}
[2022-01-20 14:22:17,205][train_inner][INFO] - {"epoch": 2, "update": 1.725, "loss": "5.103", "ntokens": "1805.1", "nsentences": "5.895", "prob_perplexity": "43.156", "code_perplexity": "43.087", "temp": "1.928", "loss_0": "4.937", "loss_1": "0.135", "loss_2": "0.032", "accuracy": "0.23103", "wps": "3710.7", "ups": "2.06", "wpb": "1805.1", "bsz": "5.9", "num_updates": "7400", "lr": "0.000115625", "gnorm": "1.106", "clip": "0", "train_wall": "97", "gb_free": "11.2", "wall": "3645"}
[2022-01-20 14:23:54,400][train_inner][INFO] - {"epoch": 2, "update": 1.771, "loss": "5.022", "ntokens": "1772.58", "nsentences": "6.14", "prob_perplexity": "43.044", "code_perplexity": "42.992", "temp": "1.926", "loss_0": "4.855", "loss_1": "0.135", "loss_2": "0.033", "accuracy": "0.24347", "wps": "3648", "ups": "2.06", "wpb": "1772.6", "bsz": "6.1", "num_updates": "7600", "lr": "0.00011875", "gnorm": "1.111", "clip": "0", "train_wall": "97", "gb_free": "11.4", "wall": "3742"}
[2022-01-20 14:25:29,964][train_inner][INFO] - {"epoch": 2, "update": 1.818, "loss": "5.004", "ntokens": "1752.34", "nsentences": "6.17", "prob_perplexity": "43.982", "code_perplexity": "43.916", "temp": "1.924", "loss_0": "4.837", "loss_1": "0.134", "loss_2": "0.033", "accuracy": "0.24667", "wps": "3667.9", "ups": "2.09", "wpb": "1752.3", "bsz": "6.2", "num_updates": "7800", "lr": "0.000121875", "gnorm": "1.122", "clip": "0", "train_wall": "95", "gb_free": "11.3", "wall": "3838"}
[2022-01-20 14:27:08,018][train_inner][INFO] - {"epoch": 2, "update": 1.864, "loss": "4.991", "ntokens": "1786.37", "nsentences": "6.07", "prob_perplexity": "45.33", "code_perplexity": "45.272", "temp": "1.923", "loss_0": "4.824", "loss_1": "0.134", "loss_2": "0.033", "accuracy": "0.24372", "wps": "3644.1", "ups": "2.04", "wpb": "1786.4", "bsz": "6.1", "num_updates": "8000", "lr": "0.000125", "gnorm": "1.046", "clip": "0", "train_wall": "98", "gb_free": "11.4", "wall": "3936"}
[2022-01-20 14:28:44,745][train_inner][INFO] - {"epoch": 2, "update": 1.911, "loss": "4.987", "ntokens": "1772.27", "nsentences": "6.435", "prob_perplexity": "44.561", "code_perplexity": "44.502", "temp": "1.921", "loss_0": "4.819", "loss_1": "0.134", "loss_2": "0.034", "accuracy": "0.24572", "wps": "3665", "ups": "2.07", "wpb": "1772.3", "bsz": "6.4", "num_updates": "8200", "lr": "0.000128125", "gnorm": "1.049", "clip": "0", "train_wall": "96", "gb_free": "11.5", "wall": "4032"}
[2022-01-20 14:30:21,440][train_inner][INFO] - {"epoch": 2, "update": 1.958, "loss": "4.995", "ntokens": "1776.17", "nsentences": "5.99", "prob_perplexity": "46.222", "code_perplexity": "46.174", "temp": "1.919", "loss_0": "4.828", "loss_1": "0.134", "loss_2": "0.034", "accuracy": "0.24112", "wps": "3674.2", "ups": "2.07", "wpb": "1776.2", "bsz": "6", "num_updates": "8400", "lr": "0.00013125", "gnorm": "1.007", "clip": "0", "train_wall": "96", "gb_free": "10.9", "wall": "4129"}
[2022-01-20 14:31:49,290][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-20 14:31:56,027][valid][INFO] - {"epoch": 2, "valid_loss": "4.94", "valid_ntokens": "1702.24", "valid_nsentences": "6.04444", "valid_prob_perplexity": "43.972", "valid_code_perplexity": "43.932", "valid_temp": "1.916", "valid_loss_0": "4.771", "valid_loss_1": "0.134", "valid_loss_2": "0.035", "valid_accuracy": "0.25643", "valid_wps": "11727.7", "valid_wpb": "1702.2", "valid_bsz": "6", "valid_num_updates": "8582", "valid_best_loss": "4.94"}
[2022-01-20 14:31:56,028][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 8582 updates
[2022-01-20 14:31:56,029][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 14:32:07,944][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 14:32:43,418][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 8582 updates, score 4.94) (writing took 47.389037664048374 seconds)
[2022-01-20 14:32:43,418][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-01-20 14:32:43,446][train][INFO] - {"epoch": 2, "train_loss": "5.191", "train_ntokens": "1777.55", "train_nsentences": "6.15894", "train_prob_perplexity": "41.014", "train_code_perplexity": "40.918", "train_temp": "1.937", "train_loss_0": "5.026", "train_loss_1": "0.135", "train_loss_2": "0.029", "train_accuracy": "0.2259", "train_wps": "3572.7", "train_ups": "2.01", "train_wpb": "1777.6", "train_bsz": "6.2", "train_num_updates": "8582", "train_lr": "0.000134094", "train_gnorm": "1.229", "train_clip": "0", "train_train_wall": "2073", "train_gb_free": "11.8", "train_wall": "4271"}
[2022-01-20 14:32:43,474][fairseq.trainer][INFO] - begin training epoch 3
[2022-01-20 14:32:43,475][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-20 14:32:52,336][train_inner][INFO] - {"epoch": 3, "update": 2.004, "loss": "4.976", "ntokens": "1766.64", "nsentences": "6.14", "prob_perplexity": "46.282", "code_perplexity": "46.238", "temp": "1.917", "loss_0": "4.807", "loss_1": "0.134", "loss_2": "0.035", "accuracy": "0.24344", "wps": "2341.7", "ups": "1.33", "wpb": "1766.6", "bsz": "6.1", "num_updates": "8600", "lr": "0.000134375", "gnorm": "0.996", "clip": "0", "train_wall": "96", "gb_free": "11.9", "wall": "4280"}
[2022-01-20 14:34:28,677][train_inner][INFO] - {"epoch": 3, "update": 2.051, "loss": "4.994", "ntokens": "1770.63", "nsentences": "6.34", "prob_perplexity": "47.388", "code_perplexity": "47.342", "temp": "1.915", "loss_0": "4.825", "loss_1": "0.134", "loss_2": "0.035", "accuracy": "0.2392", "wps": "3676.3", "ups": "2.08", "wpb": "1770.6", "bsz": "6.3", "num_updates": "8800", "lr": "0.0001375", "gnorm": "0.998", "clip": "0", "train_wall": "96", "gb_free": "11.9", "wall": "4376"}
[2022-01-20 14:36:04,743][train_inner][INFO] - {"epoch": 3, "update": 2.097, "loss": "4.958", "ntokens": "1766.48", "nsentences": "6.31", "prob_perplexity": "47.348", "code_perplexity": "47.297", "temp": "1.913", "loss_0": "4.788", "loss_1": "0.134", "loss_2": "0.036", "accuracy": "0.24499", "wps": "3678.2", "ups": "2.08", "wpb": "1766.5", "bsz": "6.3", "num_updates": "9000", "lr": "0.000140625", "gnorm": "0.978", "clip": "0", "train_wall": "96", "gb_free": "11.6", "wall": "4472"}
[2022-01-20 14:37:40,700][train_inner][INFO] - {"epoch": 3, "update": 2.144, "loss": "4.916", "ntokens": "1750.73", "nsentences": "6.37", "prob_perplexity": "47.067", "code_perplexity": "47.022", "temp": "1.911", "loss_0": "4.745", "loss_1": "0.134", "loss_2": "0.037", "accuracy": "0.25173", "wps": "3649.5", "ups": "2.08", "wpb": "1750.7", "bsz": "6.4", "num_updates": "9200", "lr": "0.00014375", "gnorm": "0.992", "clip": "0", "train_wall": "96", "gb_free": "11.8", "wall": "4568"}
[2022-01-20 14:39:18,540][train_inner][INFO] - {"epoch": 3, "update": 2.191, "loss": "4.916", "ntokens": "1787.15", "nsentences": "5.905", "prob_perplexity": "46.682", "code_perplexity": "46.636", "temp": "1.909", "loss_0": "4.745", "loss_1": "0.134", "loss_2": "0.037", "accuracy": "0.2508", "wps": "3653.7", "ups": "2.04", "wpb": "1787.2", "bsz": "5.9", "num_updates": "9400", "lr": "0.000146875", "gnorm": "0.947", "clip": "0", "train_wall": "97", "gb_free": "11.6", "wall": "4666"}
[2022-01-20 14:40:56,328][train_inner][INFO] - {"epoch": 3, "update": 2.237, "loss": "4.91", "ntokens": "1784.05", "nsentences": "6.18", "prob_perplexity": "47.242", "code_perplexity": "47.194", "temp": "1.907", "loss_0": "4.739", "loss_1": "0.134", "loss_2": "0.038", "accuracy": "0.25284", "wps": "3649.3", "ups": "2.05", "wpb": "1784", "bsz": "6.2", "num_updates": "9600", "lr": "0.00015", "gnorm": "0.932", "clip": "0", "train_wall": "97", "gb_free": "11.7", "wall": "4764"}
[2022-01-20 14:42:32,071][train_inner][INFO] - {"epoch": 3, "update": 2.284, "loss": "4.89", "ntokens": "1751.15", "nsentences": "6.335", "prob_perplexity": "47.902", "code_perplexity": "47.863", "temp": "1.905", "loss_0": "4.717", "loss_1": "0.133", "loss_2": "0.039", "accuracy": "0.25473", "wps": "3658.6", "ups": "2.09", "wpb": "1751.2", "bsz": "6.3", "num_updates": "9800", "lr": "0.000153125", "gnorm": "0.929", "clip": "0", "train_wall": "95", "gb_free": "11.6", "wall": "4860"}
[2022-01-20 14:44:09,013][train_inner][INFO] - {"epoch": 3, "update": 2.33, "loss": "4.904", "ntokens": "1774.45", "nsentences": "6.285", "prob_perplexity": "47.603", "code_perplexity": "47.556", "temp": "1.903", "loss_0": "4.731", "loss_1": "0.134", "loss_2": "0.04", "accuracy": "0.25054", "wps": "3661.3", "ups": "2.06", "wpb": "1774.5", "bsz": "6.3", "num_updates": "10000", "lr": "0.00015625", "gnorm": "0.933", "clip": "0", "train_wall": "97", "gb_free": "11.9", "wall": "4957"}
[2022-01-20 14:45:46,802][train_inner][INFO] - {"epoch": 3, "update": 2.377, "loss": "4.926", "ntokens": "1799.98", "nsentences": "5.785", "prob_perplexity": "48.692", "code_perplexity": "48.653", "temp": "1.902", "loss_0": "4.753", "loss_1": "0.133", "loss_2": "0.039", "accuracy": "0.24625", "wps": "3681.9", "ups": "2.05", "wpb": "1800", "bsz": "5.8", "num_updates": "10200", "lr": "0.000159375", "gnorm": "0.922", "clip": "0", "train_wall": "97", "gb_free": "11.6", "wall": "5054"}
[2022-01-20 14:47:23,319][train_inner][INFO] - {"epoch": 3, "update": 2.424, "loss": "4.865", "ntokens": "1773.68", "nsentences": "6.24", "prob_perplexity": "48.889", "code_perplexity": "48.852", "temp": "1.9", "loss_0": "4.692", "loss_1": "0.133", "loss_2": "0.039", "accuracy": "0.25444", "wps": "3675.9", "ups": "2.07", "wpb": "1773.7", "bsz": "6.2", "num_updates": "10400", "lr": "0.0001625", "gnorm": "0.888", "clip": "0", "train_wall": "96", "gb_free": "12.2", "wall": "5151"}
[2022-01-20 14:49:01,337][train_inner][INFO] - {"epoch": 3, "update": 2.47, "loss": "4.866", "ntokens": "1796.02", "nsentences": "6.065", "prob_perplexity": "49.939", "code_perplexity": "49.899", "temp": "1.898", "loss_0": "4.694", "loss_1": "0.133", "loss_2": "0.039", "accuracy": "0.2511", "wps": "3665.2", "ups": "2.04", "wpb": "1796", "bsz": "6.1", "num_updates": "10600", "lr": "0.000165625", "gnorm": "0.888", "clip": "0", "train_wall": "98", "gb_free": "14.4", "wall": "5249"}
[2022-01-20 14:50:38,294][train_inner][INFO] - {"epoch": 3, "update": 2.517, "loss": "4.894", "ntokens": "1791.5", "nsentences": "5.975", "prob_perplexity": "49.934", "code_perplexity": "49.899", "temp": "1.896", "loss_0": "4.721", "loss_1": "0.133", "loss_2": "0.04", "accuracy": "0.24717", "wps": "3696", "ups": "2.06", "wpb": "1791.5", "bsz": "6", "num_updates": "10800", "lr": "0.00016875", "gnorm": "0.86", "clip": "0", "train_wall": "97", "gb_free": "13.4", "wall": "5346"}
[2022-01-20 14:52:14,894][train_inner][INFO] - {"epoch": 3, "update": 2.564, "loss": "4.862", "ntokens": "1765.66", "nsentences": "6.375", "prob_perplexity": "50.054", "code_perplexity": "50.016", "temp": "1.894", "loss_0": "4.689", "loss_1": "0.133", "loss_2": "0.04", "accuracy": "0.2532", "wps": "3656.1", "ups": "2.07", "wpb": "1765.7", "bsz": "6.4", "num_updates": "11000", "lr": "0.000171875", "gnorm": "0.869", "clip": "0", "train_wall": "96", "gb_free": "12.2", "wall": "5442"}
[2022-01-20 14:53:51,401][train_inner][INFO] - {"epoch": 3, "update": 2.61, "loss": "4.86", "ntokens": "1776.22", "nsentences": "6.09", "prob_perplexity": "49.573", "code_perplexity": "49.535", "temp": "1.892", "loss_0": "4.686", "loss_1": "0.133", "loss_2": "0.041", "accuracy": "0.25347", "wps": "3681.5", "ups": "2.07", "wpb": "1776.2", "bsz": "6.1", "num_updates": "11200", "lr": "0.000175", "gnorm": "0.874", "clip": "0", "train_wall": "96", "gb_free": "13.9", "wall": "5539"}
[2022-01-20 14:55:28,389][train_inner][INFO] - {"epoch": 3, "update": 2.657, "loss": "4.853", "ntokens": "1778.17", "nsentences": "6.14", "prob_perplexity": "49.903", "code_perplexity": "49.863", "temp": "1.89", "loss_0": "4.678", "loss_1": "0.133", "loss_2": "0.042", "accuracy": "0.25376", "wps": "3667.3", "ups": "2.06", "wpb": "1778.2", "bsz": "6.1", "num_updates": "11400", "lr": "0.000178125", "gnorm": "0.87", "clip": "0", "train_wall": "97", "gb_free": "12.3", "wall": "5636"}
[2022-01-20 14:57:05,358][train_inner][INFO] - {"epoch": 3, "update": 2.703, "loss": "4.863", "ntokens": "1783.29", "nsentences": "5.95", "prob_perplexity": "50.669", "code_perplexity": "50.63", "temp": "1.888", "loss_0": "4.688", "loss_1": "0.133", "loss_2": "0.042", "accuracy": "0.25347", "wps": "3678.6", "ups": "2.06", "wpb": "1783.3", "bsz": "6", "num_updates": "11600", "lr": "0.00018125", "gnorm": "0.842", "clip": "0", "train_wall": "97", "gb_free": "14.9", "wall": "5733"}
[2022-01-20 14:58:44,035][train_inner][INFO] - {"epoch": 3, "update": 2.75, "loss": "4.875", "ntokens": "1804.64", "nsentences": "5.865", "prob_perplexity": "50.833", "code_perplexity": "50.801", "temp": "1.886", "loss_0": "4.699", "loss_1": "0.133", "loss_2": "0.043", "accuracy": "0.24962", "wps": "3658.1", "ups": "2.03", "wpb": "1804.6", "bsz": "5.9", "num_updates": "11800", "lr": "0.000184375", "gnorm": "0.828", "clip": "0", "train_wall": "98", "gb_free": "12.4", "wall": "5832"}
[2022-01-20 15:00:20,119][train_inner][INFO] - {"epoch": 3, "update": 2.797, "loss": "4.867", "ntokens": "1772.82", "nsentences": "6.11", "prob_perplexity": "49.934", "code_perplexity": "49.889", "temp": "1.884", "loss_0": "4.691", "loss_1": "0.133", "loss_2": "0.042", "accuracy": "0.24935", "wps": "3690.7", "ups": "2.08", "wpb": "1772.8", "bsz": "6.1", "num_updates": "12000", "lr": "0.0001875", "gnorm": "0.819", "clip": "0", "train_wall": "96", "gb_free": "12.2", "wall": "5928"}
[2022-01-20 15:01:56,641][train_inner][INFO] - {"epoch": 3, "update": 2.843, "loss": "4.801", "ntokens": "1753.62", "nsentences": "6.52", "prob_perplexity": "51.045", "code_perplexity": "50.999", "temp": "1.883", "loss_0": "4.625", "loss_1": "0.133", "loss_2": "0.044", "accuracy": "0.25986", "wps": "3634.1", "ups": "2.07", "wpb": "1753.6", "bsz": "6.5", "num_updates": "12200", "lr": "0.000190625", "gnorm": "0.813", "clip": "0", "train_wall": "96", "gb_free": "11.6", "wall": "6024"}
[2022-01-20 15:03:34,502][train_inner][INFO] - {"epoch": 3, "update": 2.89, "loss": "4.834", "ntokens": "1799.01", "nsentences": "6.11", "prob_perplexity": "50.635", "code_perplexity": "50.601", "temp": "1.881", "loss_0": "4.657", "loss_1": "0.133", "loss_2": "0.044", "accuracy": "0.25641", "wps": "3677.2", "ups": "2.04", "wpb": "1799", "bsz": "6.1", "num_updates": "12400", "lr": "0.00019375", "gnorm": "0.81", "clip": "0", "train_wall": "97", "gb_free": "10.9", "wall": "6122"}
[2022-01-20 15:05:11,013][train_inner][INFO] - {"epoch": 3, "update": 2.936, "loss": "4.839", "ntokens": "1773.77", "nsentences": "6.155", "prob_perplexity": "52.533", "code_perplexity": "52.498", "temp": "1.879", "loss_0": "4.662", "loss_1": "0.132", "loss_2": "0.045", "accuracy": "0.25207", "wps": "3676.3", "ups": "2.07", "wpb": "1773.8", "bsz": "6.2", "num_updates": "12600", "lr": "0.000196875", "gnorm": "0.804", "clip": "0", "train_wall": "96", "gb_free": "11.7", "wall": "6219"}
[2022-01-20 15:06:48,126][train_inner][INFO] - {"epoch": 3, "update": 2.983, "loss": "4.828", "ntokens": "1785.88", "nsentences": "6.195", "prob_perplexity": "50.951", "code_perplexity": "50.92", "temp": "1.877", "loss_0": "4.649", "loss_1": "0.133", "loss_2": "0.046", "accuracy": "0.25731", "wps": "3678.4", "ups": "2.06", "wpb": "1785.9", "bsz": "6.2", "num_updates": "12800", "lr": "0.0002", "gnorm": "0.814", "clip": "0", "train_wall": "97", "gb_free": "10.7", "wall": "6316"}
[2022-01-20 15:07:23,719][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-20 15:07:30,449][valid][INFO] - {"epoch": 3, "valid_loss": "4.802", "valid_ntokens": "1704.62", "valid_nsentences": "6.04444", "valid_prob_perplexity": "52.09", "valid_code_perplexity": "52.052", "valid_temp": "1.875", "valid_loss_0": "4.623", "valid_loss_1": "0.133", "valid_loss_2": "0.046", "valid_accuracy": "0.25611", "valid_wps": "11750.6", "valid_wpb": "1704.6", "valid_bsz": "6", "valid_num_updates": "12873", "valid_best_loss": "4.802"}
[2022-01-20 15:07:30,450][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 12873 updates
[2022-01-20 15:07:30,451][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 15:07:42,345][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 15:08:16,051][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 12873 updates, score 4.802) (writing took 45.60080177336931 seconds)
[2022-01-20 15:08:16,052][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2022-01-20 15:08:16,080][train][INFO] - {"epoch": 3, "train_loss": "4.88", "train_ntokens": "1777.9", "train_nsentences": "6.15894", "train_prob_perplexity": "49.311", "train_code_perplexity": "49.271", "train_temp": "1.896", "train_loss_0": "4.706", "train_loss_1": "0.133", "train_loss_2": "0.041", "train_accuracy": "0.25179", "train_wps": "3577.3", "train_ups": "2.01", "train_wpb": "1777.9", "train_bsz": "6.2", "train_num_updates": "12873", "train_lr": "0.000201141", "train_gnorm": "0.885", "train_clip": "0", "train_train_wall": "2072", "train_gb_free": "11.8", "train_wall": "6404"}
[2022-01-20 15:08:16,109][fairseq.trainer][INFO] - begin training epoch 4
[2022-01-20 15:08:16,109][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-20 15:09:16,590][train_inner][INFO] - {"epoch": 4, "update": 3.03, "loss": "4.797", "ntokens": "1759.53", "nsentences": "6.34", "prob_perplexity": "51.33", "code_perplexity": "51.301", "temp": "1.875", "loss_0": "4.617", "loss_1": "0.133", "loss_2": "0.048", "accuracy": "0.26389", "wps": "2370.5", "ups": "1.35", "wpb": "1759.5", "bsz": "6.3", "num_updates": "13000", "lr": "0.000203125", "gnorm": "0.808", "clip": "0", "train_wall": "96", "gb_free": "10", "wall": "6464"}
[2022-01-20 15:10:53,705][train_inner][INFO] - {"epoch": 4, "update": 3.076, "loss": "4.779", "ntokens": "1774.32", "nsentences": "6.26", "prob_perplexity": "52.066", "code_perplexity": "52.023", "temp": "1.873", "loss_0": "4.6", "loss_1": "0.133", "loss_2": "0.047", "accuracy": "0.26143", "wps": "3654.6", "ups": "2.06", "wpb": "1774.3", "bsz": "6.3", "num_updates": "13200", "lr": "0.00020625", "gnorm": "0.782", "clip": "0", "train_wall": "97", "gb_free": "12.8", "wall": "6561"}
[2022-01-20 15:12:32,463][train_inner][INFO] - {"epoch": 4, "update": 3.123, "loss": "4.783", "ntokens": "1787.6", "nsentences": "6.5", "prob_perplexity": "52.373", "code_perplexity": "52.333", "temp": "1.871", "loss_0": "4.603", "loss_1": "0.132", "loss_2": "0.047", "accuracy": "0.25847", "wps": "3620.6", "ups": "2.03", "wpb": "1787.6", "bsz": "6.5", "num_updates": "13400", "lr": "0.000209375", "gnorm": "0.785", "clip": "0", "train_wall": "98", "gb_free": "11.2", "wall": "6660"}
[2022-01-20 15:14:09,273][train_inner][INFO] - {"epoch": 4, "update": 3.169, "loss": "4.801", "ntokens": "1791.43", "nsentences": "6.015", "prob_perplexity": "51.883", "code_perplexity": "51.845", "temp": "1.869", "loss_0": "4.621", "loss_1": "0.133", "loss_2": "0.047", "accuracy": "0.25805", "wps": "3701.4", "ups": "2.07", "wpb": "1791.4", "bsz": "6", "num_updates": "13600", "lr": "0.0002125", "gnorm": "0.785", "clip": "0", "train_wall": "96", "gb_free": "15.2", "wall": "6757"}
[2022-01-20 15:15:46,027][train_inner][INFO] - {"epoch": 4, "update": 3.216, "loss": "5.358", "ntokens": "1765.3", "nsentences": "6.195", "prob_perplexity": "48.257", "code_perplexity": "48.21", "temp": "1.868", "loss_0": "5.176", "loss_1": "0.133", "loss_2": "0.048", "accuracy": "0.19812", "wps": "3649.6", "ups": "2.07", "wpb": "1765.3", "bsz": "6.2", "num_updates": "13800", "lr": "0.000215625", "gnorm": "0.739", "clip": "0", "train_wall": "96", "gb_free": "11.6", "wall": "6854"}
[2022-01-20 15:17:22,966][train_inner][INFO] - {"epoch": 4, "update": 3.263, "loss": "6.777", "ntokens": "1761.13", "nsentences": "6.325", "prob_perplexity": "173.514", "code_perplexity": "171.23", "temp": "1.866", "loss_0": "6.645", "loss_1": "0.105", "loss_2": "0.026", "accuracy": "0.02036", "wps": "3634", "ups": "2.06", "wpb": "1761.1", "bsz": "6.3", "num_updates": "14000", "lr": "0.00021875", "gnorm": "0.497", "clip": "1", "train_wall": "97", "gb_free": "11.5", "wall": "6951"}
[2022-01-20 15:19:00,445][train_inner][INFO] - {"epoch": 4, "update": 3.309, "loss": "6.706", "ntokens": "1768.53", "nsentences": "6.055", "prob_perplexity": "489.439", "code_perplexity": "481.776", "temp": "1.864", "loss_0": "6.658", "loss_1": "0.034", "loss_2": "0.014", "accuracy": "0.01202", "wps": "3629", "ups": "2.05", "wpb": "1768.5", "bsz": "6.1", "num_updates": "14200", "lr": "0.000221875", "gnorm": "0.046", "clip": "0", "train_wall": "97", "gb_free": "13", "wall": "7048"}
[2022-01-20 15:20:37,217][train_inner][INFO] - {"epoch": 4, "update": 3.356, "loss": "6.697", "ntokens": "1780.46", "nsentences": "6.265", "prob_perplexity": "525.158", "code_perplexity": "517.556", "temp": "1.862", "loss_0": "6.658", "loss_1": "0.026", "loss_2": "0.013", "accuracy": "0.01167", "wps": "3680.3", "ups": "2.07", "wpb": "1780.5", "bsz": "6.3", "num_updates": "14400", "lr": "0.000225", "gnorm": "0.042", "clip": "0", "train_wall": "96", "gb_free": "11.3", "wall": "7145"}
[2022-01-20 15:22:15,769][train_inner][INFO] - {"epoch": 4, "update": 3.402, "loss": "6.69", "ntokens": "1799.11", "nsentences": "6.405", "prob_perplexity": "540.593", "code_perplexity": "533.002", "temp": "1.86", "loss_0": "6.658", "loss_1": "0.022", "loss_2": "0.01", "accuracy": "0.01239", "wps": "3651.5", "ups": "2.03", "wpb": "1799.1", "bsz": "6.4", "num_updates": "14600", "lr": "0.000228125", "gnorm": "0.045", "clip": "0", "train_wall": "98", "gb_free": "15", "wall": "7243"}
[2022-01-20 15:23:52,067][train_inner][INFO] - {"epoch": 4, "update": 3.449, "loss": "6.683", "ntokens": "1773.2", "nsentences": "6.05", "prob_perplexity": "556.431", "code_perplexity": "464.759", "temp": "1.858", "loss_0": "6.658", "loss_1": "0.019", "loss_2": "0.006", "accuracy": "0.01192", "wps": "3683.3", "ups": "2.08", "wpb": "1773.2", "bsz": "6", "num_updates": "14800", "lr": "0.00023125", "gnorm": "0.041", "clip": "0", "train_wall": "96", "gb_free": "11.1", "wall": "7340"}
[2022-01-20 15:25:28,091][train_inner][INFO] - {"epoch": 4, "update": 3.496, "loss": "6.671", "ntokens": "1790.22", "nsentences": "6.02", "prob_perplexity": "594.249", "code_perplexity": "193.316", "temp": "1.856", "loss_0": "6.658", "loss_1": "0.01", "loss_2": "0.003", "accuracy": "0.01154", "wps": "3729.2", "ups": "2.08", "wpb": "1790.2", "bsz": "6", "num_updates": "15000", "lr": "0.000234375", "gnorm": "0.027", "clip": "0", "train_wall": "96", "gb_free": "12.4", "wall": "7436"}
[2022-01-20 15:27:05,484][train_inner][INFO] - {"epoch": 4, "update": 3.542, "loss": "6.669", "ntokens": "1774.87", "nsentences": "6.035", "prob_perplexity": "602.814", "code_perplexity": "148.006", "temp": "1.855", "loss_0": "6.658", "loss_1": "0.008", "loss_2": "0.003", "accuracy": "0.01189", "wps": "3645.2", "ups": "2.05", "wpb": "1774.9", "bsz": "6", "num_updates": "15200", "lr": "0.0002375", "gnorm": "0.027", "clip": "0", "train_wall": "97", "gb_free": "11.1", "wall": "7533"}
[2022-01-20 15:28:41,415][train_inner][INFO] - {"epoch": 4, "update": 3.589, "loss": "6.668", "ntokens": "1773.77", "nsentences": "5.82", "prob_perplexity": "606.224", "code_perplexity": "152.383", "temp": "1.853", "loss_0": "6.658", "loss_1": "0.007", "loss_2": "0.002", "accuracy": "0.01153", "wps": "3698.5", "ups": "2.09", "wpb": "1773.8", "bsz": "5.8", "num_updates": "15400", "lr": "0.000240625", "gnorm": "0.025", "clip": "0", "train_wall": "96", "gb_free": "11.4", "wall": "7629"}
[2022-01-20 15:30:18,211][train_inner][INFO] - {"epoch": 4, "update": 3.636, "loss": "6.667", "ntokens": "1780.9", "nsentences": "6.055", "prob_perplexity": "608.192", "code_perplexity": "125.737", "temp": "1.851", "loss_0": "6.658", "loss_1": "0.007", "loss_2": "0.002", "accuracy": "0.01191", "wps": "3680.2", "ups": "2.07", "wpb": "1780.9", "bsz": "6.1", "num_updates": "15600", "lr": "0.00024375", "gnorm": "0.025", "clip": "0", "train_wall": "96", "gb_free": "10.4", "wall": "7726"}
[2022-01-20 15:31:53,907][train_inner][INFO] - {"epoch": 4, "update": 3.682, "loss": "6.667", "ntokens": "1770.51", "nsentences": "5.885", "prob_perplexity": "611.512", "code_perplexity": "115.347", "temp": "1.849", "loss_0": "6.658", "loss_1": "0.006", "loss_2": "0.002", "accuracy": "0.01151", "wps": "3700.8", "ups": "2.09", "wpb": "1770.5", "bsz": "5.9", "num_updates": "15800", "lr": "0.000246875", "gnorm": "0.026", "clip": "0", "train_wall": "95", "gb_free": "10.9", "wall": "7821"}
[2022-01-20 15:33:30,737][train_inner][INFO] - {"epoch": 4, "update": 3.729, "loss": "6.666", "ntokens": "1789.03", "nsentences": "6.265", "prob_perplexity": "614.555", "code_perplexity": "108.251", "temp": "1.847", "loss_0": "6.658", "loss_1": "0.006", "loss_2": "0.002", "accuracy": "0.01184", "wps": "3695.7", "ups": "2.07", "wpb": "1789", "bsz": "6.3", "num_updates": "16000", "lr": "0.00025", "gnorm": "0.024", "clip": "0", "train_wall": "96", "gb_free": "10.4", "wall": "7918"}
[2022-01-20 15:35:06,012][train_inner][INFO] - {"epoch": 4, "update": 3.775, "loss": "6.665", "ntokens": "1772.31", "nsentences": "6.325", "prob_perplexity": "618.623", "code_perplexity": "92.664", "temp": "1.845", "loss_0": "6.658", "loss_1": "0.005", "loss_2": "0.002", "accuracy": "0.01182", "wps": "3720.9", "ups": "2.1", "wpb": "1772.3", "bsz": "6.3", "num_updates": "16200", "lr": "0.000253125", "gnorm": "0.023", "clip": "0", "train_wall": "95", "gb_free": "11.7", "wall": "8014"}
[2022-01-20 15:36:41,413][train_inner][INFO] - {"epoch": 4, "update": 3.822, "loss": "6.664", "ntokens": "1748.5", "nsentences": "6.48", "prob_perplexity": "620.395", "code_perplexity": "87.11", "temp": "1.843", "loss_0": "6.658", "loss_1": "0.004", "loss_2": "0.002", "accuracy": "0.01202", "wps": "3666.1", "ups": "2.1", "wpb": "1748.5", "bsz": "6.5", "num_updates": "16400", "lr": "0.00025625", "gnorm": "0.022", "clip": "0", "train_wall": "95", "gb_free": "11.9", "wall": "8109"}
[2022-01-20 15:38:17,290][train_inner][INFO] - {"epoch": 4, "update": 3.869, "loss": "6.664", "ntokens": "1769.94", "nsentences": "6.22", "prob_perplexity": "621.226", "code_perplexity": "88.208", "temp": "1.842", "loss_0": "6.658", "loss_1": "0.004", "loss_2": "0.002", "accuracy": "0.01194", "wps": "3692.6", "ups": "2.09", "wpb": "1769.9", "bsz": "6.2", "num_updates": "16600", "lr": "0.000259375", "gnorm": "0.022", "clip": "0", "train_wall": "96", "gb_free": "11", "wall": "8205"}
[2022-01-20 15:39:52,454][train_inner][INFO] - {"epoch": 4, "update": 3.915, "loss": "6.663", "ntokens": "1788.56", "nsentences": "6.07", "prob_perplexity": "625.107", "code_perplexity": "83.406", "temp": "1.84", "loss_0": "6.658", "loss_1": "0.003", "loss_2": "0.001", "accuracy": "0.01177", "wps": "3759.4", "ups": "2.1", "wpb": "1788.6", "bsz": "6.1", "num_updates": "16800", "lr": "0.0002625", "gnorm": "0.02", "clip": "0", "train_wall": "95", "gb_free": "11.6", "wall": "8300"}
[2022-01-20 15:41:28,662][train_inner][INFO] - {"epoch": 4, "update": 3.962, "loss": "6.661", "ntokens": "1778.04", "nsentences": "5.93", "prob_perplexity": "628.876", "code_perplexity": "75.687", "temp": "1.838", "loss_0": "6.658", "loss_1": "0.003", "loss_2": "0.001", "accuracy": "0.01187", "wps": "3696.8", "ups": "2.08", "wpb": "1778", "bsz": "5.9", "num_updates": "17000", "lr": "0.000265625", "gnorm": "0.015", "clip": "0", "train_wall": "96", "gb_free": "16.6", "wall": "8396"}
[2022-01-20 15:42:47,285][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-20 15:42:53,954][valid][INFO] - {"epoch": 4, "valid_loss": "0.505", "valid_ntokens": "1689.8", "valid_nsentences": "6.04444", "valid_prob_perplexity": "639.956", "valid_code_perplexity": "2.113", "valid_temp": "1.836", "valid_loss_0": "0.505", "valid_loss_1": "0", "valid_loss_2": "0", "valid_accuracy": "0.87676", "valid_wps": "11785.2", "valid_wpb": "1689.8", "valid_bsz": "6", "valid_num_updates": "17164", "valid_best_loss": "0.505"}
[2022-01-20 15:42:53,955][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 17164 updates
[2022-01-20 15:42:53,956][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 15:43:05,750][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 15:43:41,263][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 17164 updates, score 0.505) (writing took 47.30736109800637 seconds)
[2022-01-20 15:43:41,264][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2022-01-20 15:43:41,295][train][INFO] - {"epoch": 4, "train_loss": "6.297", "train_ntokens": "1776.42", "train_nsentences": "6.15894", "train_prob_perplexity": "456.697", "train_code_perplexity": "176.593", "train_temp": "1.855", "train_loss_0": "6.242", "train_loss_1": "0.041", "train_loss_2": "0.015", "train_accuracy": "0.06299", "train_wps": "3586.8", "train_ups": "2.02", "train_wpb": "1776.4", "train_bsz": "6.2", "train_num_updates": "17164", "train_lr": "0.000268188", "train_gnorm": "0.212", "train_clip": "0", "train_train_wall": "2063", "train_gb_free": "11.9", "train_wall": "8529"}
[2022-01-20 15:43:41,323][fairseq.trainer][INFO] - begin training epoch 5
[2022-01-20 15:43:41,324][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-20 15:43:58,707][train_inner][INFO] - {"epoch": 5, "update": 4.008, "loss": "6.658", "ntokens": "1784.06", "nsentences": "5.945", "prob_perplexity": "639.792", "code_perplexity": "169.45", "temp": "1.836", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01173", "wps": "2378.2", "ups": "1.33", "wpb": "1784.1", "bsz": "5.9", "num_updates": "17200", "lr": "0.00026875", "gnorm": "0.01", "clip": "0", "train_wall": "95", "gb_free": "10.9", "wall": "8546"}
[2022-01-20 15:45:33,672][train_inner][INFO] - {"epoch": 5, "update": 4.055, "loss": "6.658", "ntokens": "1778.4", "nsentences": "6.4", "prob_perplexity": "639.978", "code_perplexity": "509.445", "temp": "1.834", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01173", "wps": "3745.9", "ups": "2.11", "wpb": "1778.4", "bsz": "6.4", "num_updates": "17400", "lr": "0.000271875", "gnorm": "0.004", "clip": "0", "train_wall": "95", "gb_free": "12.3", "wall": "8641"}
[2022-01-20 15:47:10,619][train_inner][INFO] - {"epoch": 5, "update": 4.102, "loss": "6.658", "ntokens": "1791.02", "nsentences": "6.015", "prob_perplexity": "639.979", "code_perplexity": "533.32", "temp": "1.832", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01173", "wps": "3695.4", "ups": "2.06", "wpb": "1791", "bsz": "6", "num_updates": "17600", "lr": "0.000275", "gnorm": "0.004", "clip": "0", "train_wall": "97", "gb_free": "9.9", "wall": "8738"}
[2022-01-20 15:48:47,279][train_inner][INFO] - {"epoch": 5, "update": 4.148, "loss": "6.658", "ntokens": "1782.51", "nsentences": "6.625", "prob_perplexity": "639.979", "code_perplexity": "527.65", "temp": "1.831", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01168", "wps": "3688.7", "ups": "2.07", "wpb": "1782.5", "bsz": "6.6", "num_updates": "17800", "lr": "0.000278125", "gnorm": "0.004", "clip": "0", "train_wall": "96", "gb_free": "11.3", "wall": "8835"}
[2022-01-20 15:50:22,667][train_inner][INFO] - {"epoch": 5, "update": 4.195, "loss": "6.658", "ntokens": "1764.53", "nsentences": "6.425", "prob_perplexity": "639.979", "code_perplexity": "526.569", "temp": "1.829", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0119", "wps": "3700.3", "ups": "2.1", "wpb": "1764.5", "bsz": "6.4", "num_updates": "18000", "lr": "0.00028125", "gnorm": "0.004", "clip": "0", "train_wall": "95", "gb_free": "10.8", "wall": "8930"}
[2022-01-20 15:51:59,257][train_inner][INFO] - {"epoch": 5, "update": 4.241, "loss": "6.658", "ntokens": "1764.89", "nsentences": "6.24", "prob_perplexity": "639.979", "code_perplexity": "527.03", "temp": "1.827", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01151", "wps": "3654.8", "ups": "2.07", "wpb": "1764.9", "bsz": "6.2", "num_updates": "18200", "lr": "0.000284375", "gnorm": "0.004", "clip": "0", "train_wall": "96", "gb_free": "11.7", "wall": "9027"}
[2022-01-20 15:53:35,163][train_inner][INFO] - {"epoch": 5, "update": 4.288, "loss": "6.658", "ntokens": "1770.26", "nsentences": "6.1", "prob_perplexity": "639.979", "code_perplexity": "526.357", "temp": "1.825", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01186", "wps": "3692.1", "ups": "2.09", "wpb": "1770.3", "bsz": "6.1", "num_updates": "18400", "lr": "0.0002875", "gnorm": "0.003", "clip": "0", "train_wall": "96", "gb_free": "12.2", "wall": "9123"}
[2022-01-20 15:55:11,559][train_inner][INFO] - {"epoch": 5, "update": 4.335, "loss": "6.658", "ntokens": "1777.91", "nsentences": "6.185", "prob_perplexity": "639.979", "code_perplexity": "526.243", "temp": "1.823", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01179", "wps": "3689.3", "ups": "2.08", "wpb": "1777.9", "bsz": "6.2", "num_updates": "18600", "lr": "0.000290625", "gnorm": "0.003", "clip": "0", "train_wall": "96", "gb_free": "11.8", "wall": "9219"}
[2022-01-20 15:56:46,963][train_inner][INFO] - {"epoch": 5, "update": 4.381, "loss": "6.658", "ntokens": "1782.53", "nsentences": "6.045", "prob_perplexity": "639.979", "code_perplexity": "525.832", "temp": "1.821", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0118", "wps": "3737.3", "ups": "2.1", "wpb": "1782.5", "bsz": "6", "num_updates": "18800", "lr": "0.00029375", "gnorm": "0.003", "clip": "0", "train_wall": "95", "gb_free": "13.1", "wall": "9315"}
[2022-01-20 15:58:21,942][train_inner][INFO] - {"epoch": 5, "update": 4.428, "loss": "6.658", "ntokens": "1774.97", "nsentences": "6.24", "prob_perplexity": "639.979", "code_perplexity": "525.655", "temp": "1.82", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01165", "wps": "3738.1", "ups": "2.11", "wpb": "1775", "bsz": "6.2", "num_updates": "19000", "lr": "0.000296875", "gnorm": "0.003", "clip": "0", "train_wall": "95", "gb_free": "12.1", "wall": "9409"}
[2022-01-20 15:59:57,585][train_inner][INFO] - {"epoch": 5, "update": 4.474, "loss": "6.658", "ntokens": "1767.85", "nsentences": "5.985", "prob_perplexity": "639.979", "code_perplexity": "525.747", "temp": "1.818", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01177", "wps": "3697.3", "ups": "2.09", "wpb": "1767.8", "bsz": "6", "num_updates": "19200", "lr": "0.0003", "gnorm": "0.003", "clip": "0", "train_wall": "95", "gb_free": "11", "wall": "9505"}
[2022-01-20 16:01:32,518][train_inner][INFO] - {"epoch": 5, "update": 4.521, "loss": "6.658", "ntokens": "1756.95", "nsentences": "5.935", "prob_perplexity": "639.979", "code_perplexity": "520.152", "temp": "1.816", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01168", "wps": "3702", "ups": "2.11", "wpb": "1757", "bsz": "5.9", "num_updates": "19400", "lr": "0.000303125", "gnorm": "0.003", "clip": "0", "train_wall": "95", "gb_free": "11.3", "wall": "9600"}
[2022-01-20 16:03:08,197][train_inner][INFO] - {"epoch": 5, "update": 4.568, "loss": "6.658", "ntokens": "1778.53", "nsentences": "6.145", "prob_perplexity": "639.979", "code_perplexity": "517.965", "temp": "1.814", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01194", "wps": "3718.3", "ups": "2.09", "wpb": "1778.5", "bsz": "6.1", "num_updates": "19600", "lr": "0.00030625", "gnorm": "0.003", "clip": "0", "train_wall": "95", "gb_free": "10.9", "wall": "9696"}
[2022-01-20 16:04:43,799][train_inner][INFO] - {"epoch": 5, "update": 4.614, "loss": "6.658", "ntokens": "1764.09", "nsentences": "6.31", "prob_perplexity": "639.979", "code_perplexity": "524.45", "temp": "1.812", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0119", "wps": "3691", "ups": "2.09", "wpb": "1764.1", "bsz": "6.3", "num_updates": "19800", "lr": "0.000309375", "gnorm": "0.003", "clip": "0", "train_wall": "95", "gb_free": "12", "wall": "9791"}
[2022-01-20 16:06:19,942][train_inner][INFO] - {"epoch": 5, "update": 4.661, "loss": "6.658", "ntokens": "1770.56", "nsentences": "6.17", "prob_perplexity": "639.979", "code_perplexity": "524.66", "temp": "1.811", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01175", "wps": "3683.7", "ups": "2.08", "wpb": "1770.6", "bsz": "6.2", "num_updates": "20000", "lr": "0.0003125", "gnorm": "0.003", "clip": "0", "train_wall": "96", "gb_free": "10.9", "wall": "9887"}
[2022-01-20 16:07:56,280][train_inner][INFO] - {"epoch": 5, "update": 4.708, "loss": "6.658", "ntokens": "1777.53", "nsentences": "6.015", "prob_perplexity": "639.979", "code_perplexity": "524.802", "temp": "1.809", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0116", "wps": "3690.7", "ups": "2.08", "wpb": "1777.5", "bsz": "6", "num_updates": "20200", "lr": "0.000315625", "gnorm": "0.003", "clip": "0", "train_wall": "96", "gb_free": "11.9", "wall": "9984"}
[2022-01-20 16:09:32,212][train_inner][INFO] - {"epoch": 5, "update": 4.754, "loss": "6.658", "ntokens": "1788.8", "nsentences": "5.985", "prob_perplexity": "639.979", "code_perplexity": "525.042", "temp": "1.807", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01166", "wps": "3729.8", "ups": "2.09", "wpb": "1788.8", "bsz": "6", "num_updates": "20400", "lr": "0.00031875", "gnorm": "0.003", "clip": "0", "train_wall": "96", "gb_free": "12", "wall": "10080"}
[2022-01-20 16:11:09,329][train_inner][INFO] - {"epoch": 5, "update": 4.801, "loss": "6.658", "ntokens": "1790.24", "nsentences": "6.225", "prob_perplexity": "639.979", "code_perplexity": "525.401", "temp": "1.805", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01176", "wps": "3687.3", "ups": "2.06", "wpb": "1790.2", "bsz": "6.2", "num_updates": "20600", "lr": "0.000321875", "gnorm": "0.003", "clip": "0", "train_wall": "97", "gb_free": "11.6", "wall": "10177"}
[2022-01-20 16:12:46,016][train_inner][INFO] - {"epoch": 5, "update": 4.847, "loss": "6.658", "ntokens": "1781.01", "nsentences": "6.11", "prob_perplexity": "639.979", "code_perplexity": "524.381", "temp": "1.803", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01153", "wps": "3684.6", "ups": "2.07", "wpb": "1781", "bsz": "6.1", "num_updates": "20800", "lr": "0.000325", "gnorm": "0.003", "clip": "0", "train_wall": "96", "gb_free": "11.7", "wall": "10274"}
[2022-01-20 16:14:22,013][train_inner][INFO] - {"epoch": 5, "update": 4.894, "loss": "6.658", "ntokens": "1769.27", "nsentences": "6.605", "prob_perplexity": "639.979", "code_perplexity": "522.911", "temp": "1.802", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01193", "wps": "3686.6", "ups": "2.08", "wpb": "1769.3", "bsz": "6.6", "num_updates": "21000", "lr": "0.000328125", "gnorm": "0.003", "clip": "0", "train_wall": "96", "gb_free": "12.1", "wall": "10370"}
[2022-01-20 16:15:58,697][train_inner][INFO] - {"epoch": 5, "update": 4.941, "loss": "6.658", "ntokens": "1799.44", "nsentences": "5.735", "prob_perplexity": "639.979", "code_perplexity": "524.377", "temp": "1.8", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01135", "wps": "3722.9", "ups": "2.07", "wpb": "1799.4", "bsz": "5.7", "num_updates": "21200", "lr": "0.00033125", "gnorm": "0.003", "clip": "0", "train_wall": "96", "gb_free": "10.8", "wall": "10466"}
[2022-01-20 16:17:34,531][train_inner][INFO] - {"epoch": 5, "update": 4.987, "loss": "6.658", "ntokens": "1787.12", "nsentences": "5.85", "prob_perplexity": "639.979", "code_perplexity": "523.893", "temp": "1.798", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01173", "wps": "3730.1", "ups": "2.09", "wpb": "1787.1", "bsz": "5.8", "num_updates": "21400", "lr": "0.000334375", "gnorm": "0.003", "clip": "0", "train_wall": "95", "gb_free": "11.4", "wall": "10562"}
[2022-01-20 16:17:59,913][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-20 16:18:06,634][valid][INFO] - {"epoch": 5, "valid_loss": "0.136", "valid_ntokens": "1704.47", "valid_nsentences": "6.04444", "valid_prob_perplexity": "639.979", "valid_code_perplexity": "2.015", "valid_temp": "1.797", "valid_loss_0": "0.136", "valid_loss_1": "0", "valid_loss_2": "0", "valid_accuracy": "0.99699", "valid_wps": "11883.8", "valid_wpb": "1704.5", "valid_bsz": "6", "valid_num_updates": "21455", "valid_best_loss": "0.136"}
[2022-01-20 16:18:06,635][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 21455 updates
[2022-01-20 16:18:06,636][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 16:18:18,467][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 16:18:54,327][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 21455 updates, score 0.136) (writing took 47.69212334044278 seconds)
[2022-01-20 16:18:54,328][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2022-01-20 16:18:54,355][train][INFO] - {"epoch": 5, "train_loss": "6.658", "train_ntokens": "1776.49", "train_nsentences": "6.15894", "train_prob_perplexity": "639.978", "train_code_perplexity": "522.526", "train_temp": "1.816", "train_loss_0": "6.658", "train_loss_1": "0", "train_loss_2": "0", "train_accuracy": "0.01173", "train_wps": "3607.6", "train_ups": "2.03", "train_wpb": "1776.5", "train_bsz": "6.2", "train_num_updates": "21455", "train_lr": "0.000335234", "train_gnorm": "0.003", "train_clip": "0", "train_train_wall": "2051", "train_gb_free": "12.2", "train_wall": "10642"}
[2022-01-20 16:18:54,417][fairseq.trainer][INFO] - begin training epoch 6
[2022-01-20 16:18:54,418][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-20 16:20:02,820][train_inner][INFO] - {"epoch": 6, "update": 5.034, "loss": "6.658", "ntokens": "1759.75", "nsentences": "6.185", "prob_perplexity": "639.979", "code_perplexity": "518.188", "temp": "1.796", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01191", "wps": "2373.6", "ups": "1.35", "wpb": "1759.8", "bsz": "6.2", "num_updates": "21600", "lr": "0.0003375", "gnorm": "0.003", "clip": "0", "train_wall": "93", "gb_free": "12", "wall": "10710"}
[2022-01-20 16:21:38,987][train_inner][INFO] - {"epoch": 6, "update": 5.08, "loss": "6.658", "ntokens": "1784.35", "nsentences": "6.155", "prob_perplexity": "639.979", "code_perplexity": "523.317", "temp": "1.794", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01193", "wps": "3711.4", "ups": "2.08", "wpb": "1784.3", "bsz": "6.2", "num_updates": "21800", "lr": "0.000340625", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.8", "wall": "10807"}
[2022-01-20 16:23:14,951][train_inner][INFO] - {"epoch": 6, "update": 5.127, "loss": "6.658", "ntokens": "1785.84", "nsentences": "6.095", "prob_perplexity": "639.979", "code_perplexity": "523.455", "temp": "1.793", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01164", "wps": "3722.4", "ups": "2.08", "wpb": "1785.8", "bsz": "6.1", "num_updates": "22000", "lr": "0.00034375", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.4", "wall": "10903"}
[2022-01-20 16:24:51,778][train_inner][INFO] - {"epoch": 6, "update": 5.174, "loss": "6.658", "ntokens": "1785.05", "nsentences": "6.285", "prob_perplexity": "639.979", "code_perplexity": "522.938", "temp": "1.791", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01157", "wps": "3687.6", "ups": "2.07", "wpb": "1785", "bsz": "6.3", "num_updates": "22200", "lr": "0.000346875", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.3", "wall": "10999"}
[2022-01-20 16:26:28,503][train_inner][INFO] - {"epoch": 6, "update": 5.22, "loss": "6.658", "ntokens": "1804.55", "nsentences": "5.89", "prob_perplexity": "639.979", "code_perplexity": "523.411", "temp": "1.789", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01142", "wps": "3731.8", "ups": "2.07", "wpb": "1804.5", "bsz": "5.9", "num_updates": "22400", "lr": "0.00035", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.9", "wall": "11096"}
[2022-01-20 16:28:04,867][train_inner][INFO] - {"epoch": 6, "update": 5.267, "loss": "6.658", "ntokens": "1774.85", "nsentences": "5.88", "prob_perplexity": "639.979", "code_perplexity": "522.047", "temp": "1.787", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01164", "wps": "3684.1", "ups": "2.08", "wpb": "1774.8", "bsz": "5.9", "num_updates": "22600", "lr": "0.000353125", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "12.6", "wall": "11192"}
[2022-01-20 16:29:40,451][train_inner][INFO] - {"epoch": 6, "update": 5.313, "loss": "6.658", "ntokens": "1774.62", "nsentences": "6.175", "prob_perplexity": "639.979", "code_perplexity": "521.586", "temp": "1.785", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01176", "wps": "3713.7", "ups": "2.09", "wpb": "1774.6", "bsz": "6.2", "num_updates": "22800", "lr": "0.00035625", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11", "wall": "11288"}
[2022-01-20 16:31:15,309][train_inner][INFO] - {"epoch": 6, "update": 5.36, "loss": "6.658", "ntokens": "1748.55", "nsentences": "6.64", "prob_perplexity": "639.979", "code_perplexity": "519.995", "temp": "1.784", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01202", "wps": "3687.2", "ups": "2.11", "wpb": "1748.5", "bsz": "6.6", "num_updates": "23000", "lr": "0.000359375", "gnorm": "0.002", "clip": "0", "train_wall": "94", "gb_free": "12", "wall": "11383"}
[2022-01-20 16:32:51,037][train_inner][INFO] - {"epoch": 6, "update": 5.407, "loss": "6.658", "ntokens": "1785.87", "nsentences": "6.365", "prob_perplexity": "639.979", "code_perplexity": "520.963", "temp": "1.782", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0118", "wps": "3731.6", "ups": "2.09", "wpb": "1785.9", "bsz": "6.4", "num_updates": "23200", "lr": "0.0003625", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11.9", "wall": "11479"}
[2022-01-20 16:34:26,660][train_inner][INFO] - {"epoch": 6, "update": 5.453, "loss": "6.658", "ntokens": "1760", "nsentences": "6.075", "prob_perplexity": "639.979", "code_perplexity": "519.582", "temp": "1.78", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01168", "wps": "3681.7", "ups": "2.09", "wpb": "1760", "bsz": "6.1", "num_updates": "23400", "lr": "0.000365625", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11.7", "wall": "11574"}
[2022-01-20 16:36:02,110][train_inner][INFO] - {"epoch": 6, "update": 5.5, "loss": "6.658", "ntokens": "1778.21", "nsentences": "6.03", "prob_perplexity": "639.979", "code_perplexity": "520.389", "temp": "1.778", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01196", "wps": "3726.4", "ups": "2.1", "wpb": "1778.2", "bsz": "6", "num_updates": "23600", "lr": "0.00036875", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "10.9", "wall": "11670"}
[2022-01-20 16:37:38,139][train_inner][INFO] - {"epoch": 6, "update": 5.546, "loss": "6.658", "ntokens": "1776.6", "nsentences": "6.125", "prob_perplexity": "639.979", "code_perplexity": "519.484", "temp": "1.777", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01187", "wps": "3700.6", "ups": "2.08", "wpb": "1776.6", "bsz": "6.1", "num_updates": "23800", "lr": "0.000371875", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "12.1", "wall": "11766"}
[2022-01-20 16:39:14,141][train_inner][INFO] - {"epoch": 6, "update": 5.593, "loss": "6.658", "ntokens": "1779.71", "nsentences": "6", "prob_perplexity": "639.979", "code_perplexity": "519.91", "temp": "1.775", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01152", "wps": "3708.2", "ups": "2.08", "wpb": "1779.7", "bsz": "6", "num_updates": "24000", "lr": "0.000375", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "10.3", "wall": "11862"}
[2022-01-20 16:40:50,549][train_inner][INFO] - {"epoch": 6, "update": 5.64, "loss": "6.658", "ntokens": "1794.06", "nsentences": "6.155", "prob_perplexity": "639.979", "code_perplexity": "520.501", "temp": "1.773", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01188", "wps": "3722.4", "ups": "2.07", "wpb": "1794.1", "bsz": "6.2", "num_updates": "24200", "lr": "0.000378125", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.9", "wall": "11958"}
[2022-01-20 16:42:26,334][train_inner][INFO] - {"epoch": 6, "update": 5.686, "loss": "6.658", "ntokens": "1781.43", "nsentences": "6", "prob_perplexity": "639.979", "code_perplexity": "518.392", "temp": "1.771", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01148", "wps": "3720.2", "ups": "2.09", "wpb": "1781.4", "bsz": "6", "num_updates": "24400", "lr": "0.00038125", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "10.2", "wall": "12054"}
[2022-01-20 16:44:02,694][train_inner][INFO] - {"epoch": 6, "update": 5.733, "loss": "6.658", "ntokens": "1791.62", "nsentences": "6.005", "prob_perplexity": "639.979", "code_perplexity": "518.336", "temp": "1.769", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01175", "wps": "3719.1", "ups": "2.08", "wpb": "1791.6", "bsz": "6", "num_updates": "24600", "lr": "0.000384375", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "10.1", "wall": "12150"}
[2022-01-20 16:45:38,676][train_inner][INFO] - {"epoch": 6, "update": 5.78, "loss": "6.658", "ntokens": "1765.09", "nsentences": "6.4", "prob_perplexity": "639.979", "code_perplexity": "516.787", "temp": "1.768", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01167", "wps": "3678.5", "ups": "2.08", "wpb": "1765.1", "bsz": "6.4", "num_updates": "24800", "lr": "0.0003875", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.4", "wall": "12246"}
[2022-01-20 16:47:13,489][train_inner][INFO] - {"epoch": 6, "update": 5.826, "loss": "6.658", "ntokens": "1760.87", "nsentences": "6.165", "prob_perplexity": "639.979", "code_perplexity": "515.377", "temp": "1.766", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01173", "wps": "3714.9", "ups": "2.11", "wpb": "1760.9", "bsz": "6.2", "num_updates": "25000", "lr": "0.000390625", "gnorm": "0.002", "clip": "0", "train_wall": "94", "gb_free": "12.8", "wall": "12341"}
[2022-01-20 16:47:13,490][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-20 16:47:20,207][valid][INFO] - {"epoch": 6, "valid_loss": "0.115", "valid_ntokens": "1691.93", "valid_nsentences": "6.04444", "valid_prob_perplexity": "639.979", "valid_code_perplexity": "2.015", "valid_temp": "1.765", "valid_loss_0": "0.115", "valid_loss_1": "0", "valid_loss_2": "0", "valid_accuracy": "0.96832", "valid_wps": "11769.1", "valid_wpb": "1691.9", "valid_bsz": "6", "valid_num_updates": "25000", "valid_best_loss": "0.115"}
[2022-01-20 16:47:20,209][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 25000 updates
[2022-01-20 16:47:20,209][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_6_25000.pt
[2022-01-20 16:47:31,854][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_6_25000.pt
[2022-01-20 16:48:16,960][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_6_25000.pt (epoch 6 @ 25000 updates, score 0.115) (writing took 56.7506353687495 seconds)
[2022-01-20 16:49:52,662][train_inner][INFO] - {"epoch": 6, "update": 5.873, "loss": "6.658", "ntokens": "1765.88", "nsentences": "6.605", "prob_perplexity": "639.979", "code_perplexity": "516.539", "temp": "1.764", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01192", "wps": "2219", "ups": "1.26", "wpb": "1765.9", "bsz": "6.6", "num_updates": "25200", "lr": "0.00039375", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11.4", "wall": "12500"}
[2022-01-20 16:51:29,270][train_inner][INFO] - {"epoch": 6, "update": 5.919, "loss": "6.658", "ntokens": "1782.61", "nsentences": "6.015", "prob_perplexity": "639.979", "code_perplexity": "515.358", "temp": "1.762", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0117", "wps": "3690.9", "ups": "2.07", "wpb": "1782.6", "bsz": "6", "num_updates": "25400", "lr": "0.000396875", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "12.2", "wall": "12597"}
[2022-01-20 16:53:05,396][train_inner][INFO] - {"epoch": 6, "update": 5.966, "loss": "6.658", "ntokens": "1795.67", "nsentences": "5.98", "prob_perplexity": "639.979", "code_perplexity": "516.658", "temp": "1.761", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01173", "wps": "3736.6", "ups": "2.08", "wpb": "1795.7", "bsz": "6", "num_updates": "25600", "lr": "0.0004", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.5", "wall": "12693"}
[2022-01-20 16:54:15,654][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-20 16:54:22,323][valid][INFO] - {"epoch": 6, "valid_loss": "0.089", "valid_ntokens": "1713.64", "valid_nsentences": "6.04444", "valid_prob_perplexity": "639.979", "valid_code_perplexity": "2.011", "valid_temp": "1.758", "valid_loss_0": "0.089", "valid_loss_1": "0", "valid_loss_2": "0", "valid_accuracy": "0.94835", "valid_wps": "11932.7", "valid_wpb": "1713.6", "valid_bsz": "6", "valid_num_updates": "25746", "valid_best_loss": "0.089"}
[2022-01-20 16:54:22,324][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 25746 updates
[2022-01-20 16:54:22,325][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 16:54:34,331][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 16:55:07,469][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 25746 updates, score 0.089) (writing took 45.1445802077651 seconds)
[2022-01-20 16:55:07,470][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2022-01-20 16:55:07,499][train][INFO] - {"epoch": 6, "train_loss": "6.658", "train_ntokens": "1778.1", "train_nsentences": "6.15894", "train_prob_perplexity": "639.979", "train_code_perplexity": "519.477", "train_temp": "1.777", "train_loss_0": "6.658", "train_loss_1": "0", "train_loss_2": "0", "train_accuracy": "0.01173", "train_wps": "3511", "train_ups": "1.97", "train_wpb": "1778.1", "train_bsz": "6.2", "train_num_updates": "25746", "train_lr": "0.000402281", "train_gnorm": "0.002", "train_clip": "0", "train_train_wall": "2050", "train_gb_free": "16.4", "train_wall": "12815"}
[2022-01-20 16:55:07,525][fairseq.trainer][INFO] - begin training epoch 7
[2022-01-20 16:55:07,525][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-20 16:55:33,307][train_inner][INFO] - {"epoch": 7, "update": 6.013, "loss": "6.658", "ntokens": "1784.18", "nsentences": "6.265", "prob_perplexity": "639.979", "code_perplexity": "515.178", "temp": "1.759", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01154", "wps": "2412.7", "ups": "1.35", "wpb": "1784.2", "bsz": "6.3", "num_updates": "25800", "lr": "0.000403125", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11.1", "wall": "12841"}
[2022-01-20 16:57:08,908][train_inner][INFO] - {"epoch": 7, "update": 6.059, "loss": "6.658", "ntokens": "1767.51", "nsentences": "5.885", "prob_perplexity": "639.979", "code_perplexity": "514.191", "temp": "1.757", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0115", "wps": "3698.2", "ups": "2.09", "wpb": "1767.5", "bsz": "5.9", "num_updates": "26000", "lr": "0.00040625", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11.4", "wall": "12936"}
[2022-01-20 16:58:44,820][train_inner][INFO] - {"epoch": 7, "update": 6.106, "loss": "6.658", "ntokens": "1775.72", "nsentences": "6.16", "prob_perplexity": "639.979", "code_perplexity": "514.623", "temp": "1.755", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0117", "wps": "3703.3", "ups": "2.09", "wpb": "1775.7", "bsz": "6.2", "num_updates": "26200", "lr": "0.000409375", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "15.4", "wall": "13032"}
[2022-01-20 17:00:20,632][train_inner][INFO] - {"epoch": 7, "update": 6.152, "loss": "6.658", "ntokens": "1777.76", "nsentences": "5.995", "prob_perplexity": "639.979", "code_perplexity": "515.032", "temp": "1.754", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01166", "wps": "3711.5", "ups": "2.09", "wpb": "1777.8", "bsz": "6", "num_updates": "26400", "lr": "0.0004125", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11.1", "wall": "13128"}
[2022-01-20 17:01:56,507][train_inner][INFO] - {"epoch": 7, "update": 6.199, "loss": "6.658", "ntokens": "1790.27", "nsentences": "6.175", "prob_perplexity": "639.979", "code_perplexity": "514.975", "temp": "1.752", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01179", "wps": "3735.1", "ups": "2.09", "wpb": "1790.3", "bsz": "6.2", "num_updates": "26600", "lr": "0.000415625", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.3", "wall": "13224"}
[2022-01-20 17:03:32,774][train_inner][INFO] - {"epoch": 7, "update": 6.246, "loss": "6.658", "ntokens": "1779.69", "nsentences": "5.985", "prob_perplexity": "639.979", "code_perplexity": "513.63", "temp": "1.75", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01191", "wps": "3697.9", "ups": "2.08", "wpb": "1779.7", "bsz": "6", "num_updates": "26800", "lr": "0.00041875", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "13.2", "wall": "13320"}
[2022-01-20 17:05:08,637][train_inner][INFO] - {"epoch": 7, "update": 6.292, "loss": "6.658", "ntokens": "1775.27", "nsentences": "6.26", "prob_perplexity": "639.979", "code_perplexity": "512.963", "temp": "1.748", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0119", "wps": "3704.2", "ups": "2.09", "wpb": "1775.3", "bsz": "6.3", "num_updates": "27000", "lr": "0.000421875", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11", "wall": "13416"}
[2022-01-20 17:06:44,348][train_inner][INFO] - {"epoch": 7, "update": 6.339, "loss": "6.658", "ntokens": "1776.63", "nsentences": "6.125", "prob_perplexity": "639.979", "code_perplexity": "513.119", "temp": "1.747", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01186", "wps": "3713", "ups": "2.09", "wpb": "1776.6", "bsz": "6.1", "num_updates": "27200", "lr": "0.000425", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "11.2", "wall": "13512"}
[2022-01-20 17:08:20,427][train_inner][INFO] - {"epoch": 7, "update": 6.385, "loss": "6.658", "ntokens": "1758.87", "nsentences": "6.655", "prob_perplexity": "639.979", "code_perplexity": "511.885", "temp": "1.745", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01195", "wps": "3661.8", "ups": "2.08", "wpb": "1758.9", "bsz": "6.7", "num_updates": "27400", "lr": "0.000428125", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.7", "wall": "13608"}
[2022-01-20 17:09:56,997][train_inner][INFO] - {"epoch": 7, "update": 6.432, "loss": "6.658", "ntokens": "1788.69", "nsentences": "6.19", "prob_perplexity": "639.979", "code_perplexity": "512.907", "temp": "1.743", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01178", "wps": "3705", "ups": "2.07", "wpb": "1788.7", "bsz": "6.2", "num_updates": "27600", "lr": "0.00043125", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.7", "wall": "13705"}
[2022-01-20 17:11:33,065][train_inner][INFO] - {"epoch": 7, "update": 6.479, "loss": "6.658", "ntokens": "1761.06", "nsentences": "6.05", "prob_perplexity": "639.979", "code_perplexity": "510.891", "temp": "1.741", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01181", "wps": "3666.8", "ups": "2.08", "wpb": "1761.1", "bsz": "6", "num_updates": "27800", "lr": "0.000434375", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "12.5", "wall": "13801"}
[2022-01-20 17:13:09,626][train_inner][INFO] - {"epoch": 7, "update": 6.525, "loss": "6.658", "ntokens": "1784.17", "nsentences": "5.99", "prob_perplexity": "639.979", "code_perplexity": "511.602", "temp": "1.74", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01172", "wps": "3695.9", "ups": "2.07", "wpb": "1784.2", "bsz": "6", "num_updates": "28000", "lr": "0.0004375", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "12.3", "wall": "13897"}
[2022-01-20 17:14:46,768][train_inner][INFO] - {"epoch": 7, "update": 6.572, "loss": "6.658", "ntokens": "1806.19", "nsentences": "5.955", "prob_perplexity": "639.979", "code_perplexity": "510.223", "temp": "1.738", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01172", "wps": "3719.2", "ups": "2.06", "wpb": "1806.2", "bsz": "6", "num_updates": "28200", "lr": "0.000440625", "gnorm": "0.002", "clip": "0", "train_wall": "97", "gb_free": "10.9", "wall": "13994"}
[2022-01-20 17:16:21,775][train_inner][INFO] - {"epoch": 7, "update": 6.619, "loss": "6.658", "ntokens": "1765.15", "nsentences": "6.055", "prob_perplexity": "639.979", "code_perplexity": "502.826", "temp": "1.736", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01203", "wps": "3716.4", "ups": "2.11", "wpb": "1765.2", "bsz": "6.1", "num_updates": "28400", "lr": "0.00044375", "gnorm": "0.003", "clip": "0", "train_wall": "95", "gb_free": "11.9", "wall": "14089"}
[2022-01-20 17:17:57,961][train_inner][INFO] - {"epoch": 7, "update": 6.665, "loss": "6.658", "ntokens": "1782.18", "nsentences": "6.16", "prob_perplexity": "639.979", "code_perplexity": "506.314", "temp": "1.734", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01173", "wps": "3706.2", "ups": "2.08", "wpb": "1782.2", "bsz": "6.2", "num_updates": "28600", "lr": "0.000446875", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.9", "wall": "14186"}
[2022-01-20 17:19:32,711][train_inner][INFO] - {"epoch": 7, "update": 6.712, "loss": "6.658", "ntokens": "1760.96", "nsentences": "6.625", "prob_perplexity": "639.979", "code_perplexity": "504.752", "temp": "1.733", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01203", "wps": "3717.6", "ups": "2.11", "wpb": "1761", "bsz": "6.6", "num_updates": "28800", "lr": "0.00045", "gnorm": "0.002", "clip": "0", "train_wall": "94", "gb_free": "10", "wall": "14280"}
[2022-01-20 17:21:08,473][train_inner][INFO] - {"epoch": 7, "update": 6.758, "loss": "6.658", "ntokens": "1790.16", "nsentences": "5.87", "prob_perplexity": "639.979", "code_perplexity": "506.483", "temp": "1.731", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.0117", "wps": "3739.3", "ups": "2.09", "wpb": "1790.2", "bsz": "5.9", "num_updates": "29000", "lr": "0.000453125", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "12.1", "wall": "14376"}
[2022-01-20 17:22:41,472][train_inner][INFO] - {"epoch": 7, "update": 6.805, "loss": "6.658", "ntokens": "1737.26", "nsentences": "6.33", "prob_perplexity": "639.979", "code_perplexity": "497.639", "temp": "1.729", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01188", "wps": "3736.6", "ups": "2.15", "wpb": "1737.3", "bsz": "6.3", "num_updates": "29200", "lr": "0.00045625", "gnorm": "0.002", "clip": "0", "train_wall": "93", "gb_free": "11.7", "wall": "14469"}
[2022-01-20 17:24:16,602][train_inner][INFO] - {"epoch": 7, "update": 6.852, "loss": "6.658", "ntokens": "1765.31", "nsentences": "6.31", "prob_perplexity": "639.979", "code_perplexity": "500.353", "temp": "1.727", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01163", "wps": "3711.9", "ups": "2.1", "wpb": "1765.3", "bsz": "6.3", "num_updates": "29400", "lr": "0.000459375", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "17.7", "wall": "14564"}
[2022-01-20 17:25:52,854][train_inner][INFO] - {"epoch": 7, "update": 6.898, "loss": "6.658", "ntokens": "1769.88", "nsentences": "6.315", "prob_perplexity": "639.979", "code_perplexity": "500.742", "temp": "1.726", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01163", "wps": "3678.1", "ups": "2.08", "wpb": "1769.9", "bsz": "6.3", "num_updates": "29600", "lr": "0.0004625", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.8", "wall": "14660"}
[2022-01-20 17:27:29,628][train_inner][INFO] - {"epoch": 7, "update": 6.945, "loss": "6.658", "ntokens": "1786.53", "nsentences": "6.08", "prob_perplexity": "639.979", "code_perplexity": "499.506", "temp": "1.724", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01168", "wps": "3692.7", "ups": "2.07", "wpb": "1786.5", "bsz": "6.1", "num_updates": "29800", "lr": "0.000465625", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "10.3", "wall": "14757"}
[2022-01-20 17:29:07,227][train_inner][INFO] - {"epoch": 7, "update": 6.991, "loss": "6.658", "ntokens": "1788.15", "nsentences": "6.235", "prob_perplexity": "639.979", "code_perplexity": "498.978", "temp": "1.722", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01165", "wps": "3664.8", "ups": "2.05", "wpb": "1788.2", "bsz": "6.2", "num_updates": "30000", "lr": "0.00046875", "gnorm": "0.002", "clip": "0", "train_wall": "97", "gb_free": "12.2", "wall": "14855"}
[2022-01-20 17:29:25,418][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-01-20 17:29:32,093][valid][INFO] - {"epoch": 7, "valid_loss": "0.03", "valid_ntokens": "1693.69", "valid_nsentences": "6.04444", "valid_prob_perplexity": "639.979", "valid_code_perplexity": "2.004", "valid_temp": "1.721", "valid_loss_0": "0.03", "valid_loss_1": "0", "valid_loss_2": "0", "valid_accuracy": "0.99517", "valid_wps": "11804.5", "valid_wpb": "1693.7", "valid_bsz": "6", "valid_num_updates": "30037", "valid_best_loss": "0.03"}
[2022-01-20 17:29:32,094][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 30037 updates
[2022-01-20 17:29:32,095][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 17:29:44,041][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2022-01-20 17:30:19,527][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 30037 updates, score 0.03) (writing took 47.43227063398808 seconds)
[2022-01-20 17:30:19,527][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2022-01-20 17:30:19,559][train][INFO] - {"epoch": 7, "train_loss": "6.658", "train_ntokens": "1776.61", "train_nsentences": "6.15894", "train_prob_perplexity": "639.979", "train_code_perplexity": "508.312", "train_temp": "1.74", "train_loss_0": "6.658", "train_loss_1": "0", "train_loss_2": "0", "train_accuracy": "0.01177", "train_wps": "3609.5", "train_ups": "2.03", "train_wpb": "1776.6", "train_bsz": "6.2", "train_num_updates": "30037", "train_lr": "0.000469328", "train_gnorm": "0.002", "train_clip": "0", "train_train_wall": "2050", "train_gb_free": "13.9", "train_wall": "14927"}
[2022-01-20 17:30:19,586][fairseq.trainer][INFO] - begin training epoch 8
[2022-01-20 17:30:19,586][fairseq_cli.train][INFO] - Start iterating over samples
[2022-01-20 17:31:36,847][train_inner][INFO] - {"epoch": 8, "update": 7.038, "loss": "6.658", "ntokens": "1789.02", "nsentences": "6.05", "prob_perplexity": "639.979", "code_perplexity": "498.24", "temp": "1.721", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01151", "wps": "2391.6", "ups": "1.34", "wpb": "1789", "bsz": "6", "num_updates": "30200", "lr": "0.000471875", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "10.9", "wall": "15004"}
[2022-01-20 17:33:12,106][train_inner][INFO] - {"epoch": 8, "update": 7.085, "loss": "6.658", "ntokens": "1778.18", "nsentences": "5.85", "prob_perplexity": "639.979", "code_perplexity": "498.742", "temp": "1.719", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01179", "wps": "3733.9", "ups": "2.1", "wpb": "1778.2", "bsz": "5.8", "num_updates": "30400", "lr": "0.000475", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "15.3", "wall": "15100"}
[2022-01-20 17:34:47,889][train_inner][INFO] - {"epoch": 8, "update": 7.131, "loss": "6.658", "ntokens": "1772.87", "nsentences": "6.145", "prob_perplexity": "639.979", "code_perplexity": "496.556", "temp": "1.717", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01185", "wps": "3702.3", "ups": "2.09", "wpb": "1772.9", "bsz": "6.1", "num_updates": "30600", "lr": "0.000478125", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "17.6", "wall": "15195"}
[2022-01-20 17:36:23,458][train_inner][INFO] - {"epoch": 8, "update": 7.178, "loss": "6.658", "ntokens": "1769.21", "nsentences": "6.15", "prob_perplexity": "639.979", "code_perplexity": "491.365", "temp": "1.715", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01182", "wps": "3703", "ups": "2.09", "wpb": "1769.2", "bsz": "6.2", "num_updates": "30800", "lr": "0.00048125", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "12.1", "wall": "15291"}
[2022-01-20 17:37:59,296][train_inner][INFO] - {"epoch": 8, "update": 7.224, "loss": "6.658", "ntokens": "1771.55", "nsentences": "6.21", "prob_perplexity": "639.979", "code_perplexity": "495.717", "temp": "1.714", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01162", "wps": "3697.5", "ups": "2.09", "wpb": "1771.5", "bsz": "6.2", "num_updates": "31000", "lr": "0.000484375", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "12.5", "wall": "15387"}
[2022-01-20 17:39:35,040][train_inner][INFO] - {"epoch": 8, "update": 7.271, "loss": "6.658", "ntokens": "1789.39", "nsentences": "6.2", "prob_perplexity": "639.979", "code_perplexity": "493.629", "temp": "1.712", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01166", "wps": "3738.3", "ups": "2.09", "wpb": "1789.4", "bsz": "6.2", "num_updates": "31200", "lr": "0.0004875", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "17.3", "wall": "15483"}
[2022-01-20 17:41:11,411][train_inner][INFO] - {"epoch": 8, "update": 7.318, "loss": "6.658", "ntokens": "1773.88", "nsentences": "6.395", "prob_perplexity": "639.979", "code_perplexity": "491.618", "temp": "1.71", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01164", "wps": "3681.8", "ups": "2.08", "wpb": "1773.9", "bsz": "6.4", "num_updates": "31400", "lr": "0.000490625", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.6", "wall": "15579"}
[2022-01-20 17:42:47,824][train_inner][INFO] - {"epoch": 8, "update": 7.364, "loss": "6.658", "ntokens": "1799.9", "nsentences": "6.155", "prob_perplexity": "639.979", "code_perplexity": "492.544", "temp": "1.709", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01153", "wps": "3734.2", "ups": "2.07", "wpb": "1799.9", "bsz": "6.2", "num_updates": "31600", "lr": "0.00049375", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "10.2", "wall": "15675"}
[2022-01-20 17:44:23,486][train_inner][INFO] - {"epoch": 8, "update": 7.411, "loss": "6.658", "ntokens": "1767.8", "nsentences": "6.07", "prob_perplexity": "639.979", "code_perplexity": "490.331", "temp": "1.707", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01181", "wps": "3696.4", "ups": "2.09", "wpb": "1767.8", "bsz": "6.1", "num_updates": "31800", "lr": "0.000496875", "gnorm": "0.002", "clip": "0", "train_wall": "95", "gb_free": "10.8", "wall": "15771"}
[2022-01-20 17:45:59,432][train_inner][INFO] - {"epoch": 8, "update": 7.457, "loss": "6.658", "ntokens": "1782.22", "nsentences": "6.185", "prob_perplexity": "639.979", "code_perplexity": "489.59", "temp": "1.705", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01175", "wps": "3715.6", "ups": "2.08", "wpb": "1782.2", "bsz": "6.2", "num_updates": "32000", "lr": "0.0005", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11.8", "wall": "15867"}
[2022-01-20 17:47:35,831][train_inner][INFO] - {"epoch": 8, "update": 7.504, "loss": "6.658", "ntokens": "1789.27", "nsentences": "6.115", "prob_perplexity": "639.979", "code_perplexity": "490.605", "temp": "1.703", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01194", "wps": "3712.8", "ups": "2.08", "wpb": "1789.3", "bsz": "6.1", "num_updates": "32200", "lr": "0.000499728", "gnorm": "0.001", "clip": "0", "train_wall": "96", "gb_free": "10.8", "wall": "15963"}
[2022-01-20 17:49:11,902][train_inner][INFO] - {"epoch": 8, "update": 7.551, "loss": "6.658", "ntokens": "1775.76", "nsentences": "5.855", "prob_perplexity": "639.979", "code_perplexity": "488.262", "temp": "1.702", "loss_0": "6.658", "loss_1": "0", "loss_2": "0", "accuracy": "0.01167", "wps": "3697.2", "ups": "2.08", "wpb": "1775.8", "bsz": "5.9", "num_updates": "32400", "lr": "0.000499457", "gnorm": "0.002", "clip": "0", "train_wall": "96", "gb_free": "11", "wall": "16059"}
